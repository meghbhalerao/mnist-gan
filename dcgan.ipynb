{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1701
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42193,
     "status": "error",
     "timestamp": 1553453095017,
     "user": {
      "displayName": "Megh Bhalerao",
      "photoUrl": "https://lh3.googleusercontent.com/-sFkXmKzG5os/AAAAAAAAAAI/AAAAAAAADCw/-HXSFmytHsY/s64/photo.jpg",
      "userId": "14122289173155118268"
     },
     "user_tz": -330
    },
    "id": "ABSGn-xfHgoA",
    "outputId": "19d09043-e500-4183-89f5-d17055e50eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_99 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPaddi (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_29 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_30 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_105 (Conv2D)          (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.158962, acc.: 29.69%] [G loss: 0.510475]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d41d41c20f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-d41d41c20f2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/gdrive/My Drive/mnistwe.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# Create checkpoint callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            checkpoint_path = \"/content/gdrive/My Drive/mnistwe.ckpt\"\n",
    "            checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "            # Create checkpoint callback\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                             save_weights_only=True,\n",
    "                                                             verbose=1)\n",
    "\n",
    "            model = create_model()\n",
    "\n",
    "            \n",
    "            \n",
    "            #checkpoint = ModelCheckpoint(\"./chk_%d\" %epochs, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #callbacks_list = [checkpoint]\n",
    "            #checkpoint = ModelCheckpoint(\"/content/gdrive/My Drive/mnistwe_%d.ckpt\" %epoch, monitor = \"val_loss\", verbose = 0,save_best_only = False, save_weights_only = False, mode = \"max\", period = 1)\n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"/content/gdrive/My Drive/mnist_%d.png\" % epoch)\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=10, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aw11sP4UBvLY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74590
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 409568,
     "status": "ok",
     "timestamp": 1553454923247,
     "user": {
      "displayName": "Megh Bhalerao",
      "photoUrl": "https://lh3.googleusercontent.com/-sFkXmKzG5os/AAAAAAAAAAI/AAAAAAAADCw/-HXSFmytHsY/s64/photo.jpg",
      "userId": "14122289173155118268"
     },
     "user_tz": -330
    },
    "id": "6F7_SSSsC_6O",
    "outputId": "9cf56db9-308d-47fe-ffff-a7949bfec591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_120 (Conv2D)          (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_18 (ZeroPaddi (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_18 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.154986, acc.: 37.50%] [G loss: 0.608257]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1 [D loss: 0.669223, acc.: 56.25%] [G loss: 0.762290]\n",
      "2 [D loss: 0.659719, acc.: 60.94%] [G loss: 0.998174]\n",
      "3 [D loss: 0.434477, acc.: 79.69%] [G loss: 0.886387]\n",
      "4 [D loss: 0.385252, acc.: 81.25%] [G loss: 0.596535]\n",
      "5 [D loss: 0.299928, acc.: 84.38%] [G loss: 0.566487]\n",
      "6 [D loss: 0.321429, acc.: 85.94%] [G loss: 0.747513]\n",
      "7 [D loss: 0.441236, acc.: 81.25%] [G loss: 0.750046]\n",
      "8 [D loss: 0.801651, acc.: 54.69%] [G loss: 1.226548]\n",
      "9 [D loss: 0.937342, acc.: 40.62%] [G loss: 1.444849]\n",
      "10 [D loss: 0.839647, acc.: 62.50%] [G loss: 2.017618]\n",
      "11 [D loss: 0.571305, acc.: 68.75%] [G loss: 1.968890]\n",
      "12 [D loss: 0.576173, acc.: 78.12%] [G loss: 1.544728]\n",
      "13 [D loss: 0.468596, acc.: 78.12%] [G loss: 1.651174]\n",
      "14 [D loss: 0.462777, acc.: 75.00%] [G loss: 2.201123]\n",
      "15 [D loss: 0.681289, acc.: 68.75%] [G loss: 1.588917]\n",
      "16 [D loss: 0.635851, acc.: 62.50%] [G loss: 1.939898]\n",
      "17 [D loss: 0.616955, acc.: 59.38%] [G loss: 1.180344]\n",
      "18 [D loss: 0.448048, acc.: 81.25%] [G loss: 0.799449]\n",
      "19 [D loss: 0.520766, acc.: 76.56%] [G loss: 1.111429]\n",
      "20 [D loss: 0.714466, acc.: 68.75%] [G loss: 1.246835]\n",
      "21 [D loss: 0.694211, acc.: 65.62%] [G loss: 1.350397]\n",
      "22 [D loss: 1.121068, acc.: 42.19%] [G loss: 1.192963]\n",
      "23 [D loss: 0.839177, acc.: 59.38%] [G loss: 1.253141]\n",
      "24 [D loss: 0.592686, acc.: 65.62%] [G loss: 1.460416]\n",
      "25 [D loss: 0.801824, acc.: 60.94%] [G loss: 1.469103]\n",
      "26 [D loss: 0.740717, acc.: 64.06%] [G loss: 1.400198]\n",
      "27 [D loss: 0.775684, acc.: 56.25%] [G loss: 1.405549]\n",
      "28 [D loss: 0.939925, acc.: 48.44%] [G loss: 1.170231]\n",
      "29 [D loss: 0.938327, acc.: 53.12%] [G loss: 1.183371]\n",
      "30 [D loss: 0.902033, acc.: 43.75%] [G loss: 1.564217]\n",
      "31 [D loss: 1.121052, acc.: 35.94%] [G loss: 1.287036]\n",
      "32 [D loss: 0.898229, acc.: 50.00%] [G loss: 1.456431]\n",
      "33 [D loss: 0.919009, acc.: 54.69%] [G loss: 1.257570]\n",
      "34 [D loss: 0.996501, acc.: 39.06%] [G loss: 0.732494]\n",
      "35 [D loss: 0.810962, acc.: 54.69%] [G loss: 0.890034]\n",
      "36 [D loss: 0.567218, acc.: 68.75%] [G loss: 1.144132]\n",
      "37 [D loss: 0.873850, acc.: 43.75%] [G loss: 1.239649]\n",
      "38 [D loss: 0.807350, acc.: 60.94%] [G loss: 1.047642]\n",
      "39 [D loss: 1.148731, acc.: 34.38%] [G loss: 1.381122]\n",
      "40 [D loss: 0.931311, acc.: 45.31%] [G loss: 1.466152]\n",
      "41 [D loss: 0.840024, acc.: 45.31%] [G loss: 1.360860]\n",
      "42 [D loss: 0.948368, acc.: 40.62%] [G loss: 1.626466]\n",
      "43 [D loss: 0.874747, acc.: 51.56%] [G loss: 1.382379]\n",
      "44 [D loss: 1.089712, acc.: 39.06%] [G loss: 1.320893]\n",
      "45 [D loss: 0.893918, acc.: 51.56%] [G loss: 1.674178]\n",
      "46 [D loss: 0.716729, acc.: 54.69%] [G loss: 1.188607]\n",
      "47 [D loss: 0.830396, acc.: 50.00%] [G loss: 1.151356]\n",
      "48 [D loss: 0.840076, acc.: 57.81%] [G loss: 0.985534]\n",
      "49 [D loss: 0.623557, acc.: 68.75%] [G loss: 0.923496]\n",
      "50 [D loss: 0.704002, acc.: 68.75%] [G loss: 1.050801]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "51 [D loss: 0.913243, acc.: 39.06%] [G loss: 1.151038]\n",
      "52 [D loss: 0.968278, acc.: 39.06%] [G loss: 1.502192]\n",
      "53 [D loss: 0.914329, acc.: 43.75%] [G loss: 1.184612]\n",
      "54 [D loss: 0.829930, acc.: 57.81%] [G loss: 1.211210]\n",
      "55 [D loss: 0.759894, acc.: 53.12%] [G loss: 1.437586]\n",
      "56 [D loss: 0.762332, acc.: 59.38%] [G loss: 1.419795]\n",
      "57 [D loss: 0.740358, acc.: 59.38%] [G loss: 1.375366]\n",
      "58 [D loss: 0.695463, acc.: 59.38%] [G loss: 1.453626]\n",
      "59 [D loss: 0.934677, acc.: 48.44%] [G loss: 1.263123]\n",
      "60 [D loss: 0.981780, acc.: 37.50%] [G loss: 1.053819]\n",
      "61 [D loss: 0.736339, acc.: 56.25%] [G loss: 0.947222]\n",
      "62 [D loss: 0.775711, acc.: 50.00%] [G loss: 1.159735]\n",
      "63 [D loss: 0.680309, acc.: 54.69%] [G loss: 1.379249]\n",
      "64 [D loss: 0.707795, acc.: 60.94%] [G loss: 1.228608]\n",
      "65 [D loss: 0.887675, acc.: 43.75%] [G loss: 1.229752]\n",
      "66 [D loss: 0.772856, acc.: 54.69%] [G loss: 1.323551]\n",
      "67 [D loss: 0.850567, acc.: 54.69%] [G loss: 1.185028]\n",
      "68 [D loss: 0.834221, acc.: 53.12%] [G loss: 1.031424]\n",
      "69 [D loss: 0.795014, acc.: 53.12%] [G loss: 1.371155]\n",
      "70 [D loss: 0.624673, acc.: 65.62%] [G loss: 1.164805]\n",
      "71 [D loss: 0.739430, acc.: 53.12%] [G loss: 1.158041]\n",
      "72 [D loss: 0.827163, acc.: 53.12%] [G loss: 1.276024]\n",
      "73 [D loss: 0.815447, acc.: 50.00%] [G loss: 1.411336]\n",
      "74 [D loss: 0.973103, acc.: 46.88%] [G loss: 1.260060]\n",
      "75 [D loss: 0.909933, acc.: 50.00%] [G loss: 1.133453]\n",
      "76 [D loss: 0.800792, acc.: 54.69%] [G loss: 1.308153]\n",
      "77 [D loss: 0.927639, acc.: 45.31%] [G loss: 1.376023]\n",
      "78 [D loss: 0.831793, acc.: 48.44%] [G loss: 1.294024]\n",
      "79 [D loss: 0.886034, acc.: 62.50%] [G loss: 1.369914]\n",
      "80 [D loss: 0.843942, acc.: 56.25%] [G loss: 1.551174]\n",
      "81 [D loss: 0.972997, acc.: 35.94%] [G loss: 1.109806]\n",
      "82 [D loss: 0.942904, acc.: 35.94%] [G loss: 1.154027]\n",
      "83 [D loss: 0.833183, acc.: 46.88%] [G loss: 1.371433]\n",
      "84 [D loss: 0.708754, acc.: 57.81%] [G loss: 1.395849]\n",
      "85 [D loss: 0.823884, acc.: 40.62%] [G loss: 1.310107]\n",
      "86 [D loss: 0.777630, acc.: 53.12%] [G loss: 0.905942]\n",
      "87 [D loss: 0.827634, acc.: 46.88%] [G loss: 1.135625]\n",
      "88 [D loss: 0.896845, acc.: 50.00%] [G loss: 1.225733]\n",
      "89 [D loss: 0.917310, acc.: 43.75%] [G loss: 1.561166]\n",
      "90 [D loss: 0.870882, acc.: 39.06%] [G loss: 1.181666]\n",
      "91 [D loss: 0.756192, acc.: 60.94%] [G loss: 1.125010]\n",
      "92 [D loss: 0.821615, acc.: 53.12%] [G loss: 1.323716]\n",
      "93 [D loss: 0.713279, acc.: 57.81%] [G loss: 1.166212]\n",
      "94 [D loss: 0.776189, acc.: 51.56%] [G loss: 0.939861]\n",
      "95 [D loss: 0.956207, acc.: 43.75%] [G loss: 1.191362]\n",
      "96 [D loss: 0.739248, acc.: 57.81%] [G loss: 1.126628]\n",
      "97 [D loss: 0.895359, acc.: 50.00%] [G loss: 1.122386]\n",
      "98 [D loss: 0.700221, acc.: 65.62%] [G loss: 1.067458]\n",
      "99 [D loss: 0.834684, acc.: 51.56%] [G loss: 1.061150]\n",
      "100 [D loss: 0.694154, acc.: 54.69%] [G loss: 1.397702]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "101 [D loss: 0.754461, acc.: 54.69%] [G loss: 1.482416]\n",
      "102 [D loss: 0.786572, acc.: 50.00%] [G loss: 1.050300]\n",
      "103 [D loss: 1.035224, acc.: 35.94%] [G loss: 0.996497]\n",
      "104 [D loss: 0.814159, acc.: 60.94%] [G loss: 1.375599]\n",
      "105 [D loss: 0.958676, acc.: 43.75%] [G loss: 1.190331]\n",
      "106 [D loss: 0.901205, acc.: 45.31%] [G loss: 1.086064]\n",
      "107 [D loss: 0.795559, acc.: 56.25%] [G loss: 1.228688]\n",
      "108 [D loss: 0.905718, acc.: 40.62%] [G loss: 1.082504]\n",
      "109 [D loss: 0.702186, acc.: 62.50%] [G loss: 1.371636]\n",
      "110 [D loss: 0.968425, acc.: 46.88%] [G loss: 1.392936]\n",
      "111 [D loss: 0.940360, acc.: 35.94%] [G loss: 1.049671]\n",
      "112 [D loss: 0.920555, acc.: 53.12%] [G loss: 1.060680]\n",
      "113 [D loss: 0.804289, acc.: 48.44%] [G loss: 1.074235]\n",
      "114 [D loss: 0.887110, acc.: 45.31%] [G loss: 0.999361]\n",
      "115 [D loss: 0.797374, acc.: 54.69%] [G loss: 1.152849]\n",
      "116 [D loss: 1.061356, acc.: 37.50%] [G loss: 1.080496]\n",
      "117 [D loss: 0.755574, acc.: 53.12%] [G loss: 1.038051]\n",
      "118 [D loss: 0.855938, acc.: 37.50%] [G loss: 1.314746]\n",
      "119 [D loss: 0.921770, acc.: 40.62%] [G loss: 1.294904]\n",
      "120 [D loss: 0.996326, acc.: 35.94%] [G loss: 1.432713]\n",
      "121 [D loss: 0.954304, acc.: 45.31%] [G loss: 1.228190]\n",
      "122 [D loss: 0.889023, acc.: 40.62%] [G loss: 1.037260]\n",
      "123 [D loss: 0.828769, acc.: 50.00%] [G loss: 1.450430]\n",
      "124 [D loss: 0.835875, acc.: 51.56%] [G loss: 1.070613]\n",
      "125 [D loss: 0.713956, acc.: 59.38%] [G loss: 1.133240]\n",
      "126 [D loss: 0.876683, acc.: 56.25%] [G loss: 1.136195]\n",
      "127 [D loss: 0.920372, acc.: 39.06%] [G loss: 1.035112]\n",
      "128 [D loss: 0.732126, acc.: 53.12%] [G loss: 1.191582]\n",
      "129 [D loss: 0.770740, acc.: 51.56%] [G loss: 1.308221]\n",
      "130 [D loss: 0.616666, acc.: 70.31%] [G loss: 1.162856]\n",
      "131 [D loss: 0.982351, acc.: 37.50%] [G loss: 0.894933]\n",
      "132 [D loss: 1.031230, acc.: 37.50%] [G loss: 0.985162]\n",
      "133 [D loss: 0.905038, acc.: 43.75%] [G loss: 0.936948]\n",
      "134 [D loss: 0.827381, acc.: 46.88%] [G loss: 1.035166]\n",
      "135 [D loss: 0.647498, acc.: 62.50%] [G loss: 1.302134]\n",
      "136 [D loss: 0.838115, acc.: 53.12%] [G loss: 1.168652]\n",
      "137 [D loss: 0.935881, acc.: 34.38%] [G loss: 1.115892]\n",
      "138 [D loss: 0.961362, acc.: 35.94%] [G loss: 1.042406]\n",
      "139 [D loss: 0.804031, acc.: 46.88%] [G loss: 1.123596]\n",
      "140 [D loss: 0.771512, acc.: 54.69%] [G loss: 1.219295]\n",
      "141 [D loss: 0.955250, acc.: 40.62%] [G loss: 1.228168]\n",
      "142 [D loss: 0.781180, acc.: 56.25%] [G loss: 1.148863]\n",
      "143 [D loss: 0.950426, acc.: 50.00%] [G loss: 1.255696]\n",
      "144 [D loss: 1.004264, acc.: 34.38%] [G loss: 1.145047]\n",
      "145 [D loss: 0.898261, acc.: 45.31%] [G loss: 0.906661]\n",
      "146 [D loss: 0.922533, acc.: 40.62%] [G loss: 0.980925]\n",
      "147 [D loss: 0.785853, acc.: 50.00%] [G loss: 0.969180]\n",
      "148 [D loss: 0.791611, acc.: 46.88%] [G loss: 1.038770]\n",
      "149 [D loss: 0.853561, acc.: 45.31%] [G loss: 1.104248]\n",
      "150 [D loss: 0.748944, acc.: 53.12%] [G loss: 1.111964]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "151 [D loss: 0.844761, acc.: 48.44%] [G loss: 0.970511]\n",
      "152 [D loss: 0.931384, acc.: 40.62%] [G loss: 1.099442]\n",
      "153 [D loss: 0.832852, acc.: 53.12%] [G loss: 1.279373]\n",
      "154 [D loss: 0.851263, acc.: 46.88%] [G loss: 1.324276]\n",
      "155 [D loss: 0.988656, acc.: 42.19%] [G loss: 0.943160]\n",
      "156 [D loss: 0.715444, acc.: 56.25%] [G loss: 1.143142]\n",
      "157 [D loss: 0.933314, acc.: 39.06%] [G loss: 0.976075]\n",
      "158 [D loss: 0.773857, acc.: 51.56%] [G loss: 1.180069]\n",
      "159 [D loss: 0.941497, acc.: 51.56%] [G loss: 1.070806]\n",
      "160 [D loss: 0.786842, acc.: 53.12%] [G loss: 1.113872]\n",
      "161 [D loss: 0.859349, acc.: 51.56%] [G loss: 1.194435]\n",
      "162 [D loss: 0.715265, acc.: 57.81%] [G loss: 1.173456]\n",
      "163 [D loss: 0.914330, acc.: 39.06%] [G loss: 1.125003]\n",
      "164 [D loss: 0.761285, acc.: 53.12%] [G loss: 0.971721]\n",
      "165 [D loss: 0.852003, acc.: 50.00%] [G loss: 1.125965]\n",
      "166 [D loss: 0.767923, acc.: 54.69%] [G loss: 1.238198]\n",
      "167 [D loss: 0.932945, acc.: 39.06%] [G loss: 1.115414]\n",
      "168 [D loss: 0.874398, acc.: 40.62%] [G loss: 1.125116]\n",
      "169 [D loss: 0.882641, acc.: 46.88%] [G loss: 0.989035]\n",
      "170 [D loss: 0.917572, acc.: 50.00%] [G loss: 1.196666]\n",
      "171 [D loss: 0.762154, acc.: 48.44%] [G loss: 1.068002]\n",
      "172 [D loss: 0.867898, acc.: 48.44%] [G loss: 1.196579]\n",
      "173 [D loss: 0.801712, acc.: 51.56%] [G loss: 0.977842]\n",
      "174 [D loss: 0.792581, acc.: 50.00%] [G loss: 0.885189]\n",
      "175 [D loss: 0.791877, acc.: 50.00%] [G loss: 0.974870]\n",
      "176 [D loss: 0.932284, acc.: 35.94%] [G loss: 0.990946]\n",
      "177 [D loss: 0.953293, acc.: 42.19%] [G loss: 1.272471]\n",
      "178 [D loss: 0.798755, acc.: 43.75%] [G loss: 1.246191]\n",
      "179 [D loss: 0.896065, acc.: 46.88%] [G loss: 1.000047]\n",
      "180 [D loss: 0.877400, acc.: 48.44%] [G loss: 0.865645]\n",
      "181 [D loss: 0.829848, acc.: 48.44%] [G loss: 1.018187]\n",
      "182 [D loss: 0.788849, acc.: 46.88%] [G loss: 1.032473]\n",
      "183 [D loss: 0.969988, acc.: 37.50%] [G loss: 1.049235]\n",
      "184 [D loss: 0.699756, acc.: 59.38%] [G loss: 1.315246]\n",
      "185 [D loss: 0.872616, acc.: 43.75%] [G loss: 1.142732]\n",
      "186 [D loss: 0.879960, acc.: 40.62%] [G loss: 1.092854]\n",
      "187 [D loss: 0.662363, acc.: 57.81%] [G loss: 1.214900]\n",
      "188 [D loss: 0.860914, acc.: 51.56%] [G loss: 1.165359]\n",
      "189 [D loss: 0.825395, acc.: 48.44%] [G loss: 1.256773]\n",
      "190 [D loss: 0.792742, acc.: 48.44%] [G loss: 1.055468]\n",
      "191 [D loss: 0.892861, acc.: 40.62%] [G loss: 1.102836]\n",
      "192 [D loss: 0.844609, acc.: 39.06%] [G loss: 1.088936]\n",
      "193 [D loss: 0.751501, acc.: 53.12%] [G loss: 1.186825]\n",
      "194 [D loss: 0.803702, acc.: 53.12%] [G loss: 1.123489]\n",
      "195 [D loss: 0.724508, acc.: 59.38%] [G loss: 1.265933]\n",
      "196 [D loss: 0.884472, acc.: 45.31%] [G loss: 1.132317]\n",
      "197 [D loss: 0.928225, acc.: 51.56%] [G loss: 1.118395]\n",
      "198 [D loss: 0.898497, acc.: 40.62%] [G loss: 1.032289]\n",
      "199 [D loss: 0.829191, acc.: 50.00%] [G loss: 0.947091]\n",
      "200 [D loss: 0.732556, acc.: 54.69%] [G loss: 1.255406]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "201 [D loss: 0.759319, acc.: 50.00%] [G loss: 1.161859]\n",
      "202 [D loss: 0.827618, acc.: 40.62%] [G loss: 0.860253]\n",
      "203 [D loss: 0.853381, acc.: 48.44%] [G loss: 0.956114]\n",
      "204 [D loss: 0.790265, acc.: 42.19%] [G loss: 1.136240]\n",
      "205 [D loss: 0.837122, acc.: 50.00%] [G loss: 0.906766]\n",
      "206 [D loss: 0.680076, acc.: 57.81%] [G loss: 1.105986]\n",
      "207 [D loss: 0.787507, acc.: 53.12%] [G loss: 1.175339]\n",
      "208 [D loss: 0.896727, acc.: 40.62%] [G loss: 0.827704]\n",
      "209 [D loss: 0.774841, acc.: 50.00%] [G loss: 1.366165]\n",
      "210 [D loss: 0.853687, acc.: 43.75%] [G loss: 1.098299]\n",
      "211 [D loss: 0.804255, acc.: 54.69%] [G loss: 1.020526]\n",
      "212 [D loss: 0.862979, acc.: 40.62%] [G loss: 0.964512]\n",
      "213 [D loss: 0.826794, acc.: 45.31%] [G loss: 1.155730]\n",
      "214 [D loss: 0.712211, acc.: 59.38%] [G loss: 0.894233]\n",
      "215 [D loss: 0.880155, acc.: 45.31%] [G loss: 1.010694]\n",
      "216 [D loss: 0.748227, acc.: 53.12%] [G loss: 0.939369]\n",
      "217 [D loss: 0.769271, acc.: 56.25%] [G loss: 1.130071]\n",
      "218 [D loss: 0.875214, acc.: 46.88%] [G loss: 0.879805]\n",
      "219 [D loss: 0.841832, acc.: 51.56%] [G loss: 1.222340]\n",
      "220 [D loss: 0.731929, acc.: 53.12%] [G loss: 0.864700]\n",
      "221 [D loss: 0.728226, acc.: 51.56%] [G loss: 0.972684]\n",
      "222 [D loss: 0.796456, acc.: 54.69%] [G loss: 1.144140]\n",
      "223 [D loss: 0.892371, acc.: 42.19%] [G loss: 0.928831]\n",
      "224 [D loss: 0.847021, acc.: 53.12%] [G loss: 0.984389]\n",
      "225 [D loss: 0.911071, acc.: 35.94%] [G loss: 1.273501]\n",
      "226 [D loss: 0.889662, acc.: 37.50%] [G loss: 1.116349]\n",
      "227 [D loss: 0.929005, acc.: 31.25%] [G loss: 1.028156]\n",
      "228 [D loss: 0.660662, acc.: 59.38%] [G loss: 1.115741]\n",
      "229 [D loss: 0.867649, acc.: 43.75%] [G loss: 1.145458]\n",
      "230 [D loss: 1.038218, acc.: 32.81%] [G loss: 1.114817]\n",
      "231 [D loss: 0.748426, acc.: 53.12%] [G loss: 0.930567]\n",
      "232 [D loss: 0.750863, acc.: 51.56%] [G loss: 0.917854]\n",
      "233 [D loss: 0.847143, acc.: 50.00%] [G loss: 0.785897]\n",
      "234 [D loss: 0.844830, acc.: 48.44%] [G loss: 0.922573]\n",
      "235 [D loss: 0.842125, acc.: 46.88%] [G loss: 1.015812]\n",
      "236 [D loss: 0.750201, acc.: 50.00%] [G loss: 0.940029]\n",
      "237 [D loss: 0.747959, acc.: 43.75%] [G loss: 1.101250]\n",
      "238 [D loss: 0.799258, acc.: 51.56%] [G loss: 0.977142]\n",
      "239 [D loss: 0.829682, acc.: 48.44%] [G loss: 0.947313]\n",
      "240 [D loss: 0.896605, acc.: 43.75%] [G loss: 1.004451]\n",
      "241 [D loss: 0.758352, acc.: 50.00%] [G loss: 1.176156]\n",
      "242 [D loss: 0.705741, acc.: 57.81%] [G loss: 0.940096]\n",
      "243 [D loss: 0.858832, acc.: 45.31%] [G loss: 1.024442]\n",
      "244 [D loss: 0.795072, acc.: 46.88%] [G loss: 1.129294]\n",
      "245 [D loss: 0.855490, acc.: 51.56%] [G loss: 1.008402]\n",
      "246 [D loss: 0.760611, acc.: 54.69%] [G loss: 1.007524]\n",
      "247 [D loss: 0.891278, acc.: 40.62%] [G loss: 0.871750]\n",
      "248 [D loss: 0.737754, acc.: 54.69%] [G loss: 1.028100]\n",
      "249 [D loss: 0.872333, acc.: 40.62%] [G loss: 1.021065]\n",
      "250 [D loss: 0.942977, acc.: 43.75%] [G loss: 1.032304]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "251 [D loss: 0.835830, acc.: 42.19%] [G loss: 1.077575]\n",
      "252 [D loss: 0.843634, acc.: 46.88%] [G loss: 1.012619]\n",
      "253 [D loss: 0.613018, acc.: 70.31%] [G loss: 1.092211]\n",
      "254 [D loss: 0.785151, acc.: 51.56%] [G loss: 1.016489]\n",
      "255 [D loss: 0.748672, acc.: 56.25%] [G loss: 1.059828]\n",
      "256 [D loss: 0.728359, acc.: 56.25%] [G loss: 1.192911]\n",
      "257 [D loss: 0.806990, acc.: 56.25%] [G loss: 1.121317]\n",
      "258 [D loss: 0.882802, acc.: 51.56%] [G loss: 1.191435]\n",
      "259 [D loss: 0.967382, acc.: 39.06%] [G loss: 0.972131]\n",
      "260 [D loss: 0.865108, acc.: 43.75%] [G loss: 1.037180]\n",
      "261 [D loss: 0.891224, acc.: 42.19%] [G loss: 1.146704]\n",
      "262 [D loss: 0.780355, acc.: 53.12%] [G loss: 0.928310]\n",
      "263 [D loss: 0.930476, acc.: 37.50%] [G loss: 1.060438]\n",
      "264 [D loss: 0.824130, acc.: 45.31%] [G loss: 1.126043]\n",
      "265 [D loss: 0.711673, acc.: 54.69%] [G loss: 1.114061]\n",
      "266 [D loss: 0.861943, acc.: 45.31%] [G loss: 1.080077]\n",
      "267 [D loss: 0.902989, acc.: 43.75%] [G loss: 1.320860]\n",
      "268 [D loss: 0.828226, acc.: 46.88%] [G loss: 1.078389]\n",
      "269 [D loss: 0.757526, acc.: 51.56%] [G loss: 0.999748]\n",
      "270 [D loss: 0.654082, acc.: 57.81%] [G loss: 1.154185]\n",
      "271 [D loss: 0.874477, acc.: 45.31%] [G loss: 1.166609]\n",
      "272 [D loss: 0.823324, acc.: 48.44%] [G loss: 1.147539]\n",
      "273 [D loss: 0.814619, acc.: 48.44%] [G loss: 0.967455]\n",
      "274 [D loss: 0.844131, acc.: 48.44%] [G loss: 1.128785]\n",
      "275 [D loss: 0.764346, acc.: 51.56%] [G loss: 1.023223]\n",
      "276 [D loss: 0.765636, acc.: 53.12%] [G loss: 1.316169]\n",
      "277 [D loss: 0.990540, acc.: 25.00%] [G loss: 1.266416]\n",
      "278 [D loss: 0.847495, acc.: 50.00%] [G loss: 1.259149]\n",
      "279 [D loss: 0.789965, acc.: 50.00%] [G loss: 1.088099]\n",
      "280 [D loss: 0.845694, acc.: 48.44%] [G loss: 0.987572]\n",
      "281 [D loss: 0.748420, acc.: 54.69%] [G loss: 0.915099]\n",
      "282 [D loss: 0.826527, acc.: 48.44%] [G loss: 0.943174]\n",
      "283 [D loss: 0.777208, acc.: 46.88%] [G loss: 1.079855]\n",
      "284 [D loss: 0.829889, acc.: 48.44%] [G loss: 0.976753]\n",
      "285 [D loss: 0.757997, acc.: 53.12%] [G loss: 0.955855]\n",
      "286 [D loss: 0.693266, acc.: 67.19%] [G loss: 1.088948]\n",
      "287 [D loss: 0.740890, acc.: 59.38%] [G loss: 1.096529]\n",
      "288 [D loss: 0.838937, acc.: 45.31%] [G loss: 0.998582]\n",
      "289 [D loss: 0.760101, acc.: 48.44%] [G loss: 1.145444]\n",
      "290 [D loss: 0.984519, acc.: 31.25%] [G loss: 0.852322]\n",
      "291 [D loss: 0.770162, acc.: 53.12%] [G loss: 1.091838]\n",
      "292 [D loss: 0.789103, acc.: 56.25%] [G loss: 1.287649]\n",
      "293 [D loss: 0.790764, acc.: 45.31%] [G loss: 1.115452]\n",
      "294 [D loss: 0.795338, acc.: 45.31%] [G loss: 1.058668]\n",
      "295 [D loss: 0.773731, acc.: 51.56%] [G loss: 1.122325]\n",
      "296 [D loss: 0.717663, acc.: 60.94%] [G loss: 1.155776]\n",
      "297 [D loss: 0.773922, acc.: 50.00%] [G loss: 0.993177]\n",
      "298 [D loss: 0.878750, acc.: 40.62%] [G loss: 1.079821]\n",
      "299 [D loss: 0.703931, acc.: 50.00%] [G loss: 0.888477]\n",
      "300 [D loss: 0.937624, acc.: 34.38%] [G loss: 0.923746]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "301 [D loss: 0.798127, acc.: 48.44%] [G loss: 0.873092]\n",
      "302 [D loss: 0.819084, acc.: 51.56%] [G loss: 0.988483]\n",
      "303 [D loss: 0.803435, acc.: 53.12%] [G loss: 1.001419]\n",
      "304 [D loss: 0.719439, acc.: 56.25%] [G loss: 1.083419]\n",
      "305 [D loss: 0.690826, acc.: 54.69%] [G loss: 0.856032]\n",
      "306 [D loss: 0.850094, acc.: 48.44%] [G loss: 1.125762]\n",
      "307 [D loss: 0.752082, acc.: 54.69%] [G loss: 1.104103]\n",
      "308 [D loss: 0.779320, acc.: 48.44%] [G loss: 1.150999]\n",
      "309 [D loss: 0.782452, acc.: 48.44%] [G loss: 1.108014]\n",
      "310 [D loss: 0.706859, acc.: 65.62%] [G loss: 1.191576]\n",
      "311 [D loss: 0.804014, acc.: 48.44%] [G loss: 1.229023]\n",
      "312 [D loss: 0.800470, acc.: 50.00%] [G loss: 1.030686]\n",
      "313 [D loss: 0.837679, acc.: 45.31%] [G loss: 0.978150]\n",
      "314 [D loss: 0.859542, acc.: 48.44%] [G loss: 1.100755]\n",
      "315 [D loss: 0.760972, acc.: 51.56%] [G loss: 1.015291]\n",
      "316 [D loss: 0.855171, acc.: 46.88%] [G loss: 1.160545]\n",
      "317 [D loss: 0.881649, acc.: 39.06%] [G loss: 1.069161]\n",
      "318 [D loss: 0.754051, acc.: 54.69%] [G loss: 0.992762]\n",
      "319 [D loss: 0.775769, acc.: 48.44%] [G loss: 0.812140]\n",
      "320 [D loss: 0.738761, acc.: 54.69%] [G loss: 0.955671]\n",
      "321 [D loss: 0.809166, acc.: 48.44%] [G loss: 1.100662]\n",
      "322 [D loss: 0.769188, acc.: 48.44%] [G loss: 1.102599]\n",
      "323 [D loss: 0.691949, acc.: 56.25%] [G loss: 0.957122]\n",
      "324 [D loss: 0.719612, acc.: 50.00%] [G loss: 1.132152]\n",
      "325 [D loss: 0.636660, acc.: 67.19%] [G loss: 1.071634]\n",
      "326 [D loss: 0.731972, acc.: 51.56%] [G loss: 1.032573]\n",
      "327 [D loss: 0.851274, acc.: 45.31%] [G loss: 1.020579]\n",
      "328 [D loss: 0.806853, acc.: 53.12%] [G loss: 0.994893]\n",
      "329 [D loss: 0.863829, acc.: 45.31%] [G loss: 1.012047]\n",
      "330 [D loss: 0.719319, acc.: 60.94%] [G loss: 0.914272]\n",
      "331 [D loss: 0.810144, acc.: 51.56%] [G loss: 1.027853]\n",
      "332 [D loss: 0.832963, acc.: 40.62%] [G loss: 1.241702]\n",
      "333 [D loss: 0.811118, acc.: 51.56%] [G loss: 1.226892]\n",
      "334 [D loss: 0.883193, acc.: 35.94%] [G loss: 1.164194]\n",
      "335 [D loss: 0.769594, acc.: 56.25%] [G loss: 1.013802]\n",
      "336 [D loss: 0.842147, acc.: 40.62%] [G loss: 0.940585]\n",
      "337 [D loss: 0.944826, acc.: 35.94%] [G loss: 1.095301]\n",
      "338 [D loss: 0.795659, acc.: 46.88%] [G loss: 0.916415]\n",
      "339 [D loss: 0.738685, acc.: 62.50%] [G loss: 1.053347]\n",
      "340 [D loss: 0.882287, acc.: 32.81%] [G loss: 1.141083]\n",
      "341 [D loss: 0.777876, acc.: 48.44%] [G loss: 0.940756]\n",
      "342 [D loss: 0.804956, acc.: 50.00%] [G loss: 1.094102]\n",
      "343 [D loss: 0.793989, acc.: 42.19%] [G loss: 1.029372]\n",
      "344 [D loss: 0.798447, acc.: 45.31%] [G loss: 1.259141]\n",
      "345 [D loss: 0.763868, acc.: 45.31%] [G loss: 1.170837]\n",
      "346 [D loss: 0.729855, acc.: 56.25%] [G loss: 1.036477]\n",
      "347 [D loss: 0.800647, acc.: 50.00%] [G loss: 0.873879]\n",
      "348 [D loss: 0.937310, acc.: 35.94%] [G loss: 0.974577]\n",
      "349 [D loss: 0.708746, acc.: 57.81%] [G loss: 1.003724]\n",
      "350 [D loss: 0.764135, acc.: 50.00%] [G loss: 1.146051]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "351 [D loss: 0.950258, acc.: 34.38%] [G loss: 0.958050]\n",
      "352 [D loss: 0.761460, acc.: 50.00%] [G loss: 0.809566]\n",
      "353 [D loss: 0.762222, acc.: 46.88%] [G loss: 0.951172]\n",
      "354 [D loss: 0.871585, acc.: 53.12%] [G loss: 1.031946]\n",
      "355 [D loss: 0.734977, acc.: 56.25%] [G loss: 0.842831]\n",
      "356 [D loss: 0.627916, acc.: 68.75%] [G loss: 0.926895]\n",
      "357 [D loss: 0.825099, acc.: 45.31%] [G loss: 0.946035]\n",
      "358 [D loss: 0.793396, acc.: 46.88%] [G loss: 1.059463]\n",
      "359 [D loss: 0.760952, acc.: 54.69%] [G loss: 1.073712]\n",
      "360 [D loss: 0.724979, acc.: 51.56%] [G loss: 0.980681]\n",
      "361 [D loss: 0.675323, acc.: 56.25%] [G loss: 1.062652]\n",
      "362 [D loss: 0.819120, acc.: 53.12%] [G loss: 0.909245]\n",
      "363 [D loss: 0.886594, acc.: 42.19%] [G loss: 1.112880]\n",
      "364 [D loss: 0.854416, acc.: 51.56%] [G loss: 0.983630]\n",
      "365 [D loss: 0.723520, acc.: 51.56%] [G loss: 0.909813]\n",
      "366 [D loss: 0.826813, acc.: 48.44%] [G loss: 1.241331]\n",
      "367 [D loss: 0.766284, acc.: 56.25%] [G loss: 0.934288]\n",
      "368 [D loss: 0.794402, acc.: 53.12%] [G loss: 1.203147]\n",
      "369 [D loss: 0.817303, acc.: 43.75%] [G loss: 1.066553]\n",
      "370 [D loss: 0.840566, acc.: 48.44%] [G loss: 1.111794]\n",
      "371 [D loss: 0.817132, acc.: 43.75%] [G loss: 1.111069]\n",
      "372 [D loss: 0.732098, acc.: 51.56%] [G loss: 0.910269]\n",
      "373 [D loss: 0.762654, acc.: 51.56%] [G loss: 1.164933]\n",
      "374 [D loss: 0.815796, acc.: 50.00%] [G loss: 0.888822]\n",
      "375 [D loss: 0.855230, acc.: 43.75%] [G loss: 1.075423]\n",
      "376 [D loss: 0.822101, acc.: 40.62%] [G loss: 1.030480]\n",
      "377 [D loss: 0.788506, acc.: 48.44%] [G loss: 1.089357]\n",
      "378 [D loss: 0.706130, acc.: 64.06%] [G loss: 0.911198]\n",
      "379 [D loss: 0.800765, acc.: 45.31%] [G loss: 0.927567]\n",
      "380 [D loss: 0.705755, acc.: 59.38%] [G loss: 1.034529]\n",
      "381 [D loss: 0.677256, acc.: 57.81%] [G loss: 0.973770]\n",
      "382 [D loss: 0.702906, acc.: 57.81%] [G loss: 1.063073]\n",
      "383 [D loss: 0.713541, acc.: 65.62%] [G loss: 1.082791]\n",
      "384 [D loss: 0.746221, acc.: 48.44%] [G loss: 0.951094]\n",
      "385 [D loss: 0.746019, acc.: 50.00%] [G loss: 0.903941]\n",
      "386 [D loss: 0.782750, acc.: 48.44%] [G loss: 0.976850]\n",
      "387 [D loss: 0.771304, acc.: 56.25%] [G loss: 1.094657]\n",
      "388 [D loss: 0.886093, acc.: 35.94%] [G loss: 1.055445]\n",
      "389 [D loss: 0.783318, acc.: 45.31%] [G loss: 0.871175]\n",
      "390 [D loss: 0.860218, acc.: 45.31%] [G loss: 0.954495]\n",
      "391 [D loss: 0.804668, acc.: 40.62%] [G loss: 1.005323]\n",
      "392 [D loss: 0.703622, acc.: 53.12%] [G loss: 1.108275]\n",
      "393 [D loss: 0.735563, acc.: 51.56%] [G loss: 0.987022]\n",
      "394 [D loss: 0.661408, acc.: 60.94%] [G loss: 0.939772]\n",
      "395 [D loss: 0.743693, acc.: 54.69%] [G loss: 0.972162]\n",
      "396 [D loss: 0.776165, acc.: 48.44%] [G loss: 1.020823]\n",
      "397 [D loss: 0.749685, acc.: 50.00%] [G loss: 1.008275]\n",
      "398 [D loss: 0.810808, acc.: 54.69%] [G loss: 1.070579]\n",
      "399 [D loss: 0.746955, acc.: 46.88%] [G loss: 1.013956]\n",
      "400 [D loss: 0.757460, acc.: 54.69%] [G loss: 1.119295]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "401 [D loss: 0.832396, acc.: 42.19%] [G loss: 1.000412]\n",
      "402 [D loss: 0.885042, acc.: 39.06%] [G loss: 1.032431]\n",
      "403 [D loss: 0.722241, acc.: 60.94%] [G loss: 1.224926]\n",
      "404 [D loss: 0.653238, acc.: 57.81%] [G loss: 1.048675]\n",
      "405 [D loss: 0.737599, acc.: 56.25%] [G loss: 1.063708]\n",
      "406 [D loss: 0.858294, acc.: 45.31%] [G loss: 1.149261]\n",
      "407 [D loss: 0.782651, acc.: 50.00%] [G loss: 0.982848]\n",
      "408 [D loss: 0.875915, acc.: 37.50%] [G loss: 1.091011]\n",
      "409 [D loss: 0.688493, acc.: 56.25%] [G loss: 0.959010]\n",
      "410 [D loss: 0.751028, acc.: 45.31%] [G loss: 0.868031]\n",
      "411 [D loss: 0.735638, acc.: 54.69%] [G loss: 1.038016]\n",
      "412 [D loss: 0.715826, acc.: 62.50%] [G loss: 0.898142]\n",
      "413 [D loss: 0.755422, acc.: 46.88%] [G loss: 0.980474]\n",
      "414 [D loss: 0.782622, acc.: 45.31%] [G loss: 0.958597]\n",
      "415 [D loss: 0.689463, acc.: 60.94%] [G loss: 1.076060]\n",
      "416 [D loss: 0.762430, acc.: 51.56%] [G loss: 0.863095]\n",
      "417 [D loss: 0.751906, acc.: 56.25%] [G loss: 0.933130]\n",
      "418 [D loss: 0.810086, acc.: 45.31%] [G loss: 0.892872]\n",
      "419 [D loss: 0.857337, acc.: 42.19%] [G loss: 0.799201]\n",
      "420 [D loss: 0.769556, acc.: 54.69%] [G loss: 0.925754]\n",
      "421 [D loss: 0.703196, acc.: 53.12%] [G loss: 0.918811]\n",
      "422 [D loss: 0.775436, acc.: 45.31%] [G loss: 1.177957]\n",
      "423 [D loss: 0.805182, acc.: 45.31%] [G loss: 0.804394]\n",
      "424 [D loss: 0.886116, acc.: 39.06%] [G loss: 0.867992]\n",
      "425 [D loss: 0.830101, acc.: 45.31%] [G loss: 0.984110]\n",
      "426 [D loss: 0.701646, acc.: 51.56%] [G loss: 1.129396]\n",
      "427 [D loss: 0.793357, acc.: 48.44%] [G loss: 1.036125]\n",
      "428 [D loss: 0.764609, acc.: 51.56%] [G loss: 1.041622]\n",
      "429 [D loss: 0.829120, acc.: 43.75%] [G loss: 1.023016]\n",
      "430 [D loss: 0.798723, acc.: 43.75%] [G loss: 1.204493]\n",
      "431 [D loss: 0.813734, acc.: 40.62%] [G loss: 1.018577]\n",
      "432 [D loss: 0.752094, acc.: 53.12%] [G loss: 1.044358]\n",
      "433 [D loss: 0.665705, acc.: 62.50%] [G loss: 1.015291]\n",
      "434 [D loss: 0.734913, acc.: 51.56%] [G loss: 1.086743]\n",
      "435 [D loss: 0.720715, acc.: 50.00%] [G loss: 0.990348]\n",
      "436 [D loss: 0.762036, acc.: 51.56%] [G loss: 1.052984]\n",
      "437 [D loss: 0.804964, acc.: 39.06%] [G loss: 0.980200]\n",
      "438 [D loss: 0.824448, acc.: 48.44%] [G loss: 1.051781]\n",
      "439 [D loss: 0.927273, acc.: 34.38%] [G loss: 0.895215]\n",
      "440 [D loss: 0.760899, acc.: 46.88%] [G loss: 0.968598]\n",
      "441 [D loss: 0.750121, acc.: 57.81%] [G loss: 1.203508]\n",
      "442 [D loss: 0.705547, acc.: 56.25%] [G loss: 0.975537]\n",
      "443 [D loss: 0.758373, acc.: 46.88%] [G loss: 1.246449]\n",
      "444 [D loss: 0.915234, acc.: 29.69%] [G loss: 0.903803]\n",
      "445 [D loss: 0.856416, acc.: 42.19%] [G loss: 0.967946]\n",
      "446 [D loss: 0.732627, acc.: 53.12%] [G loss: 1.000643]\n",
      "447 [D loss: 0.793818, acc.: 46.88%] [G loss: 1.301213]\n",
      "448 [D loss: 0.893786, acc.: 43.75%] [G loss: 1.178597]\n",
      "449 [D loss: 0.800123, acc.: 48.44%] [G loss: 1.095916]\n",
      "450 [D loss: 0.843577, acc.: 43.75%] [G loss: 0.959927]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "451 [D loss: 0.734317, acc.: 62.50%] [G loss: 1.109020]\n",
      "452 [D loss: 0.763818, acc.: 50.00%] [G loss: 0.986249]\n",
      "453 [D loss: 0.756954, acc.: 45.31%] [G loss: 0.819330]\n",
      "454 [D loss: 0.796521, acc.: 45.31%] [G loss: 1.133649]\n",
      "455 [D loss: 0.724367, acc.: 48.44%] [G loss: 0.897291]\n",
      "456 [D loss: 0.728925, acc.: 56.25%] [G loss: 0.926134]\n",
      "457 [D loss: 0.936564, acc.: 28.12%] [G loss: 1.027141]\n",
      "458 [D loss: 0.649966, acc.: 59.38%] [G loss: 1.179910]\n",
      "459 [D loss: 0.739413, acc.: 59.38%] [G loss: 1.139560]\n",
      "460 [D loss: 0.710692, acc.: 54.69%] [G loss: 1.147865]\n",
      "461 [D loss: 0.711539, acc.: 50.00%] [G loss: 0.900837]\n",
      "462 [D loss: 0.786096, acc.: 46.88%] [G loss: 0.737787]\n",
      "463 [D loss: 0.805068, acc.: 40.62%] [G loss: 0.852125]\n",
      "464 [D loss: 0.729531, acc.: 56.25%] [G loss: 0.830197]\n",
      "465 [D loss: 0.701747, acc.: 57.81%] [G loss: 0.935313]\n",
      "466 [D loss: 0.718991, acc.: 51.56%] [G loss: 1.001300]\n",
      "467 [D loss: 0.699309, acc.: 59.38%] [G loss: 1.090162]\n",
      "468 [D loss: 0.780176, acc.: 48.44%] [G loss: 0.895859]\n",
      "469 [D loss: 0.644874, acc.: 65.62%] [G loss: 0.962020]\n",
      "470 [D loss: 0.723041, acc.: 53.12%] [G loss: 1.043724]\n",
      "471 [D loss: 0.882113, acc.: 31.25%] [G loss: 0.902218]\n",
      "472 [D loss: 0.757001, acc.: 56.25%] [G loss: 0.888929]\n",
      "473 [D loss: 0.744403, acc.: 51.56%] [G loss: 1.083187]\n",
      "474 [D loss: 0.822820, acc.: 43.75%] [G loss: 0.942986]\n",
      "475 [D loss: 0.777057, acc.: 45.31%] [G loss: 1.024974]\n",
      "476 [D loss: 0.791967, acc.: 42.19%] [G loss: 1.071347]\n",
      "477 [D loss: 0.834092, acc.: 50.00%] [G loss: 1.005800]\n",
      "478 [D loss: 0.760442, acc.: 56.25%] [G loss: 1.140928]\n",
      "479 [D loss: 0.775803, acc.: 54.69%] [G loss: 0.991961]\n",
      "480 [D loss: 0.796472, acc.: 51.56%] [G loss: 1.009339]\n",
      "481 [D loss: 0.736995, acc.: 53.12%] [G loss: 1.005256]\n",
      "482 [D loss: 0.733569, acc.: 56.25%] [G loss: 1.035011]\n",
      "483 [D loss: 0.670735, acc.: 62.50%] [G loss: 1.027889]\n",
      "484 [D loss: 0.839067, acc.: 39.06%] [G loss: 0.975497]\n",
      "485 [D loss: 0.717337, acc.: 56.25%] [G loss: 1.114486]\n",
      "486 [D loss: 0.740829, acc.: 51.56%] [G loss: 1.005462]\n",
      "487 [D loss: 0.709161, acc.: 53.12%] [G loss: 1.164342]\n",
      "488 [D loss: 0.704208, acc.: 60.94%] [G loss: 1.064378]\n",
      "489 [D loss: 0.714462, acc.: 56.25%] [G loss: 0.824470]\n",
      "490 [D loss: 0.749236, acc.: 48.44%] [G loss: 0.894789]\n",
      "491 [D loss: 0.737733, acc.: 50.00%] [G loss: 1.011312]\n",
      "492 [D loss: 0.884091, acc.: 43.75%] [G loss: 0.923923]\n",
      "493 [D loss: 0.848266, acc.: 42.19%] [G loss: 1.114427]\n",
      "494 [D loss: 0.736971, acc.: 45.31%] [G loss: 0.926346]\n",
      "495 [D loss: 0.822549, acc.: 42.19%] [G loss: 1.101509]\n",
      "496 [D loss: 0.816117, acc.: 42.19%] [G loss: 0.984352]\n",
      "497 [D loss: 0.794217, acc.: 43.75%] [G loss: 1.001656]\n",
      "498 [D loss: 0.721348, acc.: 51.56%] [G loss: 1.017950]\n",
      "499 [D loss: 0.886270, acc.: 39.06%] [G loss: 0.812528]\n",
      "500 [D loss: 0.753728, acc.: 45.31%] [G loss: 0.932678]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "501 [D loss: 0.708663, acc.: 54.69%] [G loss: 1.230209]\n",
      "502 [D loss: 0.680470, acc.: 60.94%] [G loss: 1.069166]\n",
      "503 [D loss: 0.850379, acc.: 51.56%] [G loss: 1.142737]\n",
      "504 [D loss: 0.753854, acc.: 56.25%] [G loss: 1.060204]\n",
      "505 [D loss: 0.772555, acc.: 46.88%] [G loss: 1.019768]\n",
      "506 [D loss: 0.748103, acc.: 56.25%] [G loss: 0.969312]\n",
      "507 [D loss: 0.729550, acc.: 65.62%] [G loss: 0.916525]\n",
      "508 [D loss: 0.681040, acc.: 60.94%] [G loss: 1.238924]\n",
      "509 [D loss: 0.838135, acc.: 37.50%] [G loss: 0.929654]\n",
      "510 [D loss: 0.756758, acc.: 50.00%] [G loss: 0.955870]\n",
      "511 [D loss: 0.800240, acc.: 42.19%] [G loss: 1.096838]\n",
      "512 [D loss: 0.786444, acc.: 53.12%] [G loss: 0.795085]\n",
      "513 [D loss: 0.774364, acc.: 51.56%] [G loss: 1.049462]\n",
      "514 [D loss: 0.700788, acc.: 60.94%] [G loss: 1.001398]\n",
      "515 [D loss: 0.672997, acc.: 62.50%] [G loss: 1.145212]\n",
      "516 [D loss: 0.858561, acc.: 40.62%] [G loss: 0.962400]\n",
      "517 [D loss: 0.741905, acc.: 51.56%] [G loss: 0.860152]\n",
      "518 [D loss: 0.735514, acc.: 56.25%] [G loss: 0.978457]\n",
      "519 [D loss: 0.831256, acc.: 42.19%] [G loss: 0.975659]\n",
      "520 [D loss: 0.678746, acc.: 59.38%] [G loss: 0.989578]\n",
      "521 [D loss: 0.755589, acc.: 57.81%] [G loss: 0.972123]\n",
      "522 [D loss: 0.730075, acc.: 48.44%] [G loss: 1.056228]\n",
      "523 [D loss: 0.667063, acc.: 54.69%] [G loss: 0.855168]\n",
      "524 [D loss: 0.729342, acc.: 51.56%] [G loss: 1.120846]\n",
      "525 [D loss: 0.705130, acc.: 50.00%] [G loss: 1.154788]\n",
      "526 [D loss: 0.709637, acc.: 51.56%] [G loss: 0.989885]\n",
      "527 [D loss: 0.740293, acc.: 51.56%] [G loss: 0.864221]\n",
      "528 [D loss: 0.704184, acc.: 54.69%] [G loss: 1.098177]\n",
      "529 [D loss: 0.721990, acc.: 54.69%] [G loss: 0.900226]\n",
      "530 [D loss: 0.766159, acc.: 53.12%] [G loss: 0.846947]\n",
      "531 [D loss: 0.710888, acc.: 51.56%] [G loss: 1.085700]\n",
      "532 [D loss: 0.679533, acc.: 53.12%] [G loss: 0.991256]\n",
      "533 [D loss: 0.679127, acc.: 51.56%] [G loss: 0.939161]\n",
      "534 [D loss: 0.688783, acc.: 60.94%] [G loss: 1.078214]\n",
      "535 [D loss: 0.687777, acc.: 60.94%] [G loss: 1.007582]\n",
      "536 [D loss: 0.731469, acc.: 56.25%] [G loss: 1.041021]\n",
      "537 [D loss: 0.753504, acc.: 43.75%] [G loss: 1.008214]\n",
      "538 [D loss: 0.743845, acc.: 48.44%] [G loss: 0.885066]\n",
      "539 [D loss: 0.774827, acc.: 54.69%] [G loss: 0.932827]\n",
      "540 [D loss: 0.757673, acc.: 45.31%] [G loss: 0.966032]\n",
      "541 [D loss: 0.753179, acc.: 42.19%] [G loss: 0.948470]\n",
      "542 [D loss: 0.681757, acc.: 59.38%] [G loss: 0.962837]\n",
      "543 [D loss: 0.594827, acc.: 71.88%] [G loss: 1.175603]\n",
      "544 [D loss: 0.726261, acc.: 60.94%] [G loss: 1.030757]\n",
      "545 [D loss: 0.734187, acc.: 53.12%] [G loss: 0.982834]\n",
      "546 [D loss: 0.727687, acc.: 53.12%] [G loss: 1.061529]\n",
      "547 [D loss: 0.841700, acc.: 45.31%] [G loss: 1.077927]\n",
      "548 [D loss: 0.827572, acc.: 51.56%] [G loss: 0.954112]\n",
      "549 [D loss: 0.767544, acc.: 51.56%] [G loss: 1.043007]\n",
      "550 [D loss: 0.754053, acc.: 46.88%] [G loss: 0.943981]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "551 [D loss: 0.722545, acc.: 54.69%] [G loss: 1.110779]\n",
      "552 [D loss: 0.703453, acc.: 62.50%] [G loss: 0.987259]\n",
      "553 [D loss: 0.862424, acc.: 43.75%] [G loss: 1.003107]\n",
      "554 [D loss: 0.718188, acc.: 59.38%] [G loss: 1.049410]\n",
      "555 [D loss: 0.722480, acc.: 59.38%] [G loss: 0.943892]\n",
      "556 [D loss: 0.696793, acc.: 54.69%] [G loss: 1.065292]\n",
      "557 [D loss: 0.672211, acc.: 59.38%] [G loss: 1.144986]\n",
      "558 [D loss: 0.740976, acc.: 48.44%] [G loss: 1.060278]\n",
      "559 [D loss: 0.740509, acc.: 50.00%] [G loss: 0.820777]\n",
      "560 [D loss: 0.654606, acc.: 57.81%] [G loss: 0.955353]\n",
      "561 [D loss: 0.766065, acc.: 53.12%] [G loss: 0.906624]\n",
      "562 [D loss: 0.679249, acc.: 50.00%] [G loss: 1.119328]\n",
      "563 [D loss: 0.618460, acc.: 67.19%] [G loss: 0.938667]\n",
      "564 [D loss: 0.709951, acc.: 62.50%] [G loss: 0.966292]\n",
      "565 [D loss: 0.667396, acc.: 65.62%] [G loss: 1.007430]\n",
      "566 [D loss: 0.705922, acc.: 54.69%] [G loss: 1.084572]\n",
      "567 [D loss: 0.715194, acc.: 53.12%] [G loss: 1.047985]\n",
      "568 [D loss: 0.817736, acc.: 48.44%] [G loss: 0.861619]\n",
      "569 [D loss: 0.614031, acc.: 70.31%] [G loss: 1.092048]\n",
      "570 [D loss: 0.719888, acc.: 56.25%] [G loss: 0.910483]\n",
      "571 [D loss: 0.729990, acc.: 53.12%] [G loss: 0.972361]\n",
      "572 [D loss: 0.660238, acc.: 54.69%] [G loss: 1.307835]\n",
      "573 [D loss: 0.762139, acc.: 50.00%] [G loss: 1.041444]\n",
      "574 [D loss: 0.722224, acc.: 57.81%] [G loss: 1.159393]\n",
      "575 [D loss: 0.791032, acc.: 46.88%] [G loss: 1.099928]\n",
      "576 [D loss: 0.765988, acc.: 51.56%] [G loss: 1.052520]\n",
      "577 [D loss: 0.651093, acc.: 64.06%] [G loss: 1.050913]\n",
      "578 [D loss: 0.696091, acc.: 62.50%] [G loss: 0.939651]\n",
      "579 [D loss: 0.754623, acc.: 50.00%] [G loss: 1.085003]\n",
      "580 [D loss: 0.660498, acc.: 60.94%] [G loss: 0.961780]\n",
      "581 [D loss: 0.593836, acc.: 65.62%] [G loss: 1.099472]\n",
      "582 [D loss: 0.629715, acc.: 71.88%] [G loss: 1.046301]\n",
      "583 [D loss: 0.733999, acc.: 60.94%] [G loss: 1.012583]\n",
      "584 [D loss: 0.655582, acc.: 53.12%] [G loss: 1.006759]\n",
      "585 [D loss: 0.771780, acc.: 50.00%] [G loss: 1.118764]\n",
      "586 [D loss: 0.637783, acc.: 68.75%] [G loss: 1.022664]\n",
      "587 [D loss: 0.740718, acc.: 50.00%] [G loss: 1.090062]\n",
      "588 [D loss: 0.835136, acc.: 39.06%] [G loss: 0.890432]\n",
      "589 [D loss: 0.754587, acc.: 48.44%] [G loss: 1.079861]\n",
      "590 [D loss: 0.667494, acc.: 60.94%] [G loss: 1.149447]\n",
      "591 [D loss: 0.640544, acc.: 68.75%] [G loss: 1.122419]\n",
      "592 [D loss: 0.786091, acc.: 50.00%] [G loss: 0.886132]\n",
      "593 [D loss: 0.845252, acc.: 40.62%] [G loss: 0.809423]\n",
      "594 [D loss: 0.737375, acc.: 45.31%] [G loss: 0.889189]\n",
      "595 [D loss: 0.843888, acc.: 42.19%] [G loss: 0.942884]\n",
      "596 [D loss: 0.723747, acc.: 59.38%] [G loss: 1.031355]\n",
      "597 [D loss: 0.666733, acc.: 59.38%] [G loss: 1.011651]\n",
      "598 [D loss: 0.708614, acc.: 59.38%] [G loss: 1.065184]\n",
      "599 [D loss: 0.694990, acc.: 59.38%] [G loss: 1.154688]\n",
      "600 [D loss: 0.690367, acc.: 56.25%] [G loss: 0.745001]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "601 [D loss: 0.695970, acc.: 59.38%] [G loss: 0.935163]\n",
      "602 [D loss: 0.746017, acc.: 51.56%] [G loss: 1.026210]\n",
      "603 [D loss: 0.646026, acc.: 62.50%] [G loss: 0.992558]\n",
      "604 [D loss: 0.628770, acc.: 67.19%] [G loss: 0.935784]\n",
      "605 [D loss: 0.704205, acc.: 51.56%] [G loss: 0.951044]\n",
      "606 [D loss: 0.739249, acc.: 53.12%] [G loss: 0.959667]\n",
      "607 [D loss: 0.808649, acc.: 43.75%] [G loss: 0.848482]\n",
      "608 [D loss: 0.750051, acc.: 59.38%] [G loss: 1.059798]\n",
      "609 [D loss: 0.753686, acc.: 50.00%] [G loss: 0.989840]\n",
      "610 [D loss: 0.631730, acc.: 57.81%] [G loss: 0.889364]\n",
      "611 [D loss: 0.719596, acc.: 53.12%] [G loss: 1.101822]\n",
      "612 [D loss: 0.700175, acc.: 56.25%] [G loss: 0.939863]\n",
      "613 [D loss: 0.676942, acc.: 53.12%] [G loss: 0.941854]\n",
      "614 [D loss: 0.601830, acc.: 73.44%] [G loss: 1.102503]\n",
      "615 [D loss: 0.748776, acc.: 53.12%] [G loss: 1.178281]\n",
      "616 [D loss: 0.678812, acc.: 59.38%] [G loss: 0.957910]\n",
      "617 [D loss: 0.776953, acc.: 48.44%] [G loss: 1.066757]\n",
      "618 [D loss: 0.675886, acc.: 57.81%] [G loss: 1.030218]\n",
      "619 [D loss: 0.749105, acc.: 56.25%] [G loss: 0.935524]\n",
      "620 [D loss: 0.690331, acc.: 53.12%] [G loss: 1.029133]\n",
      "621 [D loss: 0.632094, acc.: 62.50%] [G loss: 1.040103]\n",
      "622 [D loss: 0.742658, acc.: 46.88%] [G loss: 1.011794]\n",
      "623 [D loss: 0.701446, acc.: 54.69%] [G loss: 1.011919]\n",
      "624 [D loss: 0.640206, acc.: 64.06%] [G loss: 0.997333]\n",
      "625 [D loss: 0.769333, acc.: 46.88%] [G loss: 1.059086]\n",
      "626 [D loss: 0.644611, acc.: 57.81%] [G loss: 1.115791]\n",
      "627 [D loss: 0.704708, acc.: 60.94%] [G loss: 1.011224]\n",
      "628 [D loss: 0.794170, acc.: 50.00%] [G loss: 0.918092]\n",
      "629 [D loss: 0.733660, acc.: 51.56%] [G loss: 1.077915]\n",
      "630 [D loss: 0.625376, acc.: 68.75%] [G loss: 1.129141]\n",
      "631 [D loss: 0.797222, acc.: 53.12%] [G loss: 1.060435]\n",
      "632 [D loss: 0.744877, acc.: 53.12%] [G loss: 1.078176]\n",
      "633 [D loss: 0.684552, acc.: 62.50%] [G loss: 0.983676]\n",
      "634 [D loss: 0.683571, acc.: 65.62%] [G loss: 1.024510]\n",
      "635 [D loss: 0.611161, acc.: 64.06%] [G loss: 1.089810]\n",
      "636 [D loss: 0.744614, acc.: 57.81%] [G loss: 0.952803]\n",
      "637 [D loss: 0.652119, acc.: 59.38%] [G loss: 1.100133]\n",
      "638 [D loss: 0.645966, acc.: 59.38%] [G loss: 0.842816]\n",
      "639 [D loss: 0.684105, acc.: 53.12%] [G loss: 1.137936]\n",
      "640 [D loss: 0.718404, acc.: 56.25%] [G loss: 1.219761]\n",
      "641 [D loss: 0.743248, acc.: 50.00%] [G loss: 0.882438]\n",
      "642 [D loss: 0.724102, acc.: 51.56%] [G loss: 1.006240]\n",
      "643 [D loss: 0.836762, acc.: 51.56%] [G loss: 0.878133]\n",
      "644 [D loss: 0.754956, acc.: 51.56%] [G loss: 0.986061]\n",
      "645 [D loss: 0.758003, acc.: 57.81%] [G loss: 1.001192]\n",
      "646 [D loss: 0.731098, acc.: 53.12%] [G loss: 0.997180]\n",
      "647 [D loss: 0.745534, acc.: 40.62%] [G loss: 1.018971]\n",
      "648 [D loss: 0.718307, acc.: 54.69%] [G loss: 1.033862]\n",
      "649 [D loss: 0.653238, acc.: 67.19%] [G loss: 1.029141]\n",
      "650 [D loss: 0.709446, acc.: 54.69%] [G loss: 0.936830]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "651 [D loss: 0.602162, acc.: 70.31%] [G loss: 1.026447]\n",
      "652 [D loss: 0.688059, acc.: 56.25%] [G loss: 1.201036]\n",
      "653 [D loss: 0.756753, acc.: 48.44%] [G loss: 0.830049]\n",
      "654 [D loss: 0.777750, acc.: 46.88%] [G loss: 0.993487]\n",
      "655 [D loss: 0.654671, acc.: 65.62%] [G loss: 0.868212]\n",
      "656 [D loss: 0.732191, acc.: 59.38%] [G loss: 1.099350]\n",
      "657 [D loss: 0.731027, acc.: 51.56%] [G loss: 0.931048]\n",
      "658 [D loss: 0.736813, acc.: 48.44%] [G loss: 1.245410]\n",
      "659 [D loss: 0.685380, acc.: 57.81%] [G loss: 1.046107]\n",
      "660 [D loss: 0.828267, acc.: 43.75%] [G loss: 0.838964]\n",
      "661 [D loss: 0.676710, acc.: 60.94%] [G loss: 1.031178]\n",
      "662 [D loss: 0.788226, acc.: 46.88%] [G loss: 0.920373]\n",
      "663 [D loss: 0.714723, acc.: 57.81%] [G loss: 0.975662]\n",
      "664 [D loss: 0.667992, acc.: 62.50%] [G loss: 0.968115]\n",
      "665 [D loss: 0.737111, acc.: 51.56%] [G loss: 0.998470]\n",
      "666 [D loss: 0.693397, acc.: 54.69%] [G loss: 1.029764]\n",
      "667 [D loss: 0.686068, acc.: 57.81%] [G loss: 1.124414]\n",
      "668 [D loss: 0.763429, acc.: 54.69%] [G loss: 1.040241]\n",
      "669 [D loss: 0.714043, acc.: 57.81%] [G loss: 1.032689]\n",
      "670 [D loss: 0.566258, acc.: 73.44%] [G loss: 1.173257]\n",
      "671 [D loss: 0.705666, acc.: 57.81%] [G loss: 0.909204]\n",
      "672 [D loss: 0.747312, acc.: 50.00%] [G loss: 1.038886]\n",
      "673 [D loss: 0.633577, acc.: 71.88%] [G loss: 1.116608]\n",
      "674 [D loss: 0.684773, acc.: 60.94%] [G loss: 1.098952]\n",
      "675 [D loss: 0.690237, acc.: 59.38%] [G loss: 1.243632]\n",
      "676 [D loss: 0.798332, acc.: 48.44%] [G loss: 1.067338]\n",
      "677 [D loss: 0.717527, acc.: 53.12%] [G loss: 1.091946]\n",
      "678 [D loss: 0.791911, acc.: 51.56%] [G loss: 1.288439]\n",
      "679 [D loss: 0.764134, acc.: 51.56%] [G loss: 1.155426]\n",
      "680 [D loss: 0.715929, acc.: 48.44%] [G loss: 1.166766]\n",
      "681 [D loss: 0.717789, acc.: 56.25%] [G loss: 1.133046]\n",
      "682 [D loss: 0.709812, acc.: 59.38%] [G loss: 0.962752]\n",
      "683 [D loss: 0.635131, acc.: 64.06%] [G loss: 1.142687]\n",
      "684 [D loss: 0.613568, acc.: 60.94%] [G loss: 1.089257]\n",
      "685 [D loss: 0.727679, acc.: 53.12%] [G loss: 0.999265]\n",
      "686 [D loss: 0.704343, acc.: 62.50%] [G loss: 0.888248]\n",
      "687 [D loss: 0.673480, acc.: 56.25%] [G loss: 1.056410]\n",
      "688 [D loss: 0.724861, acc.: 56.25%] [G loss: 1.138243]\n",
      "689 [D loss: 0.749785, acc.: 53.12%] [G loss: 0.913227]\n",
      "690 [D loss: 0.689551, acc.: 56.25%] [G loss: 0.963762]\n",
      "691 [D loss: 0.723060, acc.: 50.00%] [G loss: 1.135385]\n",
      "692 [D loss: 0.580071, acc.: 73.44%] [G loss: 0.950584]\n",
      "693 [D loss: 0.674953, acc.: 54.69%] [G loss: 0.859179]\n",
      "694 [D loss: 0.791120, acc.: 51.56%] [G loss: 0.874973]\n",
      "695 [D loss: 0.680578, acc.: 62.50%] [G loss: 1.166786]\n",
      "696 [D loss: 0.644618, acc.: 71.88%] [G loss: 1.119840]\n",
      "697 [D loss: 0.777594, acc.: 48.44%] [G loss: 0.846163]\n",
      "698 [D loss: 0.662485, acc.: 54.69%] [G loss: 0.876770]\n",
      "699 [D loss: 0.637109, acc.: 67.19%] [G loss: 0.970019]\n",
      "700 [D loss: 0.743994, acc.: 57.81%] [G loss: 0.811212]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "701 [D loss: 0.692638, acc.: 56.25%] [G loss: 1.104059]\n",
      "702 [D loss: 0.683865, acc.: 64.06%] [G loss: 1.049112]\n",
      "703 [D loss: 0.745173, acc.: 57.81%] [G loss: 1.378873]\n",
      "704 [D loss: 0.828079, acc.: 43.75%] [G loss: 1.094476]\n",
      "705 [D loss: 0.774547, acc.: 50.00%] [G loss: 1.219780]\n",
      "706 [D loss: 0.700791, acc.: 51.56%] [G loss: 1.018337]\n",
      "707 [D loss: 0.744458, acc.: 46.88%] [G loss: 0.881212]\n",
      "708 [D loss: 0.730003, acc.: 54.69%] [G loss: 1.086465]\n",
      "709 [D loss: 0.651294, acc.: 68.75%] [G loss: 0.996603]\n",
      "710 [D loss: 0.863629, acc.: 50.00%] [G loss: 0.993673]\n",
      "711 [D loss: 0.630188, acc.: 65.62%] [G loss: 1.048236]\n",
      "712 [D loss: 0.723067, acc.: 57.81%] [G loss: 1.017954]\n",
      "713 [D loss: 0.705925, acc.: 54.69%] [G loss: 1.098849]\n",
      "714 [D loss: 0.800020, acc.: 50.00%] [G loss: 0.840667]\n",
      "715 [D loss: 0.687947, acc.: 57.81%] [G loss: 0.909544]\n",
      "716 [D loss: 0.707248, acc.: 54.69%] [G loss: 1.165648]\n",
      "717 [D loss: 0.746644, acc.: 46.88%] [G loss: 1.049704]\n",
      "718 [D loss: 0.617036, acc.: 67.19%] [G loss: 1.193662]\n",
      "719 [D loss: 0.666364, acc.: 59.38%] [G loss: 1.150892]\n",
      "720 [D loss: 0.658476, acc.: 62.50%] [G loss: 1.010827]\n",
      "721 [D loss: 0.707652, acc.: 59.38%] [G loss: 1.023437]\n",
      "722 [D loss: 0.819991, acc.: 37.50%] [G loss: 0.985045]\n",
      "723 [D loss: 0.762539, acc.: 53.12%] [G loss: 1.087024]\n",
      "724 [D loss: 0.728183, acc.: 59.38%] [G loss: 1.121191]\n",
      "725 [D loss: 0.621825, acc.: 65.62%] [G loss: 1.186906]\n",
      "726 [D loss: 0.727649, acc.: 53.12%] [G loss: 0.997331]\n",
      "727 [D loss: 0.759617, acc.: 43.75%] [G loss: 0.963784]\n",
      "728 [D loss: 0.742625, acc.: 60.94%] [G loss: 0.925051]\n",
      "729 [D loss: 0.668235, acc.: 60.94%] [G loss: 1.006334]\n",
      "730 [D loss: 0.642031, acc.: 62.50%] [G loss: 1.012476]\n",
      "731 [D loss: 0.749171, acc.: 51.56%] [G loss: 0.982957]\n",
      "732 [D loss: 0.597010, acc.: 67.19%] [G loss: 0.872770]\n",
      "733 [D loss: 0.730958, acc.: 56.25%] [G loss: 1.077441]\n",
      "734 [D loss: 0.757013, acc.: 50.00%] [G loss: 1.055071]\n",
      "735 [D loss: 0.689637, acc.: 60.94%] [G loss: 1.071439]\n",
      "736 [D loss: 0.786408, acc.: 51.56%] [G loss: 1.106618]\n",
      "737 [D loss: 0.674293, acc.: 56.25%] [G loss: 1.119594]\n",
      "738 [D loss: 0.711229, acc.: 54.69%] [G loss: 0.914536]\n",
      "739 [D loss: 0.777323, acc.: 46.88%] [G loss: 0.997920]\n",
      "740 [D loss: 0.762218, acc.: 50.00%] [G loss: 1.010446]\n",
      "741 [D loss: 0.686052, acc.: 57.81%] [G loss: 1.053591]\n",
      "742 [D loss: 0.682967, acc.: 59.38%] [G loss: 1.092956]\n",
      "743 [D loss: 0.678601, acc.: 59.38%] [G loss: 0.973241]\n",
      "744 [D loss: 0.671684, acc.: 56.25%] [G loss: 1.048238]\n",
      "745 [D loss: 0.735130, acc.: 51.56%] [G loss: 1.283608]\n",
      "746 [D loss: 0.805918, acc.: 42.19%] [G loss: 1.120275]\n",
      "747 [D loss: 0.717934, acc.: 56.25%] [G loss: 0.993331]\n",
      "748 [D loss: 0.754447, acc.: 62.50%] [G loss: 1.077344]\n",
      "749 [D loss: 0.751741, acc.: 50.00%] [G loss: 0.999007]\n",
      "750 [D loss: 0.690593, acc.: 51.56%] [G loss: 1.140334]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "751 [D loss: 0.817086, acc.: 43.75%] [G loss: 0.977005]\n",
      "752 [D loss: 0.681985, acc.: 54.69%] [G loss: 0.948707]\n",
      "753 [D loss: 0.711851, acc.: 54.69%] [G loss: 1.030982]\n",
      "754 [D loss: 0.729267, acc.: 51.56%] [G loss: 1.014217]\n",
      "755 [D loss: 0.745194, acc.: 46.88%] [G loss: 1.126208]\n",
      "756 [D loss: 0.705292, acc.: 56.25%] [G loss: 1.104987]\n",
      "757 [D loss: 0.741018, acc.: 51.56%] [G loss: 1.013042]\n",
      "758 [D loss: 0.696747, acc.: 48.44%] [G loss: 0.978405]\n",
      "759 [D loss: 0.830729, acc.: 43.75%] [G loss: 0.859585]\n",
      "760 [D loss: 0.653639, acc.: 62.50%] [G loss: 0.845861]\n",
      "761 [D loss: 0.732631, acc.: 56.25%] [G loss: 1.100849]\n",
      "762 [D loss: 0.712927, acc.: 57.81%] [G loss: 1.066205]\n",
      "763 [D loss: 0.829616, acc.: 31.25%] [G loss: 0.900375]\n",
      "764 [D loss: 0.667946, acc.: 57.81%] [G loss: 1.071489]\n",
      "765 [D loss: 0.650724, acc.: 67.19%] [G loss: 1.077325]\n",
      "766 [D loss: 0.788008, acc.: 46.88%] [G loss: 0.904629]\n",
      "767 [D loss: 0.667306, acc.: 62.50%] [G loss: 1.090784]\n",
      "768 [D loss: 0.652994, acc.: 60.94%] [G loss: 1.220074]\n",
      "769 [D loss: 0.707200, acc.: 54.69%] [G loss: 1.196523]\n",
      "770 [D loss: 0.766006, acc.: 46.88%] [G loss: 1.046718]\n",
      "771 [D loss: 0.675579, acc.: 60.94%] [G loss: 0.971274]\n",
      "772 [D loss: 0.723908, acc.: 56.25%] [G loss: 1.085769]\n",
      "773 [D loss: 0.658772, acc.: 60.94%] [G loss: 0.961248]\n",
      "774 [D loss: 0.783058, acc.: 48.44%] [G loss: 1.189043]\n",
      "775 [D loss: 0.738113, acc.: 50.00%] [G loss: 1.088499]\n",
      "776 [D loss: 0.772634, acc.: 57.81%] [G loss: 1.033174]\n",
      "777 [D loss: 0.656305, acc.: 62.50%] [G loss: 1.105276]\n",
      "778 [D loss: 0.843941, acc.: 46.88%] [G loss: 0.831336]\n",
      "779 [D loss: 0.735706, acc.: 48.44%] [G loss: 0.962140]\n",
      "780 [D loss: 0.613915, acc.: 64.06%] [G loss: 1.056296]\n",
      "781 [D loss: 0.704158, acc.: 59.38%] [G loss: 0.940748]\n",
      "782 [D loss: 0.725405, acc.: 46.88%] [G loss: 0.910908]\n",
      "783 [D loss: 0.608562, acc.: 62.50%] [G loss: 1.152135]\n",
      "784 [D loss: 0.723608, acc.: 50.00%] [G loss: 1.116036]\n",
      "785 [D loss: 0.743043, acc.: 46.88%] [G loss: 0.992209]\n",
      "786 [D loss: 0.694976, acc.: 54.69%] [G loss: 1.088659]\n",
      "787 [D loss: 0.743878, acc.: 48.44%] [G loss: 1.071965]\n",
      "788 [D loss: 0.752626, acc.: 56.25%] [G loss: 1.130054]\n",
      "789 [D loss: 0.650197, acc.: 62.50%] [G loss: 0.962146]\n",
      "790 [D loss: 0.682361, acc.: 62.50%] [G loss: 1.101208]\n",
      "791 [D loss: 0.709600, acc.: 60.94%] [G loss: 1.203505]\n",
      "792 [D loss: 0.645037, acc.: 56.25%] [G loss: 1.151233]\n",
      "793 [D loss: 0.608875, acc.: 60.94%] [G loss: 1.072562]\n",
      "794 [D loss: 0.740027, acc.: 53.12%] [G loss: 1.008624]\n",
      "795 [D loss: 0.643978, acc.: 68.75%] [G loss: 0.941251]\n",
      "796 [D loss: 0.654960, acc.: 62.50%] [G loss: 1.191060]\n",
      "797 [D loss: 0.714986, acc.: 48.44%] [G loss: 1.135266]\n",
      "798 [D loss: 0.558585, acc.: 70.31%] [G loss: 1.242504]\n",
      "799 [D loss: 0.585646, acc.: 68.75%] [G loss: 1.080648]\n",
      "800 [D loss: 0.817636, acc.: 50.00%] [G loss: 1.072735]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "801 [D loss: 0.740307, acc.: 54.69%] [G loss: 0.928169]\n",
      "802 [D loss: 0.734314, acc.: 51.56%] [G loss: 1.046741]\n",
      "803 [D loss: 0.744301, acc.: 57.81%] [G loss: 1.291760]\n",
      "804 [D loss: 0.689120, acc.: 56.25%] [G loss: 1.219790]\n",
      "805 [D loss: 0.710097, acc.: 59.38%] [G loss: 1.117978]\n",
      "806 [D loss: 0.689311, acc.: 53.12%] [G loss: 1.040976]\n",
      "807 [D loss: 0.719100, acc.: 54.69%] [G loss: 0.967065]\n",
      "808 [D loss: 0.608067, acc.: 68.75%] [G loss: 1.035756]\n",
      "809 [D loss: 0.712306, acc.: 60.94%] [G loss: 1.131344]\n",
      "810 [D loss: 0.721680, acc.: 60.94%] [G loss: 1.170243]\n",
      "811 [D loss: 0.704986, acc.: 54.69%] [G loss: 1.029258]\n",
      "812 [D loss: 0.589810, acc.: 70.31%] [G loss: 1.167262]\n",
      "813 [D loss: 0.825868, acc.: 39.06%] [G loss: 0.866312]\n",
      "814 [D loss: 0.654638, acc.: 60.94%] [G loss: 1.092893]\n",
      "815 [D loss: 0.774388, acc.: 54.69%] [G loss: 1.033971]\n",
      "816 [D loss: 0.741954, acc.: 51.56%] [G loss: 0.979780]\n",
      "817 [D loss: 0.650230, acc.: 59.38%] [G loss: 1.063983]\n",
      "818 [D loss: 0.674315, acc.: 59.38%] [G loss: 1.026533]\n",
      "819 [D loss: 0.773260, acc.: 50.00%] [G loss: 0.955442]\n",
      "820 [D loss: 0.725089, acc.: 50.00%] [G loss: 1.059088]\n",
      "821 [D loss: 0.636473, acc.: 59.38%] [G loss: 1.095424]\n",
      "822 [D loss: 0.702235, acc.: 57.81%] [G loss: 0.959315]\n",
      "823 [D loss: 0.688565, acc.: 62.50%] [G loss: 1.145668]\n",
      "824 [D loss: 0.705087, acc.: 59.38%] [G loss: 1.163238]\n",
      "825 [D loss: 0.783574, acc.: 45.31%] [G loss: 0.984076]\n",
      "826 [D loss: 0.612359, acc.: 70.31%] [G loss: 1.109656]\n",
      "827 [D loss: 0.652971, acc.: 62.50%] [G loss: 1.052782]\n",
      "828 [D loss: 0.693618, acc.: 56.25%] [G loss: 1.069619]\n",
      "829 [D loss: 0.632197, acc.: 59.38%] [G loss: 1.000788]\n",
      "830 [D loss: 0.710674, acc.: 59.38%] [G loss: 1.079208]\n",
      "831 [D loss: 0.550686, acc.: 70.31%] [G loss: 1.248880]\n",
      "832 [D loss: 0.694613, acc.: 54.69%] [G loss: 1.072315]\n",
      "833 [D loss: 0.754238, acc.: 50.00%] [G loss: 1.004551]\n",
      "834 [D loss: 0.675806, acc.: 57.81%] [G loss: 0.805143]\n",
      "835 [D loss: 0.633453, acc.: 59.38%] [G loss: 0.870213]\n",
      "836 [D loss: 0.732092, acc.: 51.56%] [G loss: 1.017033]\n",
      "837 [D loss: 0.774030, acc.: 48.44%] [G loss: 1.111978]\n",
      "838 [D loss: 0.767528, acc.: 45.31%] [G loss: 0.869504]\n",
      "839 [D loss: 0.673019, acc.: 60.94%] [G loss: 0.976012]\n",
      "840 [D loss: 0.658594, acc.: 57.81%] [G loss: 1.104527]\n",
      "841 [D loss: 0.773862, acc.: 51.56%] [G loss: 0.958589]\n",
      "842 [D loss: 0.676035, acc.: 59.38%] [G loss: 0.989516]\n",
      "843 [D loss: 0.638328, acc.: 62.50%] [G loss: 1.093627]\n",
      "844 [D loss: 0.824888, acc.: 39.06%] [G loss: 1.079021]\n",
      "845 [D loss: 0.638422, acc.: 62.50%] [G loss: 0.993087]\n",
      "846 [D loss: 0.690138, acc.: 51.56%] [G loss: 0.941278]\n",
      "847 [D loss: 0.725360, acc.: 51.56%] [G loss: 0.939144]\n",
      "848 [D loss: 0.753075, acc.: 50.00%] [G loss: 0.854658]\n",
      "849 [D loss: 0.734601, acc.: 51.56%] [G loss: 0.927683]\n",
      "850 [D loss: 0.653133, acc.: 60.94%] [G loss: 0.914697]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "851 [D loss: 0.718915, acc.: 50.00%] [G loss: 1.055419]\n",
      "852 [D loss: 0.718016, acc.: 53.12%] [G loss: 0.857761]\n",
      "853 [D loss: 0.676269, acc.: 57.81%] [G loss: 1.035693]\n",
      "854 [D loss: 0.767614, acc.: 50.00%] [G loss: 1.154762]\n",
      "855 [D loss: 0.722613, acc.: 54.69%] [G loss: 0.901003]\n",
      "856 [D loss: 0.746791, acc.: 54.69%] [G loss: 1.058086]\n",
      "857 [D loss: 0.736845, acc.: 56.25%] [G loss: 0.982874]\n",
      "858 [D loss: 0.672611, acc.: 59.38%] [G loss: 1.008807]\n",
      "859 [D loss: 0.740075, acc.: 46.88%] [G loss: 1.048249]\n",
      "860 [D loss: 0.646995, acc.: 59.38%] [G loss: 1.351728]\n",
      "861 [D loss: 0.697520, acc.: 51.56%] [G loss: 1.079311]\n",
      "862 [D loss: 0.811569, acc.: 45.31%] [G loss: 0.811998]\n",
      "863 [D loss: 0.693793, acc.: 50.00%] [G loss: 0.889395]\n",
      "864 [D loss: 0.626317, acc.: 68.75%] [G loss: 0.907118]\n",
      "865 [D loss: 0.726233, acc.: 46.88%] [G loss: 0.920946]\n",
      "866 [D loss: 0.854034, acc.: 45.31%] [G loss: 0.938597]\n",
      "867 [D loss: 0.623786, acc.: 73.44%] [G loss: 1.053079]\n",
      "868 [D loss: 0.740901, acc.: 57.81%] [G loss: 1.054518]\n",
      "869 [D loss: 0.687790, acc.: 57.81%] [G loss: 0.973984]\n",
      "870 [D loss: 0.624053, acc.: 62.50%] [G loss: 1.040450]\n",
      "871 [D loss: 0.795698, acc.: 46.88%] [G loss: 1.101158]\n",
      "872 [D loss: 0.658599, acc.: 56.25%] [G loss: 1.006120]\n",
      "873 [D loss: 0.685656, acc.: 59.38%] [G loss: 1.029292]\n",
      "874 [D loss: 0.709656, acc.: 65.62%] [G loss: 1.166733]\n",
      "875 [D loss: 0.736821, acc.: 46.88%] [G loss: 1.146594]\n",
      "876 [D loss: 0.702482, acc.: 50.00%] [G loss: 0.905451]\n",
      "877 [D loss: 0.695281, acc.: 56.25%] [G loss: 1.183321]\n",
      "878 [D loss: 0.660373, acc.: 70.31%] [G loss: 1.021039]\n",
      "879 [D loss: 0.667065, acc.: 64.06%] [G loss: 1.049691]\n",
      "880 [D loss: 0.618673, acc.: 60.94%] [G loss: 1.183519]\n",
      "881 [D loss: 0.738059, acc.: 54.69%] [G loss: 1.104747]\n",
      "882 [D loss: 0.692809, acc.: 57.81%] [G loss: 1.148061]\n",
      "883 [D loss: 0.675797, acc.: 59.38%] [G loss: 1.050175]\n",
      "884 [D loss: 0.624262, acc.: 67.19%] [G loss: 1.156112]\n",
      "885 [D loss: 0.639632, acc.: 62.50%] [G loss: 1.002758]\n",
      "886 [D loss: 0.713926, acc.: 53.12%] [G loss: 0.950464]\n",
      "887 [D loss: 0.805625, acc.: 39.06%] [G loss: 1.047920]\n",
      "888 [D loss: 0.743326, acc.: 48.44%] [G loss: 1.021807]\n",
      "889 [D loss: 0.647615, acc.: 60.94%] [G loss: 1.206932]\n",
      "890 [D loss: 0.668274, acc.: 59.38%] [G loss: 1.160547]\n",
      "891 [D loss: 0.747743, acc.: 53.12%] [G loss: 0.988505]\n",
      "892 [D loss: 0.709789, acc.: 50.00%] [G loss: 1.086112]\n",
      "893 [D loss: 0.635470, acc.: 68.75%] [G loss: 1.109106]\n",
      "894 [D loss: 0.700288, acc.: 59.38%] [G loss: 0.854104]\n",
      "895 [D loss: 0.794953, acc.: 56.25%] [G loss: 1.166776]\n",
      "896 [D loss: 0.711585, acc.: 53.12%] [G loss: 1.032717]\n",
      "897 [D loss: 0.660289, acc.: 59.38%] [G loss: 1.032127]\n",
      "898 [D loss: 0.631728, acc.: 67.19%] [G loss: 1.154066]\n",
      "899 [D loss: 0.681925, acc.: 57.81%] [G loss: 1.244886]\n",
      "900 [D loss: 0.734484, acc.: 60.94%] [G loss: 1.126698]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "901 [D loss: 0.717259, acc.: 53.12%] [G loss: 1.077881]\n",
      "902 [D loss: 0.806548, acc.: 48.44%] [G loss: 0.895109]\n",
      "903 [D loss: 0.668310, acc.: 57.81%] [G loss: 1.055670]\n",
      "904 [D loss: 0.686842, acc.: 59.38%] [G loss: 0.992800]\n",
      "905 [D loss: 0.765947, acc.: 56.25%] [G loss: 1.207167]\n",
      "906 [D loss: 0.768779, acc.: 53.12%] [G loss: 1.015082]\n",
      "907 [D loss: 0.656186, acc.: 62.50%] [G loss: 0.945962]\n",
      "908 [D loss: 0.699314, acc.: 62.50%] [G loss: 0.948998]\n",
      "909 [D loss: 0.535940, acc.: 76.56%] [G loss: 1.006248]\n",
      "910 [D loss: 0.846654, acc.: 42.19%] [G loss: 0.971230]\n",
      "911 [D loss: 0.746999, acc.: 54.69%] [G loss: 0.961462]\n",
      "912 [D loss: 0.661692, acc.: 57.81%] [G loss: 0.995758]\n",
      "913 [D loss: 0.672232, acc.: 50.00%] [G loss: 1.167533]\n",
      "914 [D loss: 0.778702, acc.: 50.00%] [G loss: 0.991980]\n",
      "915 [D loss: 0.671018, acc.: 57.81%] [G loss: 1.007531]\n",
      "916 [D loss: 0.740597, acc.: 51.56%] [G loss: 1.065158]\n",
      "917 [D loss: 0.699180, acc.: 60.94%] [G loss: 1.081603]\n",
      "918 [D loss: 0.704353, acc.: 53.12%] [G loss: 1.121531]\n",
      "919 [D loss: 0.699603, acc.: 60.94%] [G loss: 1.131469]\n",
      "920 [D loss: 0.619299, acc.: 68.75%] [G loss: 0.897613]\n",
      "921 [D loss: 0.708005, acc.: 65.62%] [G loss: 0.983378]\n",
      "922 [D loss: 0.630311, acc.: 65.62%] [G loss: 1.141613]\n",
      "923 [D loss: 0.732436, acc.: 53.12%] [G loss: 1.118579]\n",
      "924 [D loss: 0.717459, acc.: 56.25%] [G loss: 0.892791]\n",
      "925 [D loss: 0.681663, acc.: 51.56%] [G loss: 0.869178]\n",
      "926 [D loss: 0.761537, acc.: 51.56%] [G loss: 0.915799]\n",
      "927 [D loss: 0.745543, acc.: 56.25%] [G loss: 0.945112]\n",
      "928 [D loss: 0.640570, acc.: 60.94%] [G loss: 1.051548]\n",
      "929 [D loss: 0.630296, acc.: 65.62%] [G loss: 1.013938]\n",
      "930 [D loss: 0.739737, acc.: 59.38%] [G loss: 0.965982]\n",
      "931 [D loss: 0.684550, acc.: 54.69%] [G loss: 0.883152]\n",
      "932 [D loss: 0.758107, acc.: 53.12%] [G loss: 0.893845]\n",
      "933 [D loss: 0.743614, acc.: 54.69%] [G loss: 0.860682]\n",
      "934 [D loss: 0.716436, acc.: 56.25%] [G loss: 1.035690]\n",
      "935 [D loss: 0.647021, acc.: 56.25%] [G loss: 1.058138]\n",
      "936 [D loss: 0.615190, acc.: 64.06%] [G loss: 1.082863]\n",
      "937 [D loss: 0.673893, acc.: 51.56%] [G loss: 1.085730]\n",
      "938 [D loss: 0.770928, acc.: 46.88%] [G loss: 1.078926]\n",
      "939 [D loss: 0.703705, acc.: 57.81%] [G loss: 1.034955]\n",
      "940 [D loss: 0.679954, acc.: 68.75%] [G loss: 1.092490]\n",
      "941 [D loss: 0.662532, acc.: 56.25%] [G loss: 1.054457]\n",
      "942 [D loss: 0.748667, acc.: 50.00%] [G loss: 0.998045]\n",
      "943 [D loss: 0.758630, acc.: 50.00%] [G loss: 1.062401]\n",
      "944 [D loss: 0.532697, acc.: 75.00%] [G loss: 1.093976]\n",
      "945 [D loss: 0.701738, acc.: 53.12%] [G loss: 0.870453]\n",
      "946 [D loss: 0.676499, acc.: 59.38%] [G loss: 1.031575]\n",
      "947 [D loss: 0.680919, acc.: 64.06%] [G loss: 1.137361]\n",
      "948 [D loss: 0.767155, acc.: 50.00%] [G loss: 1.084025]\n",
      "949 [D loss: 0.700774, acc.: 51.56%] [G loss: 1.027456]\n",
      "950 [D loss: 0.743028, acc.: 53.12%] [G loss: 1.034545]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "951 [D loss: 0.651683, acc.: 64.06%] [G loss: 1.193391]\n",
      "952 [D loss: 0.683852, acc.: 62.50%] [G loss: 1.182204]\n",
      "953 [D loss: 0.692484, acc.: 59.38%] [G loss: 0.916863]\n",
      "954 [D loss: 0.660147, acc.: 62.50%] [G loss: 1.132453]\n",
      "955 [D loss: 0.806077, acc.: 46.88%] [G loss: 0.786403]\n",
      "956 [D loss: 0.644848, acc.: 62.50%] [G loss: 1.146908]\n",
      "957 [D loss: 0.720306, acc.: 53.12%] [G loss: 1.045896]\n",
      "958 [D loss: 0.776703, acc.: 51.56%] [G loss: 0.973144]\n",
      "959 [D loss: 0.665143, acc.: 60.94%] [G loss: 1.090632]\n",
      "960 [D loss: 0.658391, acc.: 62.50%] [G loss: 1.028415]\n",
      "961 [D loss: 0.643836, acc.: 57.81%] [G loss: 1.025216]\n",
      "962 [D loss: 0.663739, acc.: 67.19%] [G loss: 0.958041]\n",
      "963 [D loss: 0.691839, acc.: 60.94%] [G loss: 0.779109]\n",
      "964 [D loss: 0.666839, acc.: 62.50%] [G loss: 0.949668]\n",
      "965 [D loss: 0.683810, acc.: 62.50%] [G loss: 1.143292]\n",
      "966 [D loss: 0.638199, acc.: 62.50%] [G loss: 1.169833]\n",
      "967 [D loss: 0.776494, acc.: 57.81%] [G loss: 0.941039]\n",
      "968 [D loss: 0.691741, acc.: 54.69%] [G loss: 0.956446]\n",
      "969 [D loss: 0.696900, acc.: 56.25%] [G loss: 1.059579]\n",
      "970 [D loss: 0.690951, acc.: 51.56%] [G loss: 1.005098]\n",
      "971 [D loss: 0.766641, acc.: 53.12%] [G loss: 1.011602]\n",
      "972 [D loss: 0.736644, acc.: 53.12%] [G loss: 0.971552]\n",
      "973 [D loss: 0.676339, acc.: 62.50%] [G loss: 0.983996]\n",
      "974 [D loss: 0.695251, acc.: 50.00%] [G loss: 1.184412]\n",
      "975 [D loss: 0.731688, acc.: 46.88%] [G loss: 1.029470]\n",
      "976 [D loss: 0.670411, acc.: 56.25%] [G loss: 1.185226]\n",
      "977 [D loss: 0.712707, acc.: 57.81%] [G loss: 1.091717]\n",
      "978 [D loss: 0.716407, acc.: 54.69%] [G loss: 0.939237]\n",
      "979 [D loss: 0.742837, acc.: 54.69%] [G loss: 1.232656]\n",
      "980 [D loss: 0.691328, acc.: 53.12%] [G loss: 1.028147]\n",
      "981 [D loss: 0.750252, acc.: 45.31%] [G loss: 0.996482]\n",
      "982 [D loss: 0.605350, acc.: 67.19%] [G loss: 0.908731]\n",
      "983 [D loss: 0.762481, acc.: 46.88%] [G loss: 1.189734]\n",
      "984 [D loss: 0.641140, acc.: 60.94%] [G loss: 1.083251]\n",
      "985 [D loss: 0.657551, acc.: 60.94%] [G loss: 1.158899]\n",
      "986 [D loss: 0.710508, acc.: 53.12%] [G loss: 1.070364]\n",
      "987 [D loss: 0.734410, acc.: 54.69%] [G loss: 1.024678]\n",
      "988 [D loss: 0.752866, acc.: 53.12%] [G loss: 0.986210]\n",
      "989 [D loss: 0.642033, acc.: 59.38%] [G loss: 0.943138]\n",
      "990 [D loss: 0.621975, acc.: 62.50%] [G loss: 1.081302]\n",
      "991 [D loss: 0.754144, acc.: 50.00%] [G loss: 0.867218]\n",
      "992 [D loss: 0.829725, acc.: 43.75%] [G loss: 0.973659]\n",
      "993 [D loss: 0.700090, acc.: 59.38%] [G loss: 1.025954]\n",
      "994 [D loss: 0.665088, acc.: 64.06%] [G loss: 1.053652]\n",
      "995 [D loss: 0.715061, acc.: 53.12%] [G loss: 1.005880]\n",
      "996 [D loss: 0.630701, acc.: 70.31%] [G loss: 1.202010]\n",
      "997 [D loss: 0.878585, acc.: 50.00%] [G loss: 0.889626]\n",
      "998 [D loss: 0.729050, acc.: 54.69%] [G loss: 1.017504]\n",
      "999 [D loss: 0.694569, acc.: 65.62%] [G loss: 1.014621]\n",
      "1000 [D loss: 0.779336, acc.: 45.31%] [G loss: 0.894561]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1001 [D loss: 0.738725, acc.: 54.69%] [G loss: 1.048067]\n",
      "1002 [D loss: 0.698282, acc.: 54.69%] [G loss: 1.191216]\n",
      "1003 [D loss: 0.661136, acc.: 60.94%] [G loss: 0.996680]\n",
      "1004 [D loss: 0.702515, acc.: 57.81%] [G loss: 0.888230]\n",
      "1005 [D loss: 0.666928, acc.: 56.25%] [G loss: 1.096364]\n",
      "1006 [D loss: 0.598116, acc.: 64.06%] [G loss: 1.087697]\n",
      "1007 [D loss: 0.704766, acc.: 54.69%] [G loss: 1.100292]\n",
      "1008 [D loss: 0.729685, acc.: 50.00%] [G loss: 0.960593]\n",
      "1009 [D loss: 0.684650, acc.: 62.50%] [G loss: 0.919376]\n",
      "1010 [D loss: 0.636507, acc.: 68.75%] [G loss: 0.952464]\n",
      "1011 [D loss: 0.778931, acc.: 45.31%] [G loss: 0.956474]\n",
      "1012 [D loss: 0.793494, acc.: 48.44%] [G loss: 1.094151]\n",
      "1013 [D loss: 0.821718, acc.: 45.31%] [G loss: 0.896371]\n",
      "1014 [D loss: 0.628495, acc.: 60.94%] [G loss: 1.051408]\n",
      "1015 [D loss: 0.718001, acc.: 53.12%] [G loss: 1.019541]\n",
      "1016 [D loss: 0.788221, acc.: 51.56%] [G loss: 1.078947]\n",
      "1017 [D loss: 0.716810, acc.: 51.56%] [G loss: 1.106969]\n",
      "1018 [D loss: 0.818153, acc.: 50.00%] [G loss: 1.092007]\n",
      "1019 [D loss: 0.587538, acc.: 78.12%] [G loss: 1.054615]\n",
      "1020 [D loss: 0.615981, acc.: 65.62%] [G loss: 1.128731]\n",
      "1021 [D loss: 0.694084, acc.: 59.38%] [G loss: 1.141656]\n",
      "1022 [D loss: 0.711007, acc.: 53.12%] [G loss: 1.041068]\n",
      "1023 [D loss: 0.615757, acc.: 60.94%] [G loss: 1.208409]\n",
      "1024 [D loss: 0.718009, acc.: 48.44%] [G loss: 1.111399]\n",
      "1025 [D loss: 0.686066, acc.: 53.12%] [G loss: 1.172147]\n",
      "1026 [D loss: 0.657407, acc.: 56.25%] [G loss: 1.084850]\n",
      "1027 [D loss: 0.802872, acc.: 40.62%] [G loss: 1.080674]\n",
      "1028 [D loss: 0.769491, acc.: 45.31%] [G loss: 0.977719]\n",
      "1029 [D loss: 0.700014, acc.: 54.69%] [G loss: 1.044509]\n",
      "1030 [D loss: 0.688048, acc.: 57.81%] [G loss: 0.951370]\n",
      "1031 [D loss: 0.734093, acc.: 54.69%] [G loss: 1.036325]\n",
      "1032 [D loss: 0.730142, acc.: 46.88%] [G loss: 0.935398]\n",
      "1033 [D loss: 0.757694, acc.: 54.69%] [G loss: 0.995013]\n",
      "1034 [D loss: 0.749830, acc.: 51.56%] [G loss: 1.019710]\n",
      "1035 [D loss: 0.706969, acc.: 53.12%] [G loss: 1.327412]\n",
      "1036 [D loss: 0.732623, acc.: 56.25%] [G loss: 1.097284]\n",
      "1037 [D loss: 0.753420, acc.: 51.56%] [G loss: 0.942222]\n",
      "1038 [D loss: 0.782347, acc.: 43.75%] [G loss: 0.974372]\n",
      "1039 [D loss: 0.762173, acc.: 53.12%] [G loss: 0.927197]\n",
      "1040 [D loss: 0.712218, acc.: 57.81%] [G loss: 1.047577]\n",
      "1041 [D loss: 0.795942, acc.: 46.88%] [G loss: 1.107937]\n",
      "1042 [D loss: 0.727205, acc.: 56.25%] [G loss: 1.150018]\n",
      "1043 [D loss: 0.774410, acc.: 51.56%] [G loss: 0.955852]\n",
      "1044 [D loss: 0.702368, acc.: 54.69%] [G loss: 1.056898]\n",
      "1045 [D loss: 0.647648, acc.: 62.50%] [G loss: 1.168389]\n",
      "1046 [D loss: 0.770932, acc.: 46.88%] [G loss: 0.907303]\n",
      "1047 [D loss: 0.713841, acc.: 56.25%] [G loss: 0.976785]\n",
      "1048 [D loss: 0.685095, acc.: 57.81%] [G loss: 1.002881]\n",
      "1049 [D loss: 0.618887, acc.: 68.75%] [G loss: 0.945364]\n",
      "1050 [D loss: 0.707490, acc.: 54.69%] [G loss: 1.080596]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1051 [D loss: 0.612640, acc.: 68.75%] [G loss: 1.017387]\n",
      "1052 [D loss: 0.688369, acc.: 62.50%] [G loss: 1.165472]\n",
      "1053 [D loss: 0.737021, acc.: 53.12%] [G loss: 1.111553]\n",
      "1054 [D loss: 0.777578, acc.: 48.44%] [G loss: 1.030584]\n",
      "1055 [D loss: 0.645898, acc.: 64.06%] [G loss: 1.020499]\n",
      "1056 [D loss: 0.748088, acc.: 54.69%] [G loss: 1.177740]\n",
      "1057 [D loss: 0.815867, acc.: 42.19%] [G loss: 1.090801]\n",
      "1058 [D loss: 0.715198, acc.: 51.56%] [G loss: 1.019327]\n",
      "1059 [D loss: 0.680668, acc.: 62.50%] [G loss: 1.189264]\n",
      "1060 [D loss: 0.767356, acc.: 50.00%] [G loss: 1.048526]\n",
      "1061 [D loss: 0.744831, acc.: 51.56%] [G loss: 0.918573]\n",
      "1062 [D loss: 0.729439, acc.: 48.44%] [G loss: 1.093209]\n",
      "1063 [D loss: 0.593466, acc.: 68.75%] [G loss: 1.227121]\n",
      "1064 [D loss: 0.740132, acc.: 57.81%] [G loss: 1.070999]\n",
      "1065 [D loss: 0.761360, acc.: 50.00%] [G loss: 1.008569]\n",
      "1066 [D loss: 0.727902, acc.: 48.44%] [G loss: 1.228656]\n",
      "1067 [D loss: 0.751313, acc.: 51.56%] [G loss: 1.009497]\n",
      "1068 [D loss: 0.677258, acc.: 54.69%] [G loss: 0.941809]\n",
      "1069 [D loss: 0.732128, acc.: 46.88%] [G loss: 1.040069]\n",
      "1070 [D loss: 0.727414, acc.: 50.00%] [G loss: 0.970501]\n",
      "1071 [D loss: 0.751572, acc.: 50.00%] [G loss: 1.020942]\n",
      "1072 [D loss: 0.742450, acc.: 46.88%] [G loss: 0.890513]\n",
      "1073 [D loss: 0.664263, acc.: 64.06%] [G loss: 1.125221]\n",
      "1074 [D loss: 0.676128, acc.: 62.50%] [G loss: 0.998884]\n",
      "1075 [D loss: 0.746073, acc.: 54.69%] [G loss: 0.832804]\n",
      "1076 [D loss: 0.752607, acc.: 48.44%] [G loss: 1.099530]\n",
      "1077 [D loss: 0.565382, acc.: 75.00%] [G loss: 1.059361]\n",
      "1078 [D loss: 0.719395, acc.: 53.12%] [G loss: 1.060914]\n",
      "1079 [D loss: 0.768007, acc.: 46.88%] [G loss: 0.890518]\n",
      "1080 [D loss: 0.652649, acc.: 62.50%] [G loss: 1.049971]\n",
      "1081 [D loss: 0.722059, acc.: 50.00%] [G loss: 0.971318]\n",
      "1082 [D loss: 0.714772, acc.: 56.25%] [G loss: 1.063244]\n",
      "1083 [D loss: 0.604114, acc.: 64.06%] [G loss: 1.132336]\n",
      "1084 [D loss: 0.708385, acc.: 60.94%] [G loss: 1.023753]\n",
      "1085 [D loss: 0.757411, acc.: 42.19%] [G loss: 0.939863]\n",
      "1086 [D loss: 0.789231, acc.: 45.31%] [G loss: 0.911581]\n",
      "1087 [D loss: 0.641229, acc.: 56.25%] [G loss: 0.984818]\n",
      "1088 [D loss: 0.792525, acc.: 50.00%] [G loss: 1.130217]\n",
      "1089 [D loss: 0.683364, acc.: 54.69%] [G loss: 1.025388]\n",
      "1090 [D loss: 0.606564, acc.: 65.62%] [G loss: 1.126342]\n",
      "1091 [D loss: 0.692324, acc.: 60.94%] [G loss: 1.082353]\n",
      "1092 [D loss: 0.629133, acc.: 67.19%] [G loss: 1.028095]\n",
      "1093 [D loss: 0.658430, acc.: 62.50%] [G loss: 0.979250]\n",
      "1094 [D loss: 0.843705, acc.: 48.44%] [G loss: 0.963292]\n",
      "1095 [D loss: 0.666587, acc.: 57.81%] [G loss: 0.930644]\n",
      "1096 [D loss: 0.617751, acc.: 62.50%] [G loss: 0.965650]\n",
      "1097 [D loss: 0.694969, acc.: 51.56%] [G loss: 1.109109]\n",
      "1098 [D loss: 0.629573, acc.: 65.62%] [G loss: 1.061004]\n",
      "1099 [D loss: 0.716556, acc.: 51.56%] [G loss: 1.088374]\n",
      "1100 [D loss: 0.661153, acc.: 59.38%] [G loss: 1.052579]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1101 [D loss: 0.804096, acc.: 46.88%] [G loss: 0.989817]\n",
      "1102 [D loss: 0.592830, acc.: 65.62%] [G loss: 1.106303]\n",
      "1103 [D loss: 0.779324, acc.: 53.12%] [G loss: 0.988070]\n",
      "1104 [D loss: 0.761628, acc.: 53.12%] [G loss: 1.036020]\n",
      "1105 [D loss: 0.697539, acc.: 53.12%] [G loss: 0.806460]\n",
      "1106 [D loss: 0.729224, acc.: 54.69%] [G loss: 1.060350]\n",
      "1107 [D loss: 0.622109, acc.: 67.19%] [G loss: 0.888890]\n",
      "1108 [D loss: 0.689024, acc.: 53.12%] [G loss: 0.947836]\n",
      "1109 [D loss: 0.827623, acc.: 50.00%] [G loss: 1.023447]\n",
      "1110 [D loss: 0.753335, acc.: 45.31%] [G loss: 1.062688]\n",
      "1111 [D loss: 0.569094, acc.: 75.00%] [G loss: 1.004203]\n",
      "1112 [D loss: 0.645563, acc.: 64.06%] [G loss: 0.981052]\n",
      "1113 [D loss: 0.717366, acc.: 57.81%] [G loss: 0.940417]\n",
      "1114 [D loss: 0.802940, acc.: 46.88%] [G loss: 1.113385]\n",
      "1115 [D loss: 0.744697, acc.: 50.00%] [G loss: 0.869882]\n",
      "1116 [D loss: 0.639354, acc.: 64.06%] [G loss: 0.994300]\n",
      "1117 [D loss: 0.828731, acc.: 40.62%] [G loss: 1.039736]\n",
      "1118 [D loss: 0.684294, acc.: 59.38%] [G loss: 1.197168]\n",
      "1119 [D loss: 0.772093, acc.: 54.69%] [G loss: 1.140135]\n",
      "1120 [D loss: 0.664675, acc.: 60.94%] [G loss: 0.885413]\n",
      "1121 [D loss: 0.718943, acc.: 48.44%] [G loss: 0.870039]\n",
      "1122 [D loss: 0.730743, acc.: 54.69%] [G loss: 0.926583]\n",
      "1123 [D loss: 0.715985, acc.: 51.56%] [G loss: 0.972772]\n",
      "1124 [D loss: 0.665105, acc.: 64.06%] [G loss: 1.125601]\n",
      "1125 [D loss: 0.684250, acc.: 59.38%] [G loss: 0.987860]\n",
      "1126 [D loss: 0.783760, acc.: 46.88%] [G loss: 0.818138]\n",
      "1127 [D loss: 0.667678, acc.: 62.50%] [G loss: 1.053749]\n",
      "1128 [D loss: 0.725812, acc.: 50.00%] [G loss: 0.992998]\n",
      "1129 [D loss: 0.718382, acc.: 48.44%] [G loss: 1.220745]\n",
      "1130 [D loss: 0.734413, acc.: 46.88%] [G loss: 0.988115]\n",
      "1131 [D loss: 0.852653, acc.: 37.50%] [G loss: 0.853425]\n",
      "1132 [D loss: 0.623042, acc.: 62.50%] [G loss: 0.968632]\n",
      "1133 [D loss: 0.753210, acc.: 56.25%] [G loss: 1.110199]\n",
      "1134 [D loss: 0.715319, acc.: 54.69%] [G loss: 1.111529]\n",
      "1135 [D loss: 0.763276, acc.: 45.31%] [G loss: 0.942208]\n",
      "1136 [D loss: 0.672711, acc.: 65.62%] [G loss: 1.008228]\n",
      "1137 [D loss: 0.806403, acc.: 43.75%] [G loss: 0.971073]\n",
      "1138 [D loss: 0.673722, acc.: 57.81%] [G loss: 1.038007]\n",
      "1139 [D loss: 0.661802, acc.: 60.94%] [G loss: 1.002264]\n",
      "1140 [D loss: 0.666072, acc.: 59.38%] [G loss: 1.156247]\n",
      "1141 [D loss: 0.590939, acc.: 70.31%] [G loss: 1.118539]\n",
      "1142 [D loss: 0.600316, acc.: 70.31%] [G loss: 0.951213]\n",
      "1143 [D loss: 0.692830, acc.: 59.38%] [G loss: 1.093743]\n",
      "1144 [D loss: 0.777129, acc.: 56.25%] [G loss: 1.041907]\n",
      "1145 [D loss: 0.740257, acc.: 45.31%] [G loss: 0.966899]\n",
      "1146 [D loss: 0.708394, acc.: 53.12%] [G loss: 1.129289]\n",
      "1147 [D loss: 0.737793, acc.: 51.56%] [G loss: 1.005309]\n",
      "1148 [D loss: 0.674278, acc.: 71.88%] [G loss: 1.037485]\n",
      "1149 [D loss: 0.662508, acc.: 59.38%] [G loss: 1.121471]\n",
      "1150 [D loss: 0.806352, acc.: 45.31%] [G loss: 1.198159]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1151 [D loss: 0.661059, acc.: 60.94%] [G loss: 0.987023]\n",
      "1152 [D loss: 0.775293, acc.: 46.88%] [G loss: 0.877737]\n",
      "1153 [D loss: 0.692580, acc.: 56.25%] [G loss: 1.016676]\n",
      "1154 [D loss: 0.650943, acc.: 56.25%] [G loss: 1.069762]\n",
      "1155 [D loss: 0.682810, acc.: 56.25%] [G loss: 1.168325]\n",
      "1156 [D loss: 0.641132, acc.: 64.06%] [G loss: 1.004264]\n",
      "1157 [D loss: 0.721377, acc.: 57.81%] [G loss: 1.018047]\n",
      "1158 [D loss: 0.818137, acc.: 46.88%] [G loss: 0.871355]\n",
      "1159 [D loss: 0.802917, acc.: 40.62%] [G loss: 0.933135]\n",
      "1160 [D loss: 0.745093, acc.: 54.69%] [G loss: 0.946209]\n",
      "1161 [D loss: 0.680109, acc.: 57.81%] [G loss: 0.852494]\n",
      "1162 [D loss: 0.720604, acc.: 56.25%] [G loss: 0.930865]\n",
      "1163 [D loss: 0.670213, acc.: 56.25%] [G loss: 0.674288]\n",
      "1164 [D loss: 0.608324, acc.: 68.75%] [G loss: 0.936648]\n",
      "1165 [D loss: 0.644427, acc.: 56.25%] [G loss: 0.927721]\n",
      "1166 [D loss: 0.631649, acc.: 65.62%] [G loss: 1.069811]\n",
      "1167 [D loss: 0.696849, acc.: 56.25%] [G loss: 1.066655]\n",
      "1168 [D loss: 0.737183, acc.: 59.38%] [G loss: 0.908182]\n",
      "1169 [D loss: 0.804917, acc.: 43.75%] [G loss: 1.079059]\n",
      "1170 [D loss: 0.691152, acc.: 59.38%] [G loss: 0.972255]\n",
      "1171 [D loss: 0.651344, acc.: 62.50%] [G loss: 1.250310]\n",
      "1172 [D loss: 0.721695, acc.: 54.69%] [G loss: 1.078023]\n",
      "1173 [D loss: 0.679063, acc.: 53.12%] [G loss: 0.952403]\n",
      "1174 [D loss: 0.648605, acc.: 60.94%] [G loss: 1.062561]\n",
      "1175 [D loss: 0.648148, acc.: 65.62%] [G loss: 1.049440]\n",
      "1176 [D loss: 0.709426, acc.: 60.94%] [G loss: 0.956939]\n",
      "1177 [D loss: 0.679264, acc.: 60.94%] [G loss: 0.989692]\n",
      "1178 [D loss: 0.716685, acc.: 56.25%] [G loss: 1.032096]\n",
      "1179 [D loss: 0.680730, acc.: 56.25%] [G loss: 1.014493]\n",
      "1180 [D loss: 0.611153, acc.: 68.75%] [G loss: 1.063012]\n",
      "1181 [D loss: 0.656358, acc.: 60.94%] [G loss: 1.158095]\n",
      "1182 [D loss: 0.601381, acc.: 68.75%] [G loss: 1.051600]\n",
      "1183 [D loss: 0.725646, acc.: 54.69%] [G loss: 0.894363]\n",
      "1184 [D loss: 0.720671, acc.: 50.00%] [G loss: 1.156635]\n",
      "1185 [D loss: 0.826944, acc.: 53.12%] [G loss: 0.901945]\n",
      "1186 [D loss: 0.723482, acc.: 51.56%] [G loss: 0.830417]\n",
      "1187 [D loss: 0.704784, acc.: 59.38%] [G loss: 0.849040]\n",
      "1188 [D loss: 0.660222, acc.: 60.94%] [G loss: 0.906338]\n",
      "1189 [D loss: 0.653472, acc.: 62.50%] [G loss: 0.903172]\n",
      "1190 [D loss: 0.667889, acc.: 59.38%] [G loss: 0.900343]\n",
      "1191 [D loss: 0.634889, acc.: 62.50%] [G loss: 0.856929]\n",
      "1192 [D loss: 0.768117, acc.: 42.19%] [G loss: 0.986803]\n",
      "1193 [D loss: 0.783425, acc.: 56.25%] [G loss: 1.057901]\n",
      "1194 [D loss: 0.744762, acc.: 51.56%] [G loss: 0.971738]\n",
      "1195 [D loss: 0.731277, acc.: 54.69%] [G loss: 1.008122]\n",
      "1196 [D loss: 0.718546, acc.: 53.12%] [G loss: 0.967235]\n",
      "1197 [D loss: 0.774944, acc.: 42.19%] [G loss: 1.005217]\n",
      "1198 [D loss: 0.664522, acc.: 57.81%] [G loss: 0.958887]\n",
      "1199 [D loss: 0.730484, acc.: 51.56%] [G loss: 0.888690]\n",
      "1200 [D loss: 0.782761, acc.: 42.19%] [G loss: 0.889473]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1201 [D loss: 0.687345, acc.: 46.88%] [G loss: 0.893450]\n",
      "1202 [D loss: 0.763385, acc.: 46.88%] [G loss: 0.997876]\n",
      "1203 [D loss: 0.707558, acc.: 54.69%] [G loss: 1.021740]\n",
      "1204 [D loss: 0.719621, acc.: 51.56%] [G loss: 0.940198]\n",
      "1205 [D loss: 0.708777, acc.: 56.25%] [G loss: 0.939624]\n",
      "1206 [D loss: 0.740303, acc.: 50.00%] [G loss: 1.091263]\n",
      "1207 [D loss: 0.711714, acc.: 53.12%] [G loss: 0.910909]\n",
      "1208 [D loss: 0.716836, acc.: 51.56%] [G loss: 1.009215]\n",
      "1209 [D loss: 0.724584, acc.: 46.88%] [G loss: 1.053325]\n",
      "1210 [D loss: 0.775402, acc.: 48.44%] [G loss: 1.043920]\n",
      "1211 [D loss: 0.712470, acc.: 59.38%] [G loss: 1.163035]\n",
      "1212 [D loss: 0.660587, acc.: 57.81%] [G loss: 0.970364]\n",
      "1213 [D loss: 0.707691, acc.: 59.38%] [G loss: 1.097141]\n",
      "1214 [D loss: 0.767432, acc.: 46.88%] [G loss: 1.004807]\n",
      "1215 [D loss: 0.756146, acc.: 50.00%] [G loss: 0.988196]\n",
      "1216 [D loss: 0.670972, acc.: 62.50%] [G loss: 0.899380]\n",
      "1217 [D loss: 0.723611, acc.: 54.69%] [G loss: 0.910629]\n",
      "1218 [D loss: 0.670735, acc.: 62.50%] [G loss: 0.841053]\n",
      "1219 [D loss: 0.687311, acc.: 64.06%] [G loss: 0.873342]\n",
      "1220 [D loss: 0.734761, acc.: 60.94%] [G loss: 1.095698]\n",
      "1221 [D loss: 0.675630, acc.: 56.25%] [G loss: 0.996821]\n",
      "1222 [D loss: 0.701188, acc.: 53.12%] [G loss: 0.957201]\n",
      "1223 [D loss: 0.722773, acc.: 51.56%] [G loss: 0.959667]\n",
      "1224 [D loss: 0.680796, acc.: 59.38%] [G loss: 1.032558]\n",
      "1225 [D loss: 0.722136, acc.: 51.56%] [G loss: 1.050976]\n",
      "1226 [D loss: 0.624782, acc.: 59.38%] [G loss: 1.060545]\n",
      "1227 [D loss: 0.719165, acc.: 54.69%] [G loss: 1.014879]\n",
      "1228 [D loss: 0.620216, acc.: 64.06%] [G loss: 1.048229]\n",
      "1229 [D loss: 0.655496, acc.: 64.06%] [G loss: 1.031484]\n",
      "1230 [D loss: 0.644913, acc.: 64.06%] [G loss: 0.915607]\n",
      "1231 [D loss: 0.708026, acc.: 57.81%] [G loss: 0.890110]\n",
      "1232 [D loss: 0.690074, acc.: 59.38%] [G loss: 0.992364]\n",
      "1233 [D loss: 0.764719, acc.: 48.44%] [G loss: 1.096562]\n",
      "1234 [D loss: 0.661595, acc.: 54.69%] [G loss: 1.015105]\n",
      "1235 [D loss: 0.593830, acc.: 68.75%] [G loss: 1.349327]\n",
      "1236 [D loss: 0.733680, acc.: 53.12%] [G loss: 1.020724]\n",
      "1237 [D loss: 0.711942, acc.: 59.38%] [G loss: 1.040517]\n",
      "1238 [D loss: 0.687647, acc.: 54.69%] [G loss: 1.004526]\n",
      "1239 [D loss: 0.734691, acc.: 50.00%] [G loss: 1.107775]\n",
      "1240 [D loss: 0.758613, acc.: 45.31%] [G loss: 0.887081]\n",
      "1241 [D loss: 0.730080, acc.: 54.69%] [G loss: 1.008366]\n",
      "1242 [D loss: 0.706061, acc.: 51.56%] [G loss: 1.121696]\n",
      "1243 [D loss: 0.736663, acc.: 45.31%] [G loss: 0.861583]\n",
      "1244 [D loss: 0.763959, acc.: 48.44%] [G loss: 0.909517]\n",
      "1245 [D loss: 0.761444, acc.: 46.88%] [G loss: 1.040009]\n",
      "1246 [D loss: 0.704768, acc.: 48.44%] [G loss: 1.026919]\n",
      "1247 [D loss: 0.585811, acc.: 73.44%] [G loss: 1.031204]\n",
      "1248 [D loss: 0.739965, acc.: 54.69%] [G loss: 1.062285]\n",
      "1249 [D loss: 0.663150, acc.: 59.38%] [G loss: 0.949610]\n",
      "1250 [D loss: 0.705832, acc.: 51.56%] [G loss: 1.033023]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1251 [D loss: 0.775393, acc.: 43.75%] [G loss: 0.977291]\n",
      "1252 [D loss: 0.676439, acc.: 60.94%] [G loss: 1.140654]\n",
      "1253 [D loss: 0.738208, acc.: 54.69%] [G loss: 1.041469]\n",
      "1254 [D loss: 0.822699, acc.: 46.88%] [G loss: 0.964442]\n",
      "1255 [D loss: 0.758541, acc.: 50.00%] [G loss: 1.039952]\n",
      "1256 [D loss: 0.671967, acc.: 59.38%] [G loss: 1.202210]\n",
      "1257 [D loss: 0.737322, acc.: 43.75%] [G loss: 0.942349]\n",
      "1258 [D loss: 0.697389, acc.: 51.56%] [G loss: 1.068993]\n",
      "1259 [D loss: 0.680012, acc.: 48.44%] [G loss: 1.095013]\n",
      "1260 [D loss: 0.753408, acc.: 50.00%] [G loss: 0.909194]\n",
      "1261 [D loss: 0.701908, acc.: 59.38%] [G loss: 1.150790]\n",
      "1262 [D loss: 0.636074, acc.: 64.06%] [G loss: 1.000990]\n",
      "1263 [D loss: 0.751919, acc.: 57.81%] [G loss: 1.069421]\n",
      "1264 [D loss: 0.674560, acc.: 60.94%] [G loss: 1.045809]\n",
      "1265 [D loss: 0.700700, acc.: 50.00%] [G loss: 0.976000]\n",
      "1266 [D loss: 0.692529, acc.: 46.88%] [G loss: 0.882329]\n",
      "1267 [D loss: 0.774946, acc.: 51.56%] [G loss: 1.040162]\n",
      "1268 [D loss: 0.658198, acc.: 64.06%] [G loss: 1.048210]\n",
      "1269 [D loss: 0.657000, acc.: 62.50%] [G loss: 1.051277]\n",
      "1270 [D loss: 0.706905, acc.: 53.12%] [G loss: 1.056015]\n",
      "1271 [D loss: 0.672876, acc.: 62.50%] [G loss: 1.142308]\n",
      "1272 [D loss: 0.710862, acc.: 53.12%] [G loss: 1.012297]\n",
      "1273 [D loss: 0.732029, acc.: 50.00%] [G loss: 0.938062]\n",
      "1274 [D loss: 0.794629, acc.: 48.44%] [G loss: 1.081180]\n",
      "1275 [D loss: 0.680579, acc.: 57.81%] [G loss: 0.920122]\n",
      "1276 [D loss: 0.648468, acc.: 59.38%] [G loss: 1.039019]\n",
      "1277 [D loss: 0.696814, acc.: 48.44%] [G loss: 0.921713]\n",
      "1278 [D loss: 0.746616, acc.: 48.44%] [G loss: 0.830540]\n",
      "1279 [D loss: 0.797531, acc.: 48.44%] [G loss: 0.892805]\n",
      "1280 [D loss: 0.726886, acc.: 46.88%] [G loss: 1.033844]\n",
      "1281 [D loss: 0.693517, acc.: 53.12%] [G loss: 0.996386]\n",
      "1282 [D loss: 0.784960, acc.: 51.56%] [G loss: 1.028097]\n",
      "1283 [D loss: 0.697808, acc.: 59.38%] [G loss: 1.064409]\n",
      "1284 [D loss: 0.712540, acc.: 57.81%] [G loss: 0.956616]\n",
      "1285 [D loss: 0.643825, acc.: 65.62%] [G loss: 0.930683]\n",
      "1286 [D loss: 0.667176, acc.: 60.94%] [G loss: 0.952622]\n",
      "1287 [D loss: 0.705521, acc.: 56.25%] [G loss: 1.003946]\n",
      "1288 [D loss: 0.668632, acc.: 56.25%] [G loss: 1.117496]\n",
      "1289 [D loss: 0.831251, acc.: 42.19%] [G loss: 1.084925]\n",
      "1290 [D loss: 0.767758, acc.: 45.31%] [G loss: 0.994942]\n",
      "1291 [D loss: 0.648736, acc.: 59.38%] [G loss: 1.001883]\n",
      "1292 [D loss: 0.719717, acc.: 53.12%] [G loss: 0.817468]\n",
      "1293 [D loss: 0.622630, acc.: 67.19%] [G loss: 1.023354]\n",
      "1294 [D loss: 0.709712, acc.: 59.38%] [G loss: 0.877925]\n",
      "1295 [D loss: 0.732420, acc.: 57.81%] [G loss: 0.993100]\n",
      "1296 [D loss: 0.623705, acc.: 67.19%] [G loss: 0.972421]\n",
      "1297 [D loss: 0.729324, acc.: 54.69%] [G loss: 1.059300]\n",
      "1298 [D loss: 0.719798, acc.: 45.31%] [G loss: 0.959697]\n",
      "1299 [D loss: 0.783272, acc.: 50.00%] [G loss: 0.973077]\n",
      "1300 [D loss: 0.696102, acc.: 60.94%] [G loss: 0.932533]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1301 [D loss: 0.750399, acc.: 48.44%] [G loss: 0.980308]\n",
      "1302 [D loss: 0.698318, acc.: 57.81%] [G loss: 1.053951]\n",
      "1303 [D loss: 0.753686, acc.: 50.00%] [G loss: 1.018059]\n",
      "1304 [D loss: 0.758960, acc.: 48.44%] [G loss: 0.945251]\n",
      "1305 [D loss: 0.636858, acc.: 65.62%] [G loss: 0.926645]\n",
      "1306 [D loss: 0.717731, acc.: 54.69%] [G loss: 1.051112]\n",
      "1307 [D loss: 0.830735, acc.: 34.38%] [G loss: 0.964319]\n",
      "1308 [D loss: 0.671982, acc.: 65.62%] [G loss: 0.992750]\n",
      "1309 [D loss: 0.704914, acc.: 57.81%] [G loss: 1.120269]\n",
      "1310 [D loss: 0.752997, acc.: 42.19%] [G loss: 1.117791]\n",
      "1311 [D loss: 0.732101, acc.: 51.56%] [G loss: 0.978291]\n",
      "1312 [D loss: 0.736089, acc.: 53.12%] [G loss: 0.892591]\n",
      "1313 [D loss: 0.697168, acc.: 54.69%] [G loss: 0.976293]\n",
      "1314 [D loss: 0.626347, acc.: 65.62%] [G loss: 1.016897]\n",
      "1315 [D loss: 0.720550, acc.: 53.12%] [G loss: 0.873620]\n",
      "1316 [D loss: 0.763444, acc.: 43.75%] [G loss: 0.986170]\n",
      "1317 [D loss: 0.755401, acc.: 51.56%] [G loss: 1.109305]\n",
      "1318 [D loss: 0.737603, acc.: 46.88%] [G loss: 0.869061]\n",
      "1319 [D loss: 0.692835, acc.: 60.94%] [G loss: 1.066754]\n",
      "1320 [D loss: 0.789026, acc.: 37.50%] [G loss: 0.966612]\n",
      "1321 [D loss: 0.669693, acc.: 54.69%] [G loss: 1.009047]\n",
      "1322 [D loss: 0.791518, acc.: 48.44%] [G loss: 1.187066]\n",
      "1323 [D loss: 0.679657, acc.: 59.38%] [G loss: 1.118500]\n",
      "1324 [D loss: 0.715424, acc.: 56.25%] [G loss: 1.137873]\n",
      "1325 [D loss: 0.712398, acc.: 54.69%] [G loss: 0.811998]\n",
      "1326 [D loss: 0.748666, acc.: 46.88%] [G loss: 1.001606]\n",
      "1327 [D loss: 0.745368, acc.: 54.69%] [G loss: 0.814292]\n",
      "1328 [D loss: 0.673015, acc.: 59.38%] [G loss: 0.898781]\n",
      "1329 [D loss: 0.752762, acc.: 50.00%] [G loss: 0.942664]\n",
      "1330 [D loss: 0.702355, acc.: 59.38%] [G loss: 0.883714]\n",
      "1331 [D loss: 0.779735, acc.: 45.31%] [G loss: 1.064978]\n",
      "1332 [D loss: 0.680212, acc.: 56.25%] [G loss: 1.017174]\n",
      "1333 [D loss: 0.636064, acc.: 64.06%] [G loss: 0.993279]\n",
      "1334 [D loss: 0.733580, acc.: 51.56%] [G loss: 0.975542]\n",
      "1335 [D loss: 0.732020, acc.: 46.88%] [G loss: 0.928235]\n",
      "1336 [D loss: 0.594691, acc.: 71.88%] [G loss: 1.000367]\n",
      "1337 [D loss: 0.744627, acc.: 51.56%] [G loss: 0.953948]\n",
      "1338 [D loss: 0.736521, acc.: 51.56%] [G loss: 0.869653]\n",
      "1339 [D loss: 0.691176, acc.: 48.44%] [G loss: 0.988131]\n",
      "1340 [D loss: 0.631847, acc.: 73.44%] [G loss: 0.958494]\n",
      "1341 [D loss: 0.690173, acc.: 56.25%] [G loss: 1.082776]\n",
      "1342 [D loss: 0.723159, acc.: 53.12%] [G loss: 1.013490]\n",
      "1343 [D loss: 0.692893, acc.: 54.69%] [G loss: 0.995458]\n",
      "1344 [D loss: 0.667710, acc.: 60.94%] [G loss: 1.016033]\n",
      "1345 [D loss: 0.828903, acc.: 35.94%] [G loss: 0.939781]\n",
      "1346 [D loss: 0.802053, acc.: 42.19%] [G loss: 0.971241]\n",
      "1347 [D loss: 0.652913, acc.: 59.38%] [G loss: 0.988325]\n",
      "1348 [D loss: 0.688736, acc.: 54.69%] [G loss: 0.873708]\n",
      "1349 [D loss: 0.725318, acc.: 54.69%] [G loss: 1.094604]\n",
      "1350 [D loss: 0.699057, acc.: 57.81%] [G loss: 1.098644]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1351 [D loss: 0.671812, acc.: 57.81%] [G loss: 0.970818]\n",
      "1352 [D loss: 0.688800, acc.: 57.81%] [G loss: 1.051899]\n",
      "1353 [D loss: 0.678286, acc.: 60.94%] [G loss: 1.053574]\n",
      "1354 [D loss: 0.673216, acc.: 56.25%] [G loss: 1.114479]\n",
      "1355 [D loss: 0.777484, acc.: 50.00%] [G loss: 0.920822]\n",
      "1356 [D loss: 0.570160, acc.: 71.88%] [G loss: 1.076497]\n",
      "1357 [D loss: 0.685577, acc.: 59.38%] [G loss: 0.949661]\n",
      "1358 [D loss: 0.725570, acc.: 54.69%] [G loss: 1.025097]\n",
      "1359 [D loss: 0.628297, acc.: 67.19%] [G loss: 0.965527]\n",
      "1360 [D loss: 0.724862, acc.: 51.56%] [G loss: 0.950165]\n",
      "1361 [D loss: 0.745127, acc.: 43.75%] [G loss: 1.011285]\n",
      "1362 [D loss: 0.731465, acc.: 51.56%] [G loss: 0.883686]\n",
      "1363 [D loss: 0.721869, acc.: 51.56%] [G loss: 0.822614]\n",
      "1364 [D loss: 0.657185, acc.: 57.81%] [G loss: 1.014184]\n",
      "1365 [D loss: 0.683916, acc.: 59.38%] [G loss: 1.024361]\n",
      "1366 [D loss: 0.816172, acc.: 48.44%] [G loss: 0.902018]\n",
      "1367 [D loss: 0.734940, acc.: 56.25%] [G loss: 0.948403]\n",
      "1368 [D loss: 0.692333, acc.: 64.06%] [G loss: 0.955975]\n",
      "1369 [D loss: 0.599644, acc.: 67.19%] [G loss: 1.114718]\n",
      "1370 [D loss: 0.695448, acc.: 54.69%] [G loss: 0.993662]\n",
      "1371 [D loss: 0.755507, acc.: 53.12%] [G loss: 0.854040]\n",
      "1372 [D loss: 0.740405, acc.: 45.31%] [G loss: 1.031157]\n",
      "1373 [D loss: 0.623354, acc.: 68.75%] [G loss: 1.045250]\n",
      "1374 [D loss: 0.707635, acc.: 59.38%] [G loss: 0.958637]\n",
      "1375 [D loss: 0.667962, acc.: 59.38%] [G loss: 0.842519]\n",
      "1376 [D loss: 0.750294, acc.: 53.12%] [G loss: 0.900718]\n",
      "1377 [D loss: 0.655150, acc.: 59.38%] [G loss: 0.844191]\n",
      "1378 [D loss: 0.744602, acc.: 43.75%] [G loss: 0.978437]\n",
      "1379 [D loss: 0.733364, acc.: 51.56%] [G loss: 0.898373]\n",
      "1380 [D loss: 0.673952, acc.: 60.94%] [G loss: 0.963274]\n",
      "1381 [D loss: 0.776136, acc.: 51.56%] [G loss: 0.893251]\n",
      "1382 [D loss: 0.703293, acc.: 59.38%] [G loss: 0.985581]\n",
      "1383 [D loss: 0.750729, acc.: 51.56%] [G loss: 0.883900]\n",
      "1384 [D loss: 0.659690, acc.: 59.38%] [G loss: 1.002822]\n",
      "1385 [D loss: 0.800601, acc.: 50.00%] [G loss: 0.984494]\n",
      "1386 [D loss: 0.678591, acc.: 60.94%] [G loss: 1.222814]\n",
      "1387 [D loss: 0.660912, acc.: 53.12%] [G loss: 0.894461]\n",
      "1388 [D loss: 0.639111, acc.: 60.94%] [G loss: 0.898535]\n",
      "1389 [D loss: 0.622776, acc.: 67.19%] [G loss: 1.058059]\n",
      "1390 [D loss: 0.666255, acc.: 62.50%] [G loss: 0.984651]\n",
      "1391 [D loss: 0.623271, acc.: 60.94%] [G loss: 1.089603]\n",
      "1392 [D loss: 0.785766, acc.: 46.88%] [G loss: 0.970081]\n",
      "1393 [D loss: 0.798490, acc.: 50.00%] [G loss: 0.952881]\n",
      "1394 [D loss: 0.749787, acc.: 56.25%] [G loss: 1.012376]\n",
      "1395 [D loss: 0.627746, acc.: 65.62%] [G loss: 1.247450]\n",
      "1396 [D loss: 0.659401, acc.: 70.31%] [G loss: 1.002431]\n",
      "1397 [D loss: 0.659805, acc.: 57.81%] [G loss: 1.091426]\n",
      "1398 [D loss: 0.768087, acc.: 50.00%] [G loss: 0.919538]\n",
      "1399 [D loss: 0.766685, acc.: 54.69%] [G loss: 0.944786]\n",
      "1400 [D loss: 0.716630, acc.: 56.25%] [G loss: 0.796961]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1401 [D loss: 0.738348, acc.: 51.56%] [G loss: 1.019225]\n",
      "1402 [D loss: 0.673424, acc.: 65.62%] [G loss: 0.884632]\n",
      "1403 [D loss: 0.723721, acc.: 51.56%] [G loss: 0.912993]\n",
      "1404 [D loss: 0.653394, acc.: 60.94%] [G loss: 0.984851]\n",
      "1405 [D loss: 0.697873, acc.: 53.12%] [G loss: 0.969341]\n",
      "1406 [D loss: 0.810784, acc.: 45.31%] [G loss: 0.821852]\n",
      "1407 [D loss: 0.663675, acc.: 57.81%] [G loss: 0.927716]\n",
      "1408 [D loss: 0.762547, acc.: 51.56%] [G loss: 0.986314]\n",
      "1409 [D loss: 0.653220, acc.: 62.50%] [G loss: 1.020639]\n",
      "1410 [D loss: 0.776335, acc.: 50.00%] [G loss: 0.966230]\n",
      "1411 [D loss: 0.741621, acc.: 50.00%] [G loss: 1.020469]\n",
      "1412 [D loss: 0.699700, acc.: 50.00%] [G loss: 0.970941]\n",
      "1413 [D loss: 0.700719, acc.: 59.38%] [G loss: 0.968335]\n",
      "1414 [D loss: 0.739353, acc.: 46.88%] [G loss: 0.979665]\n",
      "1415 [D loss: 0.715198, acc.: 53.12%] [G loss: 0.958753]\n",
      "1416 [D loss: 0.820939, acc.: 43.75%] [G loss: 0.838250]\n",
      "1417 [D loss: 0.751674, acc.: 54.69%] [G loss: 0.997540]\n",
      "1418 [D loss: 0.643180, acc.: 62.50%] [G loss: 1.007033]\n",
      "1419 [D loss: 0.680550, acc.: 57.81%] [G loss: 0.906730]\n",
      "1420 [D loss: 0.638885, acc.: 59.38%] [G loss: 0.954054]\n",
      "1421 [D loss: 0.656628, acc.: 59.38%] [G loss: 1.045590]\n",
      "1422 [D loss: 0.644239, acc.: 60.94%] [G loss: 1.037503]\n",
      "1423 [D loss: 0.724165, acc.: 57.81%] [G loss: 0.984711]\n",
      "1424 [D loss: 0.691625, acc.: 56.25%] [G loss: 1.067182]\n",
      "1425 [D loss: 0.695445, acc.: 60.94%] [G loss: 1.090333]\n",
      "1426 [D loss: 0.660917, acc.: 62.50%] [G loss: 1.020288]\n",
      "1427 [D loss: 0.725791, acc.: 62.50%] [G loss: 1.072551]\n",
      "1428 [D loss: 0.689547, acc.: 65.62%] [G loss: 0.976111]\n",
      "1429 [D loss: 0.672878, acc.: 54.69%] [G loss: 1.090435]\n",
      "1430 [D loss: 0.731024, acc.: 53.12%] [G loss: 0.996616]\n",
      "1431 [D loss: 0.649985, acc.: 64.06%] [G loss: 0.763126]\n",
      "1432 [D loss: 0.618606, acc.: 65.62%] [G loss: 1.089195]\n",
      "1433 [D loss: 0.680078, acc.: 60.94%] [G loss: 1.212030]\n",
      "1434 [D loss: 0.726671, acc.: 54.69%] [G loss: 1.208232]\n",
      "1435 [D loss: 0.706648, acc.: 54.69%] [G loss: 0.966713]\n",
      "1436 [D loss: 0.747676, acc.: 51.56%] [G loss: 0.829598]\n",
      "1437 [D loss: 0.674306, acc.: 59.38%] [G loss: 1.018836]\n",
      "1438 [D loss: 0.710322, acc.: 53.12%] [G loss: 0.961303]\n",
      "1439 [D loss: 0.605828, acc.: 65.62%] [G loss: 1.041632]\n",
      "1440 [D loss: 0.625848, acc.: 65.62%] [G loss: 0.943486]\n",
      "1441 [D loss: 0.632624, acc.: 67.19%] [G loss: 0.996696]\n",
      "1442 [D loss: 0.698944, acc.: 54.69%] [G loss: 1.042600]\n",
      "1443 [D loss: 0.711760, acc.: 54.69%] [G loss: 0.989227]\n",
      "1444 [D loss: 0.714027, acc.: 51.56%] [G loss: 1.003643]\n",
      "1445 [D loss: 0.734079, acc.: 54.69%] [G loss: 1.033710]\n",
      "1446 [D loss: 0.631857, acc.: 64.06%] [G loss: 1.077108]\n",
      "1447 [D loss: 0.748322, acc.: 46.88%] [G loss: 1.039445]\n",
      "1448 [D loss: 0.704040, acc.: 54.69%] [G loss: 1.047980]\n",
      "1449 [D loss: 0.839037, acc.: 42.19%] [G loss: 1.014901]\n",
      "1450 [D loss: 0.696712, acc.: 54.69%] [G loss: 0.806902]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1451 [D loss: 0.685636, acc.: 48.44%] [G loss: 0.996832]\n",
      "1452 [D loss: 0.831882, acc.: 40.62%] [G loss: 0.997667]\n",
      "1453 [D loss: 0.713582, acc.: 56.25%] [G loss: 1.050309]\n",
      "1454 [D loss: 0.686257, acc.: 59.38%] [G loss: 1.035034]\n",
      "1455 [D loss: 0.788972, acc.: 50.00%] [G loss: 0.896555]\n",
      "1456 [D loss: 0.579378, acc.: 71.88%] [G loss: 1.088150]\n",
      "1457 [D loss: 0.713561, acc.: 57.81%] [G loss: 1.040251]\n",
      "1458 [D loss: 0.674054, acc.: 62.50%] [G loss: 0.921545]\n",
      "1459 [D loss: 0.670908, acc.: 60.94%] [G loss: 0.985885]\n",
      "1460 [D loss: 0.763574, acc.: 50.00%] [G loss: 1.104858]\n",
      "1461 [D loss: 0.714755, acc.: 45.31%] [G loss: 0.981120]\n",
      "1462 [D loss: 0.812716, acc.: 39.06%] [G loss: 1.046191]\n",
      "1463 [D loss: 0.641505, acc.: 65.62%] [G loss: 1.127328]\n",
      "1464 [D loss: 0.664834, acc.: 60.94%] [G loss: 1.113174]\n",
      "1465 [D loss: 0.707332, acc.: 54.69%] [G loss: 1.046195]\n",
      "1466 [D loss: 0.643751, acc.: 65.62%] [G loss: 0.892045]\n",
      "1467 [D loss: 0.785026, acc.: 45.31%] [G loss: 0.898541]\n",
      "1468 [D loss: 0.712000, acc.: 56.25%] [G loss: 0.909670]\n",
      "1469 [D loss: 0.661133, acc.: 64.06%] [G loss: 0.979417]\n",
      "1470 [D loss: 0.753382, acc.: 54.69%] [G loss: 0.769746]\n",
      "1471 [D loss: 0.712146, acc.: 48.44%] [G loss: 1.067511]\n",
      "1472 [D loss: 0.716637, acc.: 51.56%] [G loss: 1.014283]\n",
      "1473 [D loss: 0.635729, acc.: 60.94%] [G loss: 0.999383]\n",
      "1474 [D loss: 0.707879, acc.: 57.81%] [G loss: 1.023361]\n",
      "1475 [D loss: 0.720936, acc.: 57.81%] [G loss: 1.029746]\n",
      "1476 [D loss: 0.734121, acc.: 56.25%] [G loss: 1.154038]\n",
      "1477 [D loss: 0.576549, acc.: 71.88%] [G loss: 1.141949]\n",
      "1478 [D loss: 0.686800, acc.: 59.38%] [G loss: 0.977154]\n",
      "1479 [D loss: 0.716837, acc.: 54.69%] [G loss: 0.737785]\n",
      "1480 [D loss: 0.628825, acc.: 64.06%] [G loss: 0.994122]\n",
      "1481 [D loss: 0.819261, acc.: 45.31%] [G loss: 1.088719]\n",
      "1482 [D loss: 0.676641, acc.: 59.38%] [G loss: 0.872784]\n",
      "1483 [D loss: 0.616338, acc.: 64.06%] [G loss: 0.982851]\n",
      "1484 [D loss: 0.678025, acc.: 54.69%] [G loss: 0.957561]\n",
      "1485 [D loss: 0.548898, acc.: 76.56%] [G loss: 1.012585]\n",
      "1486 [D loss: 0.677741, acc.: 60.94%] [G loss: 0.915956]\n",
      "1487 [D loss: 0.651775, acc.: 65.62%] [G loss: 1.071166]\n",
      "1488 [D loss: 0.758880, acc.: 54.69%] [G loss: 0.878644]\n",
      "1489 [D loss: 0.703085, acc.: 48.44%] [G loss: 0.935054]\n",
      "1490 [D loss: 0.738584, acc.: 51.56%] [G loss: 0.863609]\n",
      "1491 [D loss: 0.747963, acc.: 54.69%] [G loss: 0.934835]\n",
      "1492 [D loss: 0.690072, acc.: 50.00%] [G loss: 0.934634]\n",
      "1493 [D loss: 0.667380, acc.: 54.69%] [G loss: 1.049930]\n",
      "1494 [D loss: 0.749809, acc.: 53.12%] [G loss: 0.856566]\n",
      "1495 [D loss: 0.644608, acc.: 70.31%] [G loss: 1.039025]\n",
      "1496 [D loss: 0.676423, acc.: 57.81%] [G loss: 0.968153]\n",
      "1497 [D loss: 0.684568, acc.: 57.81%] [G loss: 1.110517]\n",
      "1498 [D loss: 0.711122, acc.: 56.25%] [G loss: 1.025161]\n",
      "1499 [D loss: 0.805098, acc.: 46.88%] [G loss: 0.905041]\n",
      "1500 [D loss: 0.719546, acc.: 51.56%] [G loss: 0.870297]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1501 [D loss: 0.668096, acc.: 62.50%] [G loss: 0.945934]\n",
      "1502 [D loss: 0.683178, acc.: 68.75%] [G loss: 0.927456]\n",
      "1503 [D loss: 0.683929, acc.: 54.69%] [G loss: 1.067044]\n",
      "1504 [D loss: 0.683594, acc.: 56.25%] [G loss: 0.951371]\n",
      "1505 [D loss: 0.742247, acc.: 50.00%] [G loss: 1.025595]\n",
      "1506 [D loss: 0.662488, acc.: 67.19%] [G loss: 0.818431]\n",
      "1507 [D loss: 0.726194, acc.: 53.12%] [G loss: 0.817964]\n",
      "1508 [D loss: 0.681156, acc.: 56.25%] [G loss: 0.872205]\n",
      "1509 [D loss: 0.657865, acc.: 60.94%] [G loss: 0.906288]\n",
      "1510 [D loss: 0.676523, acc.: 57.81%] [G loss: 0.889683]\n",
      "1511 [D loss: 0.661239, acc.: 64.06%] [G loss: 1.036373]\n",
      "1512 [D loss: 0.796541, acc.: 46.88%] [G loss: 1.031042]\n",
      "1513 [D loss: 0.745486, acc.: 53.12%] [G loss: 0.988429]\n",
      "1514 [D loss: 0.668460, acc.: 57.81%] [G loss: 0.851434]\n",
      "1515 [D loss: 0.698484, acc.: 56.25%] [G loss: 1.046242]\n",
      "1516 [D loss: 0.747698, acc.: 51.56%] [G loss: 0.999915]\n",
      "1517 [D loss: 0.695930, acc.: 60.94%] [G loss: 0.931505]\n",
      "1518 [D loss: 0.753828, acc.: 53.12%] [G loss: 0.921840]\n",
      "1519 [D loss: 0.690724, acc.: 51.56%] [G loss: 1.000154]\n",
      "1520 [D loss: 0.750008, acc.: 48.44%] [G loss: 0.858343]\n",
      "1521 [D loss: 0.660148, acc.: 62.50%] [G loss: 1.037924]\n",
      "1522 [D loss: 0.666830, acc.: 53.12%] [G loss: 0.861769]\n",
      "1523 [D loss: 0.658913, acc.: 64.06%] [G loss: 0.985442]\n",
      "1524 [D loss: 0.695932, acc.: 56.25%] [G loss: 1.055353]\n",
      "1525 [D loss: 0.656203, acc.: 60.94%] [G loss: 0.936398]\n",
      "1526 [D loss: 0.783995, acc.: 48.44%] [G loss: 0.795556]\n",
      "1527 [D loss: 0.661651, acc.: 59.38%] [G loss: 0.850078]\n",
      "1528 [D loss: 0.709873, acc.: 46.88%] [G loss: 0.986303]\n",
      "1529 [D loss: 0.679194, acc.: 59.38%] [G loss: 0.856193]\n",
      "1530 [D loss: 0.703406, acc.: 53.12%] [G loss: 1.095235]\n",
      "1531 [D loss: 0.773669, acc.: 51.56%] [G loss: 0.911054]\n",
      "1532 [D loss: 0.680254, acc.: 54.69%] [G loss: 1.035064]\n",
      "1533 [D loss: 0.844771, acc.: 35.94%] [G loss: 1.035049]\n",
      "1534 [D loss: 0.679422, acc.: 57.81%] [G loss: 0.965607]\n",
      "1535 [D loss: 0.635250, acc.: 62.50%] [G loss: 1.043845]\n",
      "1536 [D loss: 0.703364, acc.: 57.81%] [G loss: 0.846682]\n",
      "1537 [D loss: 0.675800, acc.: 54.69%] [G loss: 0.878573]\n",
      "1538 [D loss: 0.710066, acc.: 60.94%] [G loss: 0.979046]\n",
      "1539 [D loss: 0.747048, acc.: 48.44%] [G loss: 1.024310]\n",
      "1540 [D loss: 0.663011, acc.: 64.06%] [G loss: 0.867729]\n",
      "1541 [D loss: 0.667096, acc.: 56.25%] [G loss: 0.938137]\n",
      "1542 [D loss: 0.643634, acc.: 59.38%] [G loss: 1.072595]\n",
      "1543 [D loss: 0.674850, acc.: 57.81%] [G loss: 0.846451]\n",
      "1544 [D loss: 0.770849, acc.: 43.75%] [G loss: 0.978106]\n",
      "1545 [D loss: 0.710705, acc.: 50.00%] [G loss: 1.004442]\n",
      "1546 [D loss: 0.647824, acc.: 64.06%] [G loss: 0.981080]\n",
      "1547 [D loss: 0.618073, acc.: 65.62%] [G loss: 1.001735]\n",
      "1548 [D loss: 0.760378, acc.: 53.12%] [G loss: 1.080168]\n",
      "1549 [D loss: 0.740720, acc.: 54.69%] [G loss: 0.895337]\n",
      "1550 [D loss: 0.695434, acc.: 65.62%] [G loss: 0.997976]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1551 [D loss: 0.718895, acc.: 53.12%] [G loss: 1.059772]\n",
      "1552 [D loss: 0.739127, acc.: 53.12%] [G loss: 0.877239]\n",
      "1553 [D loss: 0.706890, acc.: 51.56%] [G loss: 1.036689]\n",
      "1554 [D loss: 0.686034, acc.: 65.62%] [G loss: 1.053490]\n",
      "1555 [D loss: 0.804690, acc.: 42.19%] [G loss: 0.970422]\n",
      "1556 [D loss: 0.644317, acc.: 67.19%] [G loss: 1.049780]\n",
      "1557 [D loss: 0.690323, acc.: 57.81%] [G loss: 1.001068]\n",
      "1558 [D loss: 0.740933, acc.: 48.44%] [G loss: 1.089698]\n",
      "1559 [D loss: 0.673598, acc.: 59.38%] [G loss: 1.045121]\n",
      "1560 [D loss: 0.753352, acc.: 46.88%] [G loss: 1.051549]\n",
      "1561 [D loss: 0.797517, acc.: 46.88%] [G loss: 1.027180]\n",
      "1562 [D loss: 0.692356, acc.: 50.00%] [G loss: 1.238249]\n",
      "1563 [D loss: 0.641016, acc.: 60.94%] [G loss: 0.895835]\n",
      "1564 [D loss: 0.672061, acc.: 65.62%] [G loss: 0.872090]\n",
      "1565 [D loss: 0.711882, acc.: 56.25%] [G loss: 0.854693]\n",
      "1566 [D loss: 0.629052, acc.: 59.38%] [G loss: 0.954298]\n",
      "1567 [D loss: 0.643524, acc.: 59.38%] [G loss: 1.018452]\n",
      "1568 [D loss: 0.629858, acc.: 60.94%] [G loss: 0.906442]\n",
      "1569 [D loss: 0.744808, acc.: 53.12%] [G loss: 1.010613]\n",
      "1570 [D loss: 0.646648, acc.: 65.62%] [G loss: 1.023860]\n",
      "1571 [D loss: 0.657312, acc.: 60.94%] [G loss: 0.939384]\n",
      "1572 [D loss: 0.746875, acc.: 50.00%] [G loss: 0.887577]\n",
      "1573 [D loss: 0.691077, acc.: 57.81%] [G loss: 0.860586]\n",
      "1574 [D loss: 0.654245, acc.: 56.25%] [G loss: 0.908857]\n",
      "1575 [D loss: 0.722167, acc.: 54.69%] [G loss: 0.899905]\n",
      "1576 [D loss: 0.653748, acc.: 59.38%] [G loss: 0.902604]\n",
      "1577 [D loss: 0.619880, acc.: 64.06%] [G loss: 0.869586]\n",
      "1578 [D loss: 0.621974, acc.: 65.62%] [G loss: 0.969669]\n",
      "1579 [D loss: 0.636933, acc.: 65.62%] [G loss: 0.919378]\n",
      "1580 [D loss: 0.736118, acc.: 45.31%] [G loss: 1.077552]\n",
      "1581 [D loss: 0.711142, acc.: 53.12%] [G loss: 1.006721]\n",
      "1582 [D loss: 0.698510, acc.: 51.56%] [G loss: 1.007987]\n",
      "1583 [D loss: 0.758598, acc.: 54.69%] [G loss: 1.003397]\n",
      "1584 [D loss: 0.754479, acc.: 50.00%] [G loss: 0.944356]\n",
      "1585 [D loss: 0.657830, acc.: 60.94%] [G loss: 1.247458]\n",
      "1586 [D loss: 0.682208, acc.: 54.69%] [G loss: 0.993226]\n",
      "1587 [D loss: 0.652091, acc.: 56.25%] [G loss: 0.986736]\n",
      "1588 [D loss: 0.723628, acc.: 54.69%] [G loss: 0.853471]\n",
      "1589 [D loss: 0.579319, acc.: 76.56%] [G loss: 0.997314]\n",
      "1590 [D loss: 0.709815, acc.: 43.75%] [G loss: 1.101215]\n",
      "1591 [D loss: 0.634320, acc.: 67.19%] [G loss: 0.802605]\n",
      "1592 [D loss: 0.703841, acc.: 51.56%] [G loss: 1.011744]\n",
      "1593 [D loss: 0.736462, acc.: 56.25%] [G loss: 0.899286]\n",
      "1594 [D loss: 0.699521, acc.: 51.56%] [G loss: 1.049276]\n",
      "1595 [D loss: 0.582376, acc.: 73.44%] [G loss: 0.934385]\n",
      "1596 [D loss: 0.773502, acc.: 46.88%] [G loss: 0.904202]\n",
      "1597 [D loss: 0.693045, acc.: 59.38%] [G loss: 1.001653]\n",
      "1598 [D loss: 0.751124, acc.: 57.81%] [G loss: 0.801500]\n",
      "1599 [D loss: 0.745513, acc.: 51.56%] [G loss: 0.925247]\n",
      "1600 [D loss: 0.643546, acc.: 57.81%] [G loss: 1.018912]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1601 [D loss: 0.782258, acc.: 42.19%] [G loss: 0.789669]\n",
      "1602 [D loss: 0.761102, acc.: 51.56%] [G loss: 0.881903]\n",
      "1603 [D loss: 0.642477, acc.: 60.94%] [G loss: 0.898887]\n",
      "1604 [D loss: 0.716822, acc.: 54.69%] [G loss: 0.934074]\n",
      "1605 [D loss: 0.712798, acc.: 53.12%] [G loss: 0.960489]\n",
      "1606 [D loss: 0.654920, acc.: 65.62%] [G loss: 0.916948]\n",
      "1607 [D loss: 0.735163, acc.: 54.69%] [G loss: 0.935951]\n",
      "1608 [D loss: 0.663025, acc.: 53.12%] [G loss: 0.990927]\n",
      "1609 [D loss: 0.737242, acc.: 57.81%] [G loss: 0.955883]\n",
      "1610 [D loss: 0.792480, acc.: 43.75%] [G loss: 0.876538]\n",
      "1611 [D loss: 0.661402, acc.: 64.06%] [G loss: 0.978939]\n",
      "1612 [D loss: 0.727868, acc.: 51.56%] [G loss: 0.920901]\n",
      "1613 [D loss: 0.726546, acc.: 54.69%] [G loss: 0.991393]\n",
      "1614 [D loss: 0.681380, acc.: 57.81%] [G loss: 1.168203]\n",
      "1615 [D loss: 0.778984, acc.: 43.75%] [G loss: 1.003569]\n",
      "1616 [D loss: 0.631383, acc.: 64.06%] [G loss: 0.984150]\n",
      "1617 [D loss: 0.698931, acc.: 51.56%] [G loss: 1.110300]\n",
      "1618 [D loss: 0.630071, acc.: 64.06%] [G loss: 0.935607]\n",
      "1619 [D loss: 0.715058, acc.: 57.81%] [G loss: 0.967963]\n",
      "1620 [D loss: 0.737701, acc.: 48.44%] [G loss: 0.994328]\n",
      "1621 [D loss: 0.794760, acc.: 46.88%] [G loss: 0.925886]\n",
      "1622 [D loss: 0.690166, acc.: 51.56%] [G loss: 0.976384]\n",
      "1623 [D loss: 0.704911, acc.: 57.81%] [G loss: 0.762019]\n",
      "1624 [D loss: 0.745733, acc.: 45.31%] [G loss: 0.865405]\n",
      "1625 [D loss: 0.684793, acc.: 54.69%] [G loss: 1.053290]\n",
      "1626 [D loss: 0.731655, acc.: 48.44%] [G loss: 0.867164]\n",
      "1627 [D loss: 0.704395, acc.: 57.81%] [G loss: 1.002446]\n",
      "1628 [D loss: 0.672233, acc.: 57.81%] [G loss: 0.987031]\n",
      "1629 [D loss: 0.647144, acc.: 56.25%] [G loss: 1.151438]\n",
      "1630 [D loss: 0.679256, acc.: 53.12%] [G loss: 0.940023]\n",
      "1631 [D loss: 0.636567, acc.: 65.62%] [G loss: 1.042873]\n",
      "1632 [D loss: 0.734721, acc.: 53.12%] [G loss: 0.876550]\n",
      "1633 [D loss: 0.626324, acc.: 68.75%] [G loss: 0.817949]\n",
      "1634 [D loss: 0.734880, acc.: 51.56%] [G loss: 0.961234]\n",
      "1635 [D loss: 0.657170, acc.: 60.94%] [G loss: 1.015269]\n",
      "1636 [D loss: 0.645007, acc.: 57.81%] [G loss: 0.940255]\n",
      "1637 [D loss: 0.742698, acc.: 46.88%] [G loss: 0.877307]\n",
      "1638 [D loss: 0.728094, acc.: 59.38%] [G loss: 0.943111]\n",
      "1639 [D loss: 0.632710, acc.: 71.88%] [G loss: 1.000406]\n",
      "1640 [D loss: 0.711516, acc.: 50.00%] [G loss: 0.939461]\n",
      "1641 [D loss: 0.646577, acc.: 68.75%] [G loss: 0.976271]\n",
      "1642 [D loss: 0.686148, acc.: 56.25%] [G loss: 0.911862]\n",
      "1643 [D loss: 0.764841, acc.: 48.44%] [G loss: 0.943151]\n",
      "1644 [D loss: 0.627942, acc.: 67.19%] [G loss: 1.112731]\n",
      "1645 [D loss: 0.633602, acc.: 64.06%] [G loss: 0.891577]\n",
      "1646 [D loss: 0.743443, acc.: 50.00%] [G loss: 0.925445]\n",
      "1647 [D loss: 0.712569, acc.: 53.12%] [G loss: 1.074533]\n",
      "1648 [D loss: 0.753152, acc.: 59.38%] [G loss: 1.081259]\n",
      "1649 [D loss: 0.670082, acc.: 53.12%] [G loss: 1.041325]\n",
      "1650 [D loss: 0.619528, acc.: 65.62%] [G loss: 1.089390]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1651 [D loss: 0.696675, acc.: 60.94%] [G loss: 0.925609]\n",
      "1652 [D loss: 0.748191, acc.: 45.31%] [G loss: 1.030367]\n",
      "1653 [D loss: 0.682052, acc.: 64.06%] [G loss: 0.909196]\n",
      "1654 [D loss: 0.761701, acc.: 53.12%] [G loss: 0.956232]\n",
      "1655 [D loss: 0.652634, acc.: 60.94%] [G loss: 0.951356]\n",
      "1656 [D loss: 0.724687, acc.: 48.44%] [G loss: 0.892789]\n",
      "1657 [D loss: 0.651055, acc.: 64.06%] [G loss: 1.058661]\n",
      "1658 [D loss: 0.677000, acc.: 59.38%] [G loss: 1.040874]\n",
      "1659 [D loss: 0.609024, acc.: 68.75%] [G loss: 0.993211]\n",
      "1660 [D loss: 0.671951, acc.: 60.94%] [G loss: 1.008013]\n",
      "1661 [D loss: 0.702641, acc.: 50.00%] [G loss: 0.978130]\n",
      "1662 [D loss: 0.708052, acc.: 62.50%] [G loss: 0.969149]\n",
      "1663 [D loss: 0.674989, acc.: 59.38%] [G loss: 0.983652]\n",
      "1664 [D loss: 0.702541, acc.: 56.25%] [G loss: 0.893348]\n",
      "1665 [D loss: 0.725730, acc.: 50.00%] [G loss: 0.885240]\n",
      "1666 [D loss: 0.742385, acc.: 48.44%] [G loss: 0.921237]\n",
      "1667 [D loss: 0.683883, acc.: 53.12%] [G loss: 0.843813]\n",
      "1668 [D loss: 0.601206, acc.: 71.88%] [G loss: 1.125497]\n",
      "1669 [D loss: 0.636579, acc.: 64.06%] [G loss: 0.873011]\n",
      "1670 [D loss: 0.707990, acc.: 50.00%] [G loss: 1.026403]\n",
      "1671 [D loss: 0.691705, acc.: 59.38%] [G loss: 0.893271]\n",
      "1672 [D loss: 0.619202, acc.: 65.62%] [G loss: 1.110559]\n",
      "1673 [D loss: 0.680682, acc.: 59.38%] [G loss: 1.011682]\n",
      "1674 [D loss: 0.701805, acc.: 56.25%] [G loss: 0.993883]\n",
      "1675 [D loss: 0.710707, acc.: 54.69%] [G loss: 0.997519]\n",
      "1676 [D loss: 0.585375, acc.: 68.75%] [G loss: 0.860611]\n",
      "1677 [D loss: 0.689295, acc.: 56.25%] [G loss: 1.014234]\n",
      "1678 [D loss: 0.656251, acc.: 56.25%] [G loss: 0.942474]\n",
      "1679 [D loss: 0.710314, acc.: 60.94%] [G loss: 0.943779]\n",
      "1680 [D loss: 0.692346, acc.: 56.25%] [G loss: 0.934375]\n",
      "1681 [D loss: 0.772495, acc.: 57.81%] [G loss: 0.972522]\n",
      "1682 [D loss: 0.663271, acc.: 57.81%] [G loss: 0.986779]\n",
      "1683 [D loss: 0.697808, acc.: 53.12%] [G loss: 1.121255]\n",
      "1684 [D loss: 0.652771, acc.: 60.94%] [G loss: 0.871537]\n",
      "1685 [D loss: 0.652458, acc.: 60.94%] [G loss: 1.124093]\n",
      "1686 [D loss: 0.608650, acc.: 67.19%] [G loss: 1.043373]\n",
      "1687 [D loss: 0.634497, acc.: 65.62%] [G loss: 0.960630]\n",
      "1688 [D loss: 0.695230, acc.: 62.50%] [G loss: 0.990806]\n",
      "1689 [D loss: 0.748483, acc.: 51.56%] [G loss: 1.141878]\n",
      "1690 [D loss: 0.571160, acc.: 78.12%] [G loss: 0.923909]\n",
      "1691 [D loss: 0.725308, acc.: 54.69%] [G loss: 0.916976]\n",
      "1692 [D loss: 0.671082, acc.: 59.38%] [G loss: 1.089350]\n",
      "1693 [D loss: 0.598858, acc.: 76.56%] [G loss: 0.974638]\n",
      "1694 [D loss: 0.686252, acc.: 59.38%] [G loss: 0.950725]\n",
      "1695 [D loss: 0.649940, acc.: 60.94%] [G loss: 0.944431]\n",
      "1696 [D loss: 0.701381, acc.: 60.94%] [G loss: 0.960359]\n",
      "1697 [D loss: 0.656090, acc.: 57.81%] [G loss: 1.118456]\n",
      "1698 [D loss: 0.706275, acc.: 45.31%] [G loss: 0.901353]\n",
      "1699 [D loss: 0.674207, acc.: 53.12%] [G loss: 1.083906]\n",
      "1700 [D loss: 0.633646, acc.: 65.62%] [G loss: 1.090214]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1701 [D loss: 0.684583, acc.: 57.81%] [G loss: 0.859016]\n",
      "1702 [D loss: 0.758376, acc.: 53.12%] [G loss: 0.898616]\n",
      "1703 [D loss: 0.644397, acc.: 59.38%] [G loss: 0.867258]\n",
      "1704 [D loss: 0.671323, acc.: 59.38%] [G loss: 1.206620]\n",
      "1705 [D loss: 0.737684, acc.: 57.81%] [G loss: 0.857570]\n",
      "1706 [D loss: 0.778170, acc.: 48.44%] [G loss: 0.888540]\n",
      "1707 [D loss: 0.613716, acc.: 68.75%] [G loss: 1.012002]\n",
      "1708 [D loss: 0.636892, acc.: 62.50%] [G loss: 1.112073]\n",
      "1709 [D loss: 0.694765, acc.: 56.25%] [G loss: 0.910770]\n",
      "1710 [D loss: 0.774279, acc.: 45.31%] [G loss: 0.759165]\n",
      "1711 [D loss: 0.652918, acc.: 62.50%] [G loss: 0.910700]\n",
      "1712 [D loss: 0.732571, acc.: 59.38%] [G loss: 0.857570]\n",
      "1713 [D loss: 0.669811, acc.: 59.38%] [G loss: 0.996281]\n",
      "1714 [D loss: 0.714317, acc.: 50.00%] [G loss: 0.889300]\n",
      "1715 [D loss: 0.621277, acc.: 59.38%] [G loss: 0.967677]\n",
      "1716 [D loss: 0.690831, acc.: 60.94%] [G loss: 0.955276]\n",
      "1717 [D loss: 0.693081, acc.: 56.25%] [G loss: 1.019141]\n",
      "1718 [D loss: 0.726499, acc.: 51.56%] [G loss: 0.960483]\n",
      "1719 [D loss: 0.623468, acc.: 67.19%] [G loss: 1.012841]\n",
      "1720 [D loss: 0.736725, acc.: 51.56%] [G loss: 0.943212]\n",
      "1721 [D loss: 0.749563, acc.: 48.44%] [G loss: 0.878398]\n",
      "1722 [D loss: 0.645949, acc.: 59.38%] [G loss: 1.171494]\n",
      "1723 [D loss: 0.706753, acc.: 57.81%] [G loss: 0.957786]\n",
      "1724 [D loss: 0.723247, acc.: 51.56%] [G loss: 0.946199]\n",
      "1725 [D loss: 0.549972, acc.: 75.00%] [G loss: 1.164922]\n",
      "1726 [D loss: 0.687487, acc.: 57.81%] [G loss: 1.000471]\n",
      "1727 [D loss: 0.731832, acc.: 50.00%] [G loss: 1.154601]\n",
      "1728 [D loss: 0.740129, acc.: 54.69%] [G loss: 1.030033]\n",
      "1729 [D loss: 0.612104, acc.: 64.06%] [G loss: 0.973467]\n",
      "1730 [D loss: 0.731895, acc.: 54.69%] [G loss: 0.848354]\n",
      "1731 [D loss: 0.644529, acc.: 60.94%] [G loss: 0.984173]\n",
      "1732 [D loss: 0.626808, acc.: 67.19%] [G loss: 1.079031]\n",
      "1733 [D loss: 0.696512, acc.: 56.25%] [G loss: 0.969597]\n",
      "1734 [D loss: 0.739534, acc.: 50.00%] [G loss: 1.008076]\n",
      "1735 [D loss: 0.702884, acc.: 57.81%] [G loss: 1.029295]\n",
      "1736 [D loss: 0.714697, acc.: 57.81%] [G loss: 1.025881]\n",
      "1737 [D loss: 0.647648, acc.: 64.06%] [G loss: 0.987101]\n",
      "1738 [D loss: 0.583813, acc.: 62.50%] [G loss: 0.965837]\n",
      "1739 [D loss: 0.772529, acc.: 48.44%] [G loss: 0.929994]\n",
      "1740 [D loss: 0.666198, acc.: 62.50%] [G loss: 0.917600]\n",
      "1741 [D loss: 0.708387, acc.: 59.38%] [G loss: 0.902274]\n",
      "1742 [D loss: 0.699532, acc.: 56.25%] [G loss: 0.935469]\n",
      "1743 [D loss: 0.673521, acc.: 62.50%] [G loss: 1.000722]\n",
      "1744 [D loss: 0.677780, acc.: 59.38%] [G loss: 0.933818]\n",
      "1745 [D loss: 0.729727, acc.: 51.56%] [G loss: 1.015760]\n",
      "1746 [D loss: 0.741966, acc.: 62.50%] [G loss: 0.842001]\n",
      "1747 [D loss: 0.823658, acc.: 40.62%] [G loss: 0.941804]\n",
      "1748 [D loss: 0.673525, acc.: 60.94%] [G loss: 1.008771]\n",
      "1749 [D loss: 0.656614, acc.: 65.62%] [G loss: 1.085899]\n",
      "1750 [D loss: 0.742337, acc.: 46.88%] [G loss: 0.978419]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1751 [D loss: 0.635158, acc.: 65.62%] [G loss: 1.035021]\n",
      "1752 [D loss: 0.673728, acc.: 51.56%] [G loss: 0.940108]\n",
      "1753 [D loss: 0.653054, acc.: 59.38%] [G loss: 0.856610]\n",
      "1754 [D loss: 0.627014, acc.: 67.19%] [G loss: 1.065875]\n",
      "1755 [D loss: 0.660952, acc.: 59.38%] [G loss: 0.937133]\n",
      "1756 [D loss: 0.618980, acc.: 65.62%] [G loss: 0.974844]\n",
      "1757 [D loss: 0.762659, acc.: 46.88%] [G loss: 1.149231]\n",
      "1758 [D loss: 0.707032, acc.: 57.81%] [G loss: 0.960778]\n",
      "1759 [D loss: 0.661995, acc.: 60.94%] [G loss: 0.986861]\n",
      "1760 [D loss: 0.623754, acc.: 64.06%] [G loss: 1.067635]\n",
      "1761 [D loss: 0.603088, acc.: 68.75%] [G loss: 1.058796]\n",
      "1762 [D loss: 0.612210, acc.: 67.19%] [G loss: 1.075552]\n",
      "1763 [D loss: 0.662106, acc.: 57.81%] [G loss: 1.090976]\n",
      "1764 [D loss: 0.613911, acc.: 59.38%] [G loss: 0.936675]\n",
      "1765 [D loss: 0.728801, acc.: 46.88%] [G loss: 1.001761]\n",
      "1766 [D loss: 0.617773, acc.: 68.75%] [G loss: 0.933573]\n",
      "1767 [D loss: 0.665391, acc.: 60.94%] [G loss: 0.960284]\n",
      "1768 [D loss: 0.683465, acc.: 57.81%] [G loss: 0.944049]\n",
      "1769 [D loss: 0.650226, acc.: 62.50%] [G loss: 1.121119]\n",
      "1770 [D loss: 0.762512, acc.: 50.00%] [G loss: 1.009309]\n",
      "1771 [D loss: 0.729692, acc.: 56.25%] [G loss: 0.970507]\n",
      "1772 [D loss: 0.652777, acc.: 62.50%] [G loss: 0.985768]\n",
      "1773 [D loss: 0.739866, acc.: 46.88%] [G loss: 0.920385]\n",
      "1774 [D loss: 0.695957, acc.: 56.25%] [G loss: 1.023034]\n",
      "1775 [D loss: 0.693501, acc.: 62.50%] [G loss: 1.012740]\n",
      "1776 [D loss: 0.705829, acc.: 54.69%] [G loss: 0.886689]\n",
      "1777 [D loss: 0.728480, acc.: 62.50%] [G loss: 1.134501]\n",
      "1778 [D loss: 0.677352, acc.: 51.56%] [G loss: 0.864470]\n",
      "1779 [D loss: 0.660805, acc.: 59.38%] [G loss: 1.014547]\n",
      "1780 [D loss: 0.670312, acc.: 54.69%] [G loss: 0.997542]\n",
      "1781 [D loss: 0.679545, acc.: 65.62%] [G loss: 1.171277]\n",
      "1782 [D loss: 0.623798, acc.: 59.38%] [G loss: 1.071947]\n",
      "1783 [D loss: 0.698484, acc.: 51.56%] [G loss: 1.017144]\n",
      "1784 [D loss: 0.688143, acc.: 62.50%] [G loss: 1.018423]\n",
      "1785 [D loss: 0.646830, acc.: 62.50%] [G loss: 1.062342]\n",
      "1786 [D loss: 0.704683, acc.: 50.00%] [G loss: 0.879610]\n",
      "1787 [D loss: 0.685964, acc.: 59.38%] [G loss: 0.909297]\n",
      "1788 [D loss: 0.676470, acc.: 65.62%] [G loss: 0.863176]\n",
      "1789 [D loss: 0.682139, acc.: 57.81%] [G loss: 0.950323]\n",
      "1790 [D loss: 0.596181, acc.: 68.75%] [G loss: 1.020073]\n",
      "1791 [D loss: 0.679722, acc.: 64.06%] [G loss: 1.058554]\n",
      "1792 [D loss: 0.669276, acc.: 54.69%] [G loss: 0.971635]\n",
      "1793 [D loss: 0.649989, acc.: 64.06%] [G loss: 0.914548]\n",
      "1794 [D loss: 0.684929, acc.: 53.12%] [G loss: 0.911681]\n",
      "1795 [D loss: 0.675430, acc.: 64.06%] [G loss: 0.828757]\n",
      "1796 [D loss: 0.647698, acc.: 60.94%] [G loss: 0.962541]\n",
      "1797 [D loss: 0.705329, acc.: 64.06%] [G loss: 0.924313]\n",
      "1798 [D loss: 0.673306, acc.: 56.25%] [G loss: 0.915671]\n",
      "1799 [D loss: 0.755386, acc.: 51.56%] [G loss: 0.813693]\n",
      "1800 [D loss: 0.650166, acc.: 56.25%] [G loss: 1.154787]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1801 [D loss: 0.627339, acc.: 67.19%] [G loss: 1.133523]\n",
      "1802 [D loss: 0.675889, acc.: 53.12%] [G loss: 0.923159]\n",
      "1803 [D loss: 0.675879, acc.: 59.38%] [G loss: 0.907696]\n",
      "1804 [D loss: 0.657934, acc.: 59.38%] [G loss: 1.074225]\n",
      "1805 [D loss: 0.660033, acc.: 59.38%] [G loss: 1.074692]\n",
      "1806 [D loss: 0.684298, acc.: 60.94%] [G loss: 1.110092]\n",
      "1807 [D loss: 0.714528, acc.: 54.69%] [G loss: 0.864632]\n",
      "1808 [D loss: 0.727466, acc.: 50.00%] [G loss: 0.944820]\n",
      "1809 [D loss: 0.764015, acc.: 53.12%] [G loss: 0.871543]\n",
      "1810 [D loss: 0.660331, acc.: 62.50%] [G loss: 0.905629]\n",
      "1811 [D loss: 0.736141, acc.: 50.00%] [G loss: 1.076741]\n",
      "1812 [D loss: 0.643533, acc.: 67.19%] [G loss: 0.932924]\n",
      "1813 [D loss: 0.687155, acc.: 53.12%] [G loss: 1.044590]\n",
      "1814 [D loss: 0.714346, acc.: 53.12%] [G loss: 0.989433]\n",
      "1815 [D loss: 0.700944, acc.: 51.56%] [G loss: 1.076633]\n",
      "1816 [D loss: 0.697978, acc.: 53.12%] [G loss: 0.851932]\n",
      "1817 [D loss: 0.768076, acc.: 51.56%] [G loss: 0.969020]\n",
      "1818 [D loss: 0.699529, acc.: 54.69%] [G loss: 0.989160]\n",
      "1819 [D loss: 0.678523, acc.: 59.38%] [G loss: 1.135070]\n",
      "1820 [D loss: 0.588189, acc.: 68.75%] [G loss: 0.982377]\n",
      "1821 [D loss: 0.728383, acc.: 51.56%] [G loss: 0.990847]\n",
      "1822 [D loss: 0.711045, acc.: 57.81%] [G loss: 1.062938]\n",
      "1823 [D loss: 0.745130, acc.: 57.81%] [G loss: 1.038266]\n",
      "1824 [D loss: 0.676768, acc.: 53.12%] [G loss: 0.852689]\n",
      "1825 [D loss: 0.671641, acc.: 59.38%] [G loss: 0.868714]\n",
      "1826 [D loss: 0.680497, acc.: 64.06%] [G loss: 1.055052]\n",
      "1827 [D loss: 0.677178, acc.: 57.81%] [G loss: 0.912728]\n",
      "1828 [D loss: 0.585537, acc.: 64.06%] [G loss: 1.092627]\n",
      "1829 [D loss: 0.714114, acc.: 56.25%] [G loss: 0.983531]\n",
      "1830 [D loss: 0.794271, acc.: 56.25%] [G loss: 0.831657]\n",
      "1831 [D loss: 0.674610, acc.: 60.94%] [G loss: 1.052742]\n",
      "1832 [D loss: 0.649528, acc.: 62.50%] [G loss: 0.938611]\n",
      "1833 [D loss: 0.709542, acc.: 51.56%] [G loss: 1.030539]\n",
      "1834 [D loss: 0.705679, acc.: 54.69%] [G loss: 0.917656]\n",
      "1835 [D loss: 0.734042, acc.: 50.00%] [G loss: 1.116969]\n",
      "1836 [D loss: 0.702219, acc.: 50.00%] [G loss: 1.029731]\n",
      "1837 [D loss: 0.687557, acc.: 60.94%] [G loss: 0.935583]\n",
      "1838 [D loss: 0.746367, acc.: 53.12%] [G loss: 0.896822]\n",
      "1839 [D loss: 0.748745, acc.: 53.12%] [G loss: 1.149155]\n",
      "1840 [D loss: 0.659013, acc.: 60.94%] [G loss: 0.968265]\n",
      "1841 [D loss: 0.648540, acc.: 64.06%] [G loss: 0.946402]\n",
      "1842 [D loss: 0.640321, acc.: 62.50%] [G loss: 1.087343]\n",
      "1843 [D loss: 0.637338, acc.: 64.06%] [G loss: 1.039553]\n",
      "1844 [D loss: 0.667328, acc.: 51.56%] [G loss: 0.946566]\n",
      "1845 [D loss: 0.641476, acc.: 67.19%] [G loss: 0.921407]\n",
      "1846 [D loss: 0.590086, acc.: 68.75%] [G loss: 1.018352]\n",
      "1847 [D loss: 0.694045, acc.: 51.56%] [G loss: 0.928589]\n",
      "1848 [D loss: 0.685971, acc.: 54.69%] [G loss: 1.010124]\n",
      "1849 [D loss: 0.580408, acc.: 73.44%] [G loss: 1.014041]\n",
      "1850 [D loss: 0.588691, acc.: 62.50%] [G loss: 1.163090]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1851 [D loss: 0.709671, acc.: 54.69%] [G loss: 0.935615]\n",
      "1852 [D loss: 0.617289, acc.: 59.38%] [G loss: 1.012050]\n",
      "1853 [D loss: 0.597266, acc.: 70.31%] [G loss: 1.111545]\n",
      "1854 [D loss: 0.747754, acc.: 45.31%] [G loss: 0.902297]\n",
      "1855 [D loss: 0.645683, acc.: 60.94%] [G loss: 1.060276]\n",
      "1856 [D loss: 0.619812, acc.: 59.38%] [G loss: 0.993802]\n",
      "1857 [D loss: 0.663698, acc.: 56.25%] [G loss: 1.086792]\n",
      "1858 [D loss: 0.656301, acc.: 64.06%] [G loss: 1.087318]\n",
      "1859 [D loss: 0.664973, acc.: 51.56%] [G loss: 0.943873]\n",
      "1860 [D loss: 0.610002, acc.: 65.62%] [G loss: 1.017829]\n",
      "1861 [D loss: 0.711300, acc.: 51.56%] [G loss: 1.029465]\n",
      "1862 [D loss: 0.702833, acc.: 56.25%] [G loss: 1.067782]\n",
      "1863 [D loss: 0.783688, acc.: 45.31%] [G loss: 0.980611]\n",
      "1864 [D loss: 0.742111, acc.: 46.88%] [G loss: 0.892974]\n",
      "1865 [D loss: 0.663513, acc.: 64.06%] [G loss: 0.913874]\n",
      "1866 [D loss: 0.694301, acc.: 54.69%] [G loss: 0.872773]\n",
      "1867 [D loss: 0.653556, acc.: 62.50%] [G loss: 0.948167]\n",
      "1868 [D loss: 0.654988, acc.: 62.50%] [G loss: 1.101202]\n",
      "1869 [D loss: 0.780048, acc.: 48.44%] [G loss: 1.083900]\n",
      "1870 [D loss: 0.645667, acc.: 64.06%] [G loss: 0.837601]\n",
      "1871 [D loss: 0.775765, acc.: 45.31%] [G loss: 0.994169]\n",
      "1872 [D loss: 0.674884, acc.: 60.94%] [G loss: 1.005703]\n",
      "1873 [D loss: 0.743858, acc.: 54.69%] [G loss: 0.978452]\n",
      "1874 [D loss: 0.700454, acc.: 56.25%] [G loss: 1.012852]\n",
      "1875 [D loss: 0.780939, acc.: 46.88%] [G loss: 0.845453]\n",
      "1876 [D loss: 0.695972, acc.: 57.81%] [G loss: 0.908500]\n",
      "1877 [D loss: 0.803080, acc.: 48.44%] [G loss: 0.966207]\n",
      "1878 [D loss: 0.775503, acc.: 46.88%] [G loss: 0.968290]\n",
      "1879 [D loss: 0.654667, acc.: 60.94%] [G loss: 0.901085]\n",
      "1880 [D loss: 0.669315, acc.: 59.38%] [G loss: 1.084251]\n",
      "1881 [D loss: 0.767661, acc.: 45.31%] [G loss: 1.046308]\n",
      "1882 [D loss: 0.772544, acc.: 48.44%] [G loss: 0.995881]\n",
      "1883 [D loss: 0.677803, acc.: 57.81%] [G loss: 0.803864]\n",
      "1884 [D loss: 0.707960, acc.: 54.69%] [G loss: 0.904318]\n",
      "1885 [D loss: 0.575155, acc.: 71.88%] [G loss: 0.986357]\n",
      "1886 [D loss: 0.576993, acc.: 68.75%] [G loss: 0.810274]\n",
      "1887 [D loss: 0.604571, acc.: 68.75%] [G loss: 0.917198]\n",
      "1888 [D loss: 0.652743, acc.: 64.06%] [G loss: 1.048427]\n",
      "1889 [D loss: 0.666932, acc.: 56.25%] [G loss: 0.928924]\n",
      "1890 [D loss: 0.677854, acc.: 54.69%] [G loss: 1.135194]\n",
      "1891 [D loss: 0.668978, acc.: 60.94%] [G loss: 0.948048]\n",
      "1892 [D loss: 0.604580, acc.: 62.50%] [G loss: 0.940139]\n",
      "1893 [D loss: 0.669206, acc.: 54.69%] [G loss: 0.952962]\n",
      "1894 [D loss: 0.650622, acc.: 60.94%] [G loss: 1.070237]\n",
      "1895 [D loss: 0.684936, acc.: 62.50%] [G loss: 0.930119]\n",
      "1896 [D loss: 0.706934, acc.: 60.94%] [G loss: 0.998641]\n",
      "1897 [D loss: 0.695358, acc.: 59.38%] [G loss: 0.935372]\n",
      "1898 [D loss: 0.635573, acc.: 64.06%] [G loss: 1.044624]\n",
      "1899 [D loss: 0.696993, acc.: 65.62%] [G loss: 0.954680]\n",
      "1900 [D loss: 0.618384, acc.: 62.50%] [G loss: 1.170644]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1901 [D loss: 0.705431, acc.: 51.56%] [G loss: 1.062059]\n",
      "1902 [D loss: 0.768840, acc.: 43.75%] [G loss: 1.074214]\n",
      "1903 [D loss: 0.714463, acc.: 54.69%] [G loss: 0.906308]\n",
      "1904 [D loss: 0.696477, acc.: 54.69%] [G loss: 1.056686]\n",
      "1905 [D loss: 0.592170, acc.: 73.44%] [G loss: 1.021613]\n",
      "1906 [D loss: 0.699761, acc.: 48.44%] [G loss: 1.060598]\n",
      "1907 [D loss: 0.667762, acc.: 60.94%] [G loss: 1.079541]\n",
      "1908 [D loss: 0.659555, acc.: 59.38%] [G loss: 0.990586]\n",
      "1909 [D loss: 0.624104, acc.: 68.75%] [G loss: 1.046328]\n",
      "1910 [D loss: 0.721991, acc.: 51.56%] [G loss: 0.886076]\n",
      "1911 [D loss: 0.587577, acc.: 73.44%] [G loss: 0.911260]\n",
      "1912 [D loss: 0.651291, acc.: 60.94%] [G loss: 0.959291]\n",
      "1913 [D loss: 0.678531, acc.: 56.25%] [G loss: 1.086508]\n",
      "1914 [D loss: 0.680212, acc.: 57.81%] [G loss: 1.224725]\n",
      "1915 [D loss: 0.735635, acc.: 54.69%] [G loss: 1.118740]\n",
      "1916 [D loss: 0.707476, acc.: 60.94%] [G loss: 1.010640]\n",
      "1917 [D loss: 0.679651, acc.: 48.44%] [G loss: 0.980562]\n",
      "1918 [D loss: 0.711395, acc.: 53.12%] [G loss: 0.939610]\n",
      "1919 [D loss: 0.691034, acc.: 59.38%] [G loss: 1.082097]\n",
      "1920 [D loss: 0.556090, acc.: 73.44%] [G loss: 0.908731]\n",
      "1921 [D loss: 0.765961, acc.: 48.44%] [G loss: 0.943782]\n",
      "1922 [D loss: 0.726704, acc.: 54.69%] [G loss: 0.964050]\n",
      "1923 [D loss: 0.734015, acc.: 53.12%] [G loss: 1.018998]\n",
      "1924 [D loss: 0.700402, acc.: 56.25%] [G loss: 0.977887]\n",
      "1925 [D loss: 0.674260, acc.: 56.25%] [G loss: 1.103085]\n",
      "1926 [D loss: 0.740215, acc.: 59.38%] [G loss: 0.859557]\n",
      "1927 [D loss: 0.689149, acc.: 59.38%] [G loss: 1.108128]\n",
      "1928 [D loss: 0.754986, acc.: 46.88%] [G loss: 0.842061]\n",
      "1929 [D loss: 0.644370, acc.: 67.19%] [G loss: 1.103683]\n",
      "1930 [D loss: 0.754193, acc.: 45.31%] [G loss: 0.928440]\n",
      "1931 [D loss: 0.691013, acc.: 56.25%] [G loss: 0.903767]\n",
      "1932 [D loss: 0.718712, acc.: 53.12%] [G loss: 0.987643]\n",
      "1933 [D loss: 0.649657, acc.: 60.94%] [G loss: 0.864103]\n",
      "1934 [D loss: 0.578836, acc.: 73.44%] [G loss: 1.049725]\n",
      "1935 [D loss: 0.705404, acc.: 53.12%] [G loss: 1.032291]\n",
      "1936 [D loss: 0.727644, acc.: 45.31%] [G loss: 1.050845]\n",
      "1937 [D loss: 0.657274, acc.: 65.62%] [G loss: 0.997038]\n",
      "1938 [D loss: 0.658493, acc.: 60.94%] [G loss: 0.957933]\n",
      "1939 [D loss: 0.668208, acc.: 59.38%] [G loss: 1.056939]\n",
      "1940 [D loss: 0.661819, acc.: 62.50%] [G loss: 1.266982]\n",
      "1941 [D loss: 0.729653, acc.: 51.56%] [G loss: 1.003759]\n",
      "1942 [D loss: 0.608828, acc.: 64.06%] [G loss: 0.894099]\n",
      "1943 [D loss: 0.670789, acc.: 62.50%] [G loss: 0.927140]\n",
      "1944 [D loss: 0.764447, acc.: 51.56%] [G loss: 0.933176]\n",
      "1945 [D loss: 0.702278, acc.: 57.81%] [G loss: 1.020752]\n",
      "1946 [D loss: 0.692468, acc.: 56.25%] [G loss: 1.027855]\n",
      "1947 [D loss: 0.624778, acc.: 57.81%] [G loss: 1.085077]\n",
      "1948 [D loss: 0.647489, acc.: 60.94%] [G loss: 1.057295]\n",
      "1949 [D loss: 0.762990, acc.: 51.56%] [G loss: 1.056667]\n",
      "1950 [D loss: 0.608574, acc.: 62.50%] [G loss: 1.104751]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "1951 [D loss: 0.695001, acc.: 59.38%] [G loss: 1.029511]\n",
      "1952 [D loss: 0.672021, acc.: 65.62%] [G loss: 1.093256]\n",
      "1953 [D loss: 0.698743, acc.: 56.25%] [G loss: 0.948944]\n",
      "1954 [D loss: 0.605432, acc.: 70.31%] [G loss: 0.940995]\n",
      "1955 [D loss: 0.648208, acc.: 62.50%] [G loss: 1.050498]\n",
      "1956 [D loss: 0.698896, acc.: 64.06%] [G loss: 1.032900]\n",
      "1957 [D loss: 0.615632, acc.: 65.62%] [G loss: 1.086255]\n",
      "1958 [D loss: 0.672624, acc.: 67.19%] [G loss: 0.968816]\n",
      "1959 [D loss: 0.711585, acc.: 60.94%] [G loss: 0.987305]\n",
      "1960 [D loss: 0.723897, acc.: 45.31%] [G loss: 0.871499]\n",
      "1961 [D loss: 0.681526, acc.: 56.25%] [G loss: 0.796945]\n",
      "1962 [D loss: 0.644581, acc.: 64.06%] [G loss: 1.024251]\n",
      "1963 [D loss: 0.636172, acc.: 64.06%] [G loss: 1.016122]\n",
      "1964 [D loss: 0.681363, acc.: 54.69%] [G loss: 0.892606]\n",
      "1965 [D loss: 0.655597, acc.: 56.25%] [G loss: 1.079023]\n",
      "1966 [D loss: 0.661621, acc.: 56.25%] [G loss: 0.974545]\n",
      "1967 [D loss: 0.670102, acc.: 56.25%] [G loss: 1.082037]\n",
      "1968 [D loss: 0.710970, acc.: 64.06%] [G loss: 1.038320]\n",
      "1969 [D loss: 0.670515, acc.: 62.50%] [G loss: 1.001340]\n",
      "1970 [D loss: 0.740471, acc.: 46.88%] [G loss: 1.056118]\n",
      "1971 [D loss: 0.630743, acc.: 56.25%] [G loss: 1.028682]\n",
      "1972 [D loss: 0.651356, acc.: 62.50%] [G loss: 0.969035]\n",
      "1973 [D loss: 0.652783, acc.: 67.19%] [G loss: 1.101432]\n",
      "1974 [D loss: 0.724666, acc.: 50.00%] [G loss: 0.929729]\n",
      "1975 [D loss: 0.639506, acc.: 65.62%] [G loss: 1.066625]\n",
      "1976 [D loss: 0.623343, acc.: 67.19%] [G loss: 1.047363]\n",
      "1977 [D loss: 0.621296, acc.: 60.94%] [G loss: 0.972545]\n",
      "1978 [D loss: 0.641360, acc.: 64.06%] [G loss: 1.051166]\n",
      "1979 [D loss: 0.702756, acc.: 62.50%] [G loss: 1.071949]\n",
      "1980 [D loss: 0.733305, acc.: 62.50%] [G loss: 1.203970]\n",
      "1981 [D loss: 0.663096, acc.: 54.69%] [G loss: 1.027701]\n",
      "1982 [D loss: 0.757704, acc.: 54.69%] [G loss: 1.079508]\n",
      "1983 [D loss: 0.701077, acc.: 54.69%] [G loss: 1.099177]\n",
      "1984 [D loss: 0.674409, acc.: 59.38%] [G loss: 1.021482]\n",
      "1985 [D loss: 0.707889, acc.: 56.25%] [G loss: 0.952236]\n",
      "1986 [D loss: 0.653753, acc.: 60.94%] [G loss: 1.002626]\n",
      "1987 [D loss: 0.561733, acc.: 71.88%] [G loss: 1.039224]\n",
      "1988 [D loss: 0.706397, acc.: 54.69%] [G loss: 1.002468]\n",
      "1989 [D loss: 0.645453, acc.: 57.81%] [G loss: 1.126886]\n",
      "1990 [D loss: 0.620675, acc.: 64.06%] [G loss: 0.973130]\n",
      "1991 [D loss: 0.637351, acc.: 67.19%] [G loss: 1.031162]\n",
      "1992 [D loss: 0.741968, acc.: 51.56%] [G loss: 1.006885]\n",
      "1993 [D loss: 0.661651, acc.: 56.25%] [G loss: 1.052383]\n",
      "1994 [D loss: 0.647985, acc.: 68.75%] [G loss: 1.001332]\n",
      "1995 [D loss: 0.767977, acc.: 50.00%] [G loss: 0.930264]\n",
      "1996 [D loss: 0.626645, acc.: 62.50%] [G loss: 1.162626]\n",
      "1997 [D loss: 0.670649, acc.: 59.38%] [G loss: 0.996410]\n",
      "1998 [D loss: 0.677391, acc.: 53.12%] [G loss: 0.915519]\n",
      "1999 [D loss: 0.690190, acc.: 56.25%] [G loss: 0.952047]\n",
      "2000 [D loss: 0.733868, acc.: 53.12%] [G loss: 0.943808]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2001 [D loss: 0.646499, acc.: 62.50%] [G loss: 0.971979]\n",
      "2002 [D loss: 0.628035, acc.: 65.62%] [G loss: 0.994314]\n",
      "2003 [D loss: 0.629888, acc.: 59.38%] [G loss: 1.064216]\n",
      "2004 [D loss: 0.665261, acc.: 56.25%] [G loss: 0.965131]\n",
      "2005 [D loss: 0.677885, acc.: 59.38%] [G loss: 0.981314]\n",
      "2006 [D loss: 0.637956, acc.: 62.50%] [G loss: 0.989246]\n",
      "2007 [D loss: 0.698309, acc.: 54.69%] [G loss: 0.941776]\n",
      "2008 [D loss: 0.702294, acc.: 57.81%] [G loss: 0.782695]\n",
      "2009 [D loss: 0.696041, acc.: 57.81%] [G loss: 1.088241]\n",
      "2010 [D loss: 0.745127, acc.: 51.56%] [G loss: 0.925000]\n",
      "2011 [D loss: 0.599218, acc.: 64.06%] [G loss: 0.897716]\n",
      "2012 [D loss: 0.601662, acc.: 64.06%] [G loss: 1.009990]\n",
      "2013 [D loss: 0.711528, acc.: 51.56%] [G loss: 0.828282]\n",
      "2014 [D loss: 0.559191, acc.: 70.31%] [G loss: 1.038757]\n",
      "2015 [D loss: 0.689299, acc.: 50.00%] [G loss: 0.991251]\n",
      "2016 [D loss: 0.744935, acc.: 48.44%] [G loss: 0.908719]\n",
      "2017 [D loss: 0.708905, acc.: 65.62%] [G loss: 0.925415]\n",
      "2018 [D loss: 0.614927, acc.: 70.31%] [G loss: 1.040318]\n",
      "2019 [D loss: 0.667846, acc.: 62.50%] [G loss: 0.917054]\n",
      "2020 [D loss: 0.655272, acc.: 62.50%] [G loss: 1.024179]\n",
      "2021 [D loss: 0.697952, acc.: 50.00%] [G loss: 0.959408]\n",
      "2022 [D loss: 0.723709, acc.: 51.56%] [G loss: 0.942197]\n",
      "2023 [D loss: 0.689334, acc.: 62.50%] [G loss: 1.001459]\n",
      "2024 [D loss: 0.677060, acc.: 59.38%] [G loss: 1.009974]\n",
      "2025 [D loss: 0.573730, acc.: 67.19%] [G loss: 1.129792]\n",
      "2026 [D loss: 0.633330, acc.: 62.50%] [G loss: 0.959703]\n",
      "2027 [D loss: 0.622765, acc.: 62.50%] [G loss: 0.987405]\n",
      "2028 [D loss: 0.724136, acc.: 50.00%] [G loss: 0.977848]\n",
      "2029 [D loss: 0.627465, acc.: 67.19%] [G loss: 0.956272]\n",
      "2030 [D loss: 0.632189, acc.: 60.94%] [G loss: 1.000613]\n",
      "2031 [D loss: 0.677569, acc.: 57.81%] [G loss: 1.019061]\n",
      "2032 [D loss: 0.743017, acc.: 46.88%] [G loss: 0.922809]\n",
      "2033 [D loss: 0.644641, acc.: 48.44%] [G loss: 1.219370]\n",
      "2034 [D loss: 0.715464, acc.: 59.38%] [G loss: 0.906475]\n",
      "2035 [D loss: 0.724963, acc.: 56.25%] [G loss: 0.961944]\n",
      "2036 [D loss: 0.665778, acc.: 59.38%] [G loss: 0.990848]\n",
      "2037 [D loss: 0.666127, acc.: 56.25%] [G loss: 1.040959]\n",
      "2038 [D loss: 0.710984, acc.: 50.00%] [G loss: 1.008181]\n",
      "2039 [D loss: 0.636574, acc.: 62.50%] [G loss: 1.029570]\n",
      "2040 [D loss: 0.777116, acc.: 51.56%] [G loss: 0.966719]\n",
      "2041 [D loss: 0.674745, acc.: 53.12%] [G loss: 1.164404]\n",
      "2042 [D loss: 0.645902, acc.: 60.94%] [G loss: 1.090665]\n",
      "2043 [D loss: 0.695408, acc.: 54.69%] [G loss: 0.974182]\n",
      "2044 [D loss: 0.726765, acc.: 53.12%] [G loss: 0.932215]\n",
      "2045 [D loss: 0.613175, acc.: 60.94%] [G loss: 1.085795]\n",
      "2046 [D loss: 0.582337, acc.: 67.19%] [G loss: 1.170796]\n",
      "2047 [D loss: 0.684390, acc.: 60.94%] [G loss: 0.900705]\n",
      "2048 [D loss: 0.690061, acc.: 53.12%] [G loss: 0.856946]\n",
      "2049 [D loss: 0.630289, acc.: 60.94%] [G loss: 0.923392]\n",
      "2050 [D loss: 0.666320, acc.: 64.06%] [G loss: 0.866402]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2051 [D loss: 0.660462, acc.: 60.94%] [G loss: 1.208169]\n",
      "2052 [D loss: 0.675026, acc.: 62.50%] [G loss: 1.148824]\n",
      "2053 [D loss: 0.607003, acc.: 70.31%] [G loss: 0.939604]\n",
      "2054 [D loss: 0.628230, acc.: 65.62%] [G loss: 0.999143]\n",
      "2055 [D loss: 0.615638, acc.: 64.06%] [G loss: 1.121677]\n",
      "2056 [D loss: 0.626901, acc.: 65.62%] [G loss: 1.087299]\n",
      "2057 [D loss: 0.705086, acc.: 59.38%] [G loss: 0.973092]\n",
      "2058 [D loss: 0.686543, acc.: 53.12%] [G loss: 1.101555]\n",
      "2059 [D loss: 0.645326, acc.: 62.50%] [G loss: 0.820903]\n",
      "2060 [D loss: 0.687217, acc.: 53.12%] [G loss: 0.966591]\n",
      "2061 [D loss: 0.686359, acc.: 50.00%] [G loss: 0.928302]\n",
      "2062 [D loss: 0.642042, acc.: 60.94%] [G loss: 0.976152]\n",
      "2063 [D loss: 0.763375, acc.: 48.44%] [G loss: 1.015621]\n",
      "2064 [D loss: 0.717566, acc.: 56.25%] [G loss: 1.078099]\n",
      "2065 [D loss: 0.593509, acc.: 75.00%] [G loss: 1.155275]\n",
      "2066 [D loss: 0.793519, acc.: 39.06%] [G loss: 1.012640]\n",
      "2067 [D loss: 0.634962, acc.: 64.06%] [G loss: 1.138079]\n",
      "2068 [D loss: 0.612826, acc.: 68.75%] [G loss: 1.051509]\n",
      "2069 [D loss: 0.681812, acc.: 56.25%] [G loss: 1.015102]\n",
      "2070 [D loss: 0.739503, acc.: 51.56%] [G loss: 0.834248]\n",
      "2071 [D loss: 0.696169, acc.: 54.69%] [G loss: 1.016445]\n",
      "2072 [D loss: 0.695783, acc.: 57.81%] [G loss: 0.912300]\n",
      "2073 [D loss: 0.701380, acc.: 57.81%] [G loss: 1.004175]\n",
      "2074 [D loss: 0.699763, acc.: 56.25%] [G loss: 1.207440]\n",
      "2075 [D loss: 0.759278, acc.: 54.69%] [G loss: 1.171227]\n",
      "2076 [D loss: 0.590792, acc.: 65.62%] [G loss: 0.945426]\n",
      "2077 [D loss: 0.676911, acc.: 64.06%] [G loss: 0.982707]\n",
      "2078 [D loss: 0.587275, acc.: 68.75%] [G loss: 1.126660]\n",
      "2079 [D loss: 0.678908, acc.: 59.38%] [G loss: 1.047738]\n",
      "2080 [D loss: 0.763631, acc.: 50.00%] [G loss: 1.136164]\n",
      "2081 [D loss: 0.632163, acc.: 64.06%] [G loss: 0.856322]\n",
      "2082 [D loss: 0.631283, acc.: 59.38%] [G loss: 1.153568]\n",
      "2083 [D loss: 0.782171, acc.: 43.75%] [G loss: 1.021191]\n",
      "2084 [D loss: 0.682806, acc.: 59.38%] [G loss: 0.800771]\n",
      "2085 [D loss: 0.639668, acc.: 59.38%] [G loss: 0.951419]\n",
      "2086 [D loss: 0.616612, acc.: 68.75%] [G loss: 0.947743]\n",
      "2087 [D loss: 0.704754, acc.: 64.06%] [G loss: 1.118592]\n",
      "2088 [D loss: 0.723351, acc.: 53.12%] [G loss: 0.971286]\n",
      "2089 [D loss: 0.628654, acc.: 64.06%] [G loss: 0.927616]\n",
      "2090 [D loss: 0.774974, acc.: 46.88%] [G loss: 0.834892]\n",
      "2091 [D loss: 0.681838, acc.: 67.19%] [G loss: 1.088470]\n",
      "2092 [D loss: 0.663540, acc.: 60.94%] [G loss: 1.033091]\n",
      "2093 [D loss: 0.781229, acc.: 51.56%] [G loss: 0.899146]\n",
      "2094 [D loss: 0.659960, acc.: 65.62%] [G loss: 0.897591]\n",
      "2095 [D loss: 0.654708, acc.: 57.81%] [G loss: 1.076662]\n",
      "2096 [D loss: 0.620038, acc.: 64.06%] [G loss: 0.959141]\n",
      "2097 [D loss: 0.625680, acc.: 64.06%] [G loss: 0.908072]\n",
      "2098 [D loss: 0.710135, acc.: 54.69%] [G loss: 0.953645]\n",
      "2099 [D loss: 0.595204, acc.: 62.50%] [G loss: 1.002529]\n",
      "2100 [D loss: 0.701277, acc.: 62.50%] [G loss: 0.947077]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2101 [D loss: 0.571064, acc.: 68.75%] [G loss: 1.187766]\n",
      "2102 [D loss: 0.711240, acc.: 54.69%] [G loss: 1.013324]\n",
      "2103 [D loss: 0.721025, acc.: 51.56%] [G loss: 0.959033]\n",
      "2104 [D loss: 0.673743, acc.: 70.31%] [G loss: 0.953653]\n",
      "2105 [D loss: 0.691270, acc.: 60.94%] [G loss: 1.067590]\n",
      "2106 [D loss: 0.792826, acc.: 51.56%] [G loss: 1.002590]\n",
      "2107 [D loss: 0.692390, acc.: 67.19%] [G loss: 1.044244]\n",
      "2108 [D loss: 0.657216, acc.: 59.38%] [G loss: 0.864946]\n",
      "2109 [D loss: 0.607333, acc.: 65.62%] [G loss: 1.064947]\n",
      "2110 [D loss: 0.727111, acc.: 51.56%] [G loss: 0.994306]\n",
      "2111 [D loss: 0.706117, acc.: 59.38%] [G loss: 0.972869]\n",
      "2112 [D loss: 0.570467, acc.: 75.00%] [G loss: 1.145144]\n",
      "2113 [D loss: 0.738427, acc.: 46.88%] [G loss: 0.914389]\n",
      "2114 [D loss: 0.647846, acc.: 62.50%] [G loss: 0.979671]\n",
      "2115 [D loss: 0.699822, acc.: 57.81%] [G loss: 0.885254]\n",
      "2116 [D loss: 0.662782, acc.: 59.38%] [G loss: 0.941669]\n",
      "2117 [D loss: 0.585578, acc.: 68.75%] [G loss: 1.014226]\n",
      "2118 [D loss: 0.630151, acc.: 65.62%] [G loss: 0.951514]\n",
      "2119 [D loss: 0.668892, acc.: 50.00%] [G loss: 1.070535]\n",
      "2120 [D loss: 0.652273, acc.: 62.50%] [G loss: 1.037626]\n",
      "2121 [D loss: 0.716212, acc.: 56.25%] [G loss: 0.917215]\n",
      "2122 [D loss: 0.601346, acc.: 73.44%] [G loss: 1.117565]\n",
      "2123 [D loss: 0.631405, acc.: 64.06%] [G loss: 0.910913]\n",
      "2124 [D loss: 0.689602, acc.: 60.94%] [G loss: 0.988841]\n",
      "2125 [D loss: 0.687703, acc.: 56.25%] [G loss: 0.949571]\n",
      "2126 [D loss: 0.649328, acc.: 67.19%] [G loss: 1.133120]\n",
      "2127 [D loss: 0.642554, acc.: 70.31%] [G loss: 0.943879]\n",
      "2128 [D loss: 0.638406, acc.: 62.50%] [G loss: 1.160653]\n",
      "2129 [D loss: 0.632152, acc.: 59.38%] [G loss: 0.970871]\n",
      "2130 [D loss: 0.701270, acc.: 56.25%] [G loss: 0.916576]\n",
      "2131 [D loss: 0.672441, acc.: 64.06%] [G loss: 0.995416]\n",
      "2132 [D loss: 0.803961, acc.: 54.69%] [G loss: 0.855494]\n",
      "2133 [D loss: 0.602605, acc.: 67.19%] [G loss: 0.985721]\n",
      "2134 [D loss: 0.717658, acc.: 54.69%] [G loss: 1.058500]\n",
      "2135 [D loss: 0.641824, acc.: 70.31%] [G loss: 0.930045]\n",
      "2136 [D loss: 0.647252, acc.: 65.62%] [G loss: 0.969929]\n",
      "2137 [D loss: 0.675723, acc.: 56.25%] [G loss: 1.000812]\n",
      "2138 [D loss: 0.695171, acc.: 60.94%] [G loss: 0.858527]\n",
      "2139 [D loss: 0.669696, acc.: 64.06%] [G loss: 1.023352]\n",
      "2140 [D loss: 0.726715, acc.: 56.25%] [G loss: 1.122841]\n",
      "2141 [D loss: 0.732759, acc.: 53.12%] [G loss: 1.206869]\n",
      "2142 [D loss: 0.674332, acc.: 56.25%] [G loss: 0.943682]\n",
      "2143 [D loss: 0.583918, acc.: 71.88%] [G loss: 1.122728]\n",
      "2144 [D loss: 0.621082, acc.: 64.06%] [G loss: 1.109322]\n",
      "2145 [D loss: 0.770506, acc.: 54.69%] [G loss: 0.855880]\n",
      "2146 [D loss: 0.634824, acc.: 71.88%] [G loss: 1.142182]\n",
      "2147 [D loss: 0.656020, acc.: 59.38%] [G loss: 1.059793]\n",
      "2148 [D loss: 0.679683, acc.: 54.69%] [G loss: 1.023661]\n",
      "2149 [D loss: 0.603890, acc.: 65.62%] [G loss: 1.149769]\n",
      "2150 [D loss: 0.653465, acc.: 59.38%] [G loss: 0.853016]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2151 [D loss: 0.712298, acc.: 53.12%] [G loss: 1.010208]\n",
      "2152 [D loss: 0.651656, acc.: 59.38%] [G loss: 0.977183]\n",
      "2153 [D loss: 0.709067, acc.: 53.12%] [G loss: 0.950850]\n",
      "2154 [D loss: 0.625471, acc.: 64.06%] [G loss: 1.035048]\n",
      "2155 [D loss: 0.687044, acc.: 59.38%] [G loss: 0.846609]\n",
      "2156 [D loss: 0.656970, acc.: 68.75%] [G loss: 0.969020]\n",
      "2157 [D loss: 0.709338, acc.: 54.69%] [G loss: 0.935025]\n",
      "2158 [D loss: 0.647785, acc.: 64.06%] [G loss: 1.055185]\n",
      "2159 [D loss: 0.746698, acc.: 50.00%] [G loss: 0.832996]\n",
      "2160 [D loss: 0.661411, acc.: 62.50%] [G loss: 0.868843]\n",
      "2161 [D loss: 0.647254, acc.: 64.06%] [G loss: 1.034050]\n",
      "2162 [D loss: 0.641913, acc.: 64.06%] [G loss: 1.096806]\n",
      "2163 [D loss: 0.625367, acc.: 59.38%] [G loss: 1.055770]\n",
      "2164 [D loss: 0.628524, acc.: 59.38%] [G loss: 1.052029]\n",
      "2165 [D loss: 0.749817, acc.: 51.56%] [G loss: 0.894783]\n",
      "2166 [D loss: 0.615483, acc.: 62.50%] [G loss: 1.176615]\n",
      "2167 [D loss: 0.663792, acc.: 67.19%] [G loss: 1.061816]\n",
      "2168 [D loss: 0.779485, acc.: 51.56%] [G loss: 0.980356]\n",
      "2169 [D loss: 0.641890, acc.: 60.94%] [G loss: 1.070532]\n",
      "2170 [D loss: 0.576438, acc.: 68.75%] [G loss: 1.122957]\n",
      "2171 [D loss: 0.646452, acc.: 59.38%] [G loss: 1.003199]\n",
      "2172 [D loss: 0.614852, acc.: 65.62%] [G loss: 1.009815]\n",
      "2173 [D loss: 0.676278, acc.: 64.06%] [G loss: 1.059852]\n",
      "2174 [D loss: 0.553220, acc.: 76.56%] [G loss: 1.062130]\n",
      "2175 [D loss: 0.739878, acc.: 51.56%] [G loss: 1.044747]\n",
      "2176 [D loss: 0.646987, acc.: 59.38%] [G loss: 0.851509]\n",
      "2177 [D loss: 0.560702, acc.: 75.00%] [G loss: 1.047737]\n",
      "2178 [D loss: 0.722476, acc.: 53.12%] [G loss: 1.099527]\n",
      "2179 [D loss: 0.650748, acc.: 65.62%] [G loss: 1.150604]\n",
      "2180 [D loss: 0.636931, acc.: 62.50%] [G loss: 0.987000]\n",
      "2181 [D loss: 0.673504, acc.: 57.81%] [G loss: 1.042471]\n",
      "2182 [D loss: 0.675795, acc.: 59.38%] [G loss: 1.049382]\n",
      "2183 [D loss: 0.662413, acc.: 65.62%] [G loss: 1.060217]\n",
      "2184 [D loss: 0.731155, acc.: 48.44%] [G loss: 0.878444]\n",
      "2185 [D loss: 0.668232, acc.: 57.81%] [G loss: 1.034293]\n",
      "2186 [D loss: 0.661272, acc.: 59.38%] [G loss: 0.991912]\n",
      "2187 [D loss: 0.656066, acc.: 57.81%] [G loss: 0.934044]\n",
      "2188 [D loss: 0.638678, acc.: 62.50%] [G loss: 1.070274]\n",
      "2189 [D loss: 0.652780, acc.: 57.81%] [G loss: 1.040979]\n",
      "2190 [D loss: 0.593789, acc.: 68.75%] [G loss: 0.871712]\n",
      "2191 [D loss: 0.665667, acc.: 54.69%] [G loss: 0.965890]\n",
      "2192 [D loss: 0.611601, acc.: 70.31%] [G loss: 1.075395]\n",
      "2193 [D loss: 0.579861, acc.: 71.88%] [G loss: 1.090400]\n",
      "2194 [D loss: 0.712521, acc.: 59.38%] [G loss: 0.838215]\n",
      "2195 [D loss: 0.635556, acc.: 64.06%] [G loss: 1.081610]\n",
      "2196 [D loss: 0.649411, acc.: 65.62%] [G loss: 0.999764]\n",
      "2197 [D loss: 0.724568, acc.: 54.69%] [G loss: 1.021566]\n",
      "2198 [D loss: 0.689451, acc.: 60.94%] [G loss: 1.024522]\n",
      "2199 [D loss: 0.674562, acc.: 56.25%] [G loss: 0.937583]\n",
      "2200 [D loss: 0.815202, acc.: 37.50%] [G loss: 1.048903]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2201 [D loss: 0.700443, acc.: 53.12%] [G loss: 1.046728]\n",
      "2202 [D loss: 0.658814, acc.: 60.94%] [G loss: 1.026677]\n",
      "2203 [D loss: 0.682023, acc.: 62.50%] [G loss: 0.995539]\n",
      "2204 [D loss: 0.690659, acc.: 57.81%] [G loss: 1.104138]\n",
      "2205 [D loss: 0.688386, acc.: 57.81%] [G loss: 1.078408]\n",
      "2206 [D loss: 0.684070, acc.: 56.25%] [G loss: 1.193216]\n",
      "2207 [D loss: 0.670189, acc.: 64.06%] [G loss: 1.023054]\n",
      "2208 [D loss: 0.582058, acc.: 62.50%] [G loss: 1.126043]\n",
      "2209 [D loss: 0.666135, acc.: 67.19%] [G loss: 1.069295]\n",
      "2210 [D loss: 0.731077, acc.: 56.25%] [G loss: 1.125289]\n",
      "2211 [D loss: 0.744183, acc.: 57.81%] [G loss: 0.864732]\n",
      "2212 [D loss: 0.661178, acc.: 54.69%] [G loss: 0.984648]\n",
      "2213 [D loss: 0.725991, acc.: 56.25%] [G loss: 1.127540]\n",
      "2214 [D loss: 0.660774, acc.: 60.94%] [G loss: 0.882249]\n",
      "2215 [D loss: 0.677508, acc.: 60.94%] [G loss: 0.953770]\n",
      "2216 [D loss: 0.694495, acc.: 56.25%] [G loss: 1.009163]\n",
      "2217 [D loss: 0.698175, acc.: 59.38%] [G loss: 1.054605]\n",
      "2218 [D loss: 0.624506, acc.: 60.94%] [G loss: 1.188219]\n",
      "2219 [D loss: 0.604200, acc.: 62.50%] [G loss: 1.031180]\n",
      "2220 [D loss: 0.657919, acc.: 67.19%] [G loss: 1.142619]\n",
      "2221 [D loss: 0.767307, acc.: 51.56%] [G loss: 1.023673]\n",
      "2222 [D loss: 0.670302, acc.: 62.50%] [G loss: 1.235838]\n",
      "2223 [D loss: 0.712027, acc.: 48.44%] [G loss: 1.049803]\n",
      "2224 [D loss: 0.702650, acc.: 46.88%] [G loss: 1.011351]\n",
      "2225 [D loss: 0.664770, acc.: 50.00%] [G loss: 1.005367]\n",
      "2226 [D loss: 0.618869, acc.: 64.06%] [G loss: 0.802988]\n",
      "2227 [D loss: 0.651563, acc.: 60.94%] [G loss: 1.009911]\n",
      "2228 [D loss: 0.748871, acc.: 50.00%] [G loss: 0.999781]\n",
      "2229 [D loss: 0.622196, acc.: 67.19%] [G loss: 0.840953]\n",
      "2230 [D loss: 0.724789, acc.: 50.00%] [G loss: 0.893110]\n",
      "2231 [D loss: 0.618544, acc.: 67.19%] [G loss: 1.056777]\n",
      "2232 [D loss: 0.614890, acc.: 65.62%] [G loss: 1.197137]\n",
      "2233 [D loss: 0.662947, acc.: 64.06%] [G loss: 1.081211]\n",
      "2234 [D loss: 0.606159, acc.: 75.00%] [G loss: 1.164844]\n",
      "2235 [D loss: 0.744988, acc.: 50.00%] [G loss: 0.988522]\n",
      "2236 [D loss: 0.705176, acc.: 48.44%] [G loss: 0.922177]\n",
      "2237 [D loss: 0.676401, acc.: 59.38%] [G loss: 0.877189]\n",
      "2238 [D loss: 0.661641, acc.: 68.75%] [G loss: 1.002159]\n",
      "2239 [D loss: 0.698207, acc.: 53.12%] [G loss: 1.014039]\n",
      "2240 [D loss: 0.699260, acc.: 53.12%] [G loss: 0.765336]\n",
      "2241 [D loss: 0.639653, acc.: 67.19%] [G loss: 0.971358]\n",
      "2242 [D loss: 0.738114, acc.: 43.75%] [G loss: 1.027517]\n",
      "2243 [D loss: 0.715867, acc.: 54.69%] [G loss: 0.927871]\n",
      "2244 [D loss: 0.645554, acc.: 64.06%] [G loss: 1.075679]\n",
      "2245 [D loss: 0.704503, acc.: 51.56%] [G loss: 1.035310]\n",
      "2246 [D loss: 0.675944, acc.: 65.62%] [G loss: 1.185227]\n",
      "2247 [D loss: 0.622373, acc.: 70.31%] [G loss: 0.979872]\n",
      "2248 [D loss: 0.686824, acc.: 59.38%] [G loss: 1.042678]\n",
      "2249 [D loss: 0.722151, acc.: 48.44%] [G loss: 0.943798]\n",
      "2250 [D loss: 0.651813, acc.: 54.69%] [G loss: 0.932012]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2251 [D loss: 0.665509, acc.: 60.94%] [G loss: 1.017753]\n",
      "2252 [D loss: 0.629174, acc.: 62.50%] [G loss: 1.041218]\n",
      "2253 [D loss: 0.608633, acc.: 68.75%] [G loss: 0.894735]\n",
      "2254 [D loss: 0.641350, acc.: 65.62%] [G loss: 1.009150]\n",
      "2255 [D loss: 0.513082, acc.: 75.00%] [G loss: 1.171291]\n",
      "2256 [D loss: 0.718403, acc.: 59.38%] [G loss: 0.874996]\n",
      "2257 [D loss: 0.635692, acc.: 60.94%] [G loss: 0.795156]\n",
      "2258 [D loss: 0.674099, acc.: 64.06%] [G loss: 1.013480]\n",
      "2259 [D loss: 0.694993, acc.: 56.25%] [G loss: 1.046042]\n",
      "2260 [D loss: 0.680092, acc.: 57.81%] [G loss: 0.988341]\n",
      "2261 [D loss: 0.675255, acc.: 59.38%] [G loss: 0.975287]\n",
      "2262 [D loss: 0.669576, acc.: 56.25%] [G loss: 1.076715]\n",
      "2263 [D loss: 0.640500, acc.: 59.38%] [G loss: 1.055965]\n",
      "2264 [D loss: 0.626869, acc.: 62.50%] [G loss: 1.003677]\n",
      "2265 [D loss: 0.662669, acc.: 59.38%] [G loss: 1.035232]\n",
      "2266 [D loss: 0.672820, acc.: 53.12%] [G loss: 0.849012]\n",
      "2267 [D loss: 0.702168, acc.: 64.06%] [G loss: 1.202496]\n",
      "2268 [D loss: 0.739676, acc.: 50.00%] [G loss: 1.142282]\n",
      "2269 [D loss: 0.704749, acc.: 56.25%] [G loss: 0.903131]\n",
      "2270 [D loss: 0.724424, acc.: 56.25%] [G loss: 0.888207]\n",
      "2271 [D loss: 0.590743, acc.: 65.62%] [G loss: 1.057410]\n",
      "2272 [D loss: 0.584038, acc.: 65.62%] [G loss: 1.148078]\n",
      "2273 [D loss: 0.760462, acc.: 50.00%] [G loss: 0.855744]\n",
      "2274 [D loss: 0.698487, acc.: 56.25%] [G loss: 1.159063]\n",
      "2275 [D loss: 0.616280, acc.: 67.19%] [G loss: 0.973542]\n",
      "2276 [D loss: 0.689140, acc.: 53.12%] [G loss: 0.992469]\n",
      "2277 [D loss: 0.563397, acc.: 70.31%] [G loss: 1.185835]\n",
      "2278 [D loss: 0.646058, acc.: 59.38%] [G loss: 1.015958]\n",
      "2279 [D loss: 0.701014, acc.: 56.25%] [G loss: 0.987294]\n",
      "2280 [D loss: 0.680568, acc.: 59.38%] [G loss: 0.994102]\n",
      "2281 [D loss: 0.802333, acc.: 45.31%] [G loss: 0.808220]\n",
      "2282 [D loss: 0.677413, acc.: 64.06%] [G loss: 0.960403]\n",
      "2283 [D loss: 0.752563, acc.: 46.88%] [G loss: 1.133843]\n",
      "2284 [D loss: 0.678109, acc.: 57.81%] [G loss: 1.130684]\n",
      "2285 [D loss: 0.601160, acc.: 70.31%] [G loss: 1.016987]\n",
      "2286 [D loss: 0.686864, acc.: 59.38%] [G loss: 1.023633]\n",
      "2287 [D loss: 0.636566, acc.: 62.50%] [G loss: 0.884739]\n",
      "2288 [D loss: 0.741371, acc.: 50.00%] [G loss: 0.999842]\n",
      "2289 [D loss: 0.589325, acc.: 62.50%] [G loss: 0.962912]\n",
      "2290 [D loss: 0.640461, acc.: 65.62%] [G loss: 1.089963]\n",
      "2291 [D loss: 0.639472, acc.: 64.06%] [G loss: 0.930660]\n",
      "2292 [D loss: 0.619393, acc.: 70.31%] [G loss: 1.036070]\n",
      "2293 [D loss: 0.625333, acc.: 65.62%] [G loss: 1.060131]\n",
      "2294 [D loss: 0.609195, acc.: 64.06%] [G loss: 0.961405]\n",
      "2295 [D loss: 0.645020, acc.: 64.06%] [G loss: 0.941860]\n",
      "2296 [D loss: 0.633585, acc.: 59.38%] [G loss: 0.803121]\n",
      "2297 [D loss: 0.572567, acc.: 70.31%] [G loss: 1.030265]\n",
      "2298 [D loss: 0.761899, acc.: 54.69%] [G loss: 0.785818]\n",
      "2299 [D loss: 0.695777, acc.: 62.50%] [G loss: 1.087970]\n",
      "2300 [D loss: 0.777579, acc.: 46.88%] [G loss: 0.876360]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2301 [D loss: 0.639540, acc.: 68.75%] [G loss: 1.031016]\n",
      "2302 [D loss: 0.654297, acc.: 67.19%] [G loss: 0.997526]\n",
      "2303 [D loss: 0.733236, acc.: 53.12%] [G loss: 1.060104]\n",
      "2304 [D loss: 0.675892, acc.: 56.25%] [G loss: 0.999705]\n",
      "2305 [D loss: 0.607331, acc.: 64.06%] [G loss: 1.115971]\n",
      "2306 [D loss: 0.691269, acc.: 51.56%] [G loss: 0.926417]\n",
      "2307 [D loss: 0.636461, acc.: 62.50%] [G loss: 0.901417]\n",
      "2308 [D loss: 0.656199, acc.: 62.50%] [G loss: 1.028198]\n",
      "2309 [D loss: 0.649000, acc.: 65.62%] [G loss: 0.976707]\n",
      "2310 [D loss: 0.732426, acc.: 45.31%] [G loss: 0.928627]\n",
      "2311 [D loss: 0.696197, acc.: 57.81%] [G loss: 1.063599]\n",
      "2312 [D loss: 0.687046, acc.: 53.12%] [G loss: 1.112785]\n",
      "2313 [D loss: 0.656851, acc.: 57.81%] [G loss: 1.074235]\n",
      "2314 [D loss: 0.741055, acc.: 46.88%] [G loss: 1.012276]\n",
      "2315 [D loss: 0.810972, acc.: 45.31%] [G loss: 1.116681]\n",
      "2316 [D loss: 0.736154, acc.: 51.56%] [G loss: 1.088942]\n",
      "2317 [D loss: 0.685811, acc.: 57.81%] [G loss: 1.022556]\n",
      "2318 [D loss: 0.709873, acc.: 53.12%] [G loss: 0.875416]\n",
      "2319 [D loss: 0.660118, acc.: 54.69%] [G loss: 1.169172]\n",
      "2320 [D loss: 0.651856, acc.: 57.81%] [G loss: 0.941363]\n",
      "2321 [D loss: 0.671687, acc.: 59.38%] [G loss: 0.888385]\n",
      "2322 [D loss: 0.677779, acc.: 62.50%] [G loss: 1.040797]\n",
      "2323 [D loss: 0.669160, acc.: 57.81%] [G loss: 0.876620]\n",
      "2324 [D loss: 0.685871, acc.: 56.25%] [G loss: 0.997311]\n",
      "2325 [D loss: 0.729038, acc.: 54.69%] [G loss: 1.029738]\n",
      "2326 [D loss: 0.625010, acc.: 62.50%] [G loss: 0.951844]\n",
      "2327 [D loss: 0.632039, acc.: 65.62%] [G loss: 1.151208]\n",
      "2328 [D loss: 0.726767, acc.: 57.81%] [G loss: 1.160611]\n",
      "2329 [D loss: 0.711682, acc.: 53.12%] [G loss: 1.110592]\n",
      "2330 [D loss: 0.603155, acc.: 70.31%] [G loss: 1.134577]\n",
      "2331 [D loss: 0.628194, acc.: 67.19%] [G loss: 1.166728]\n",
      "2332 [D loss: 0.556544, acc.: 67.19%] [G loss: 1.172826]\n",
      "2333 [D loss: 0.644823, acc.: 60.94%] [G loss: 1.093831]\n",
      "2334 [D loss: 0.613167, acc.: 59.38%] [G loss: 1.163615]\n",
      "2335 [D loss: 0.670581, acc.: 60.94%] [G loss: 1.003970]\n",
      "2336 [D loss: 0.699783, acc.: 59.38%] [G loss: 0.807328]\n",
      "2337 [D loss: 0.726798, acc.: 59.38%] [G loss: 0.953358]\n",
      "2338 [D loss: 0.695577, acc.: 56.25%] [G loss: 1.000047]\n",
      "2339 [D loss: 0.693029, acc.: 51.56%] [G loss: 1.041724]\n",
      "2340 [D loss: 0.699420, acc.: 53.12%] [G loss: 1.103942]\n",
      "2341 [D loss: 0.690198, acc.: 59.38%] [G loss: 1.026476]\n",
      "2342 [D loss: 0.680360, acc.: 56.25%] [G loss: 1.000593]\n",
      "2343 [D loss: 0.676274, acc.: 68.75%] [G loss: 1.152830]\n",
      "2344 [D loss: 0.646389, acc.: 59.38%] [G loss: 0.827738]\n",
      "2345 [D loss: 0.662362, acc.: 62.50%] [G loss: 0.990102]\n",
      "2346 [D loss: 0.688816, acc.: 54.69%] [G loss: 1.001889]\n",
      "2347 [D loss: 0.636416, acc.: 65.62%] [G loss: 0.979384]\n",
      "2348 [D loss: 0.586905, acc.: 67.19%] [G loss: 1.032385]\n",
      "2349 [D loss: 0.602753, acc.: 68.75%] [G loss: 1.032384]\n",
      "2350 [D loss: 0.742419, acc.: 53.12%] [G loss: 1.035762]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2351 [D loss: 0.667793, acc.: 62.50%] [G loss: 1.112242]\n",
      "2352 [D loss: 0.637089, acc.: 64.06%] [G loss: 1.046352]\n",
      "2353 [D loss: 0.563395, acc.: 70.31%] [G loss: 1.183566]\n",
      "2354 [D loss: 0.772948, acc.: 46.88%] [G loss: 0.952452]\n",
      "2355 [D loss: 0.684990, acc.: 60.94%] [G loss: 0.916863]\n",
      "2356 [D loss: 0.693103, acc.: 56.25%] [G loss: 1.004710]\n",
      "2357 [D loss: 0.699864, acc.: 59.38%] [G loss: 1.039084]\n",
      "2358 [D loss: 0.709895, acc.: 54.69%] [G loss: 0.835895]\n",
      "2359 [D loss: 0.571157, acc.: 68.75%] [G loss: 1.024449]\n",
      "2360 [D loss: 0.683704, acc.: 57.81%] [G loss: 1.054484]\n",
      "2361 [D loss: 0.627306, acc.: 67.19%] [G loss: 1.223411]\n",
      "2362 [D loss: 0.573229, acc.: 71.88%] [G loss: 1.212911]\n",
      "2363 [D loss: 0.700378, acc.: 53.12%] [G loss: 1.018610]\n",
      "2364 [D loss: 0.636423, acc.: 68.75%] [G loss: 0.980090]\n",
      "2365 [D loss: 0.578434, acc.: 78.12%] [G loss: 1.168244]\n",
      "2366 [D loss: 0.657081, acc.: 62.50%] [G loss: 0.838803]\n",
      "2367 [D loss: 0.642839, acc.: 62.50%] [G loss: 0.826831]\n",
      "2368 [D loss: 0.594852, acc.: 64.06%] [G loss: 1.110059]\n",
      "2369 [D loss: 0.701015, acc.: 56.25%] [G loss: 1.175146]\n",
      "2370 [D loss: 0.793831, acc.: 46.88%] [G loss: 0.902061]\n",
      "2371 [D loss: 0.616418, acc.: 60.94%] [G loss: 1.143056]\n",
      "2372 [D loss: 0.724737, acc.: 54.69%] [G loss: 0.825399]\n",
      "2373 [D loss: 0.645105, acc.: 64.06%] [G loss: 1.074354]\n",
      "2374 [D loss: 0.634744, acc.: 62.50%] [G loss: 1.043315]\n",
      "2375 [D loss: 0.623785, acc.: 62.50%] [G loss: 0.982246]\n",
      "2376 [D loss: 0.671556, acc.: 59.38%] [G loss: 1.194064]\n",
      "2377 [D loss: 0.734464, acc.: 53.12%] [G loss: 0.945562]\n",
      "2378 [D loss: 0.716246, acc.: 54.69%] [G loss: 0.949719]\n",
      "2379 [D loss: 0.706722, acc.: 56.25%] [G loss: 1.014326]\n",
      "2380 [D loss: 0.647823, acc.: 57.81%] [G loss: 1.000277]\n",
      "2381 [D loss: 0.619240, acc.: 70.31%] [G loss: 1.148538]\n",
      "2382 [D loss: 0.589417, acc.: 70.31%] [G loss: 0.836806]\n",
      "2383 [D loss: 0.686380, acc.: 59.38%] [G loss: 0.989375]\n",
      "2384 [D loss: 0.643102, acc.: 59.38%] [G loss: 1.018597]\n",
      "2385 [D loss: 0.724250, acc.: 53.12%] [G loss: 0.982596]\n",
      "2386 [D loss: 0.693580, acc.: 53.12%] [G loss: 1.167376]\n",
      "2387 [D loss: 0.657484, acc.: 59.38%] [G loss: 0.889527]\n",
      "2388 [D loss: 0.706131, acc.: 60.94%] [G loss: 1.031679]\n",
      "2389 [D loss: 0.714628, acc.: 51.56%] [G loss: 0.991768]\n",
      "2390 [D loss: 0.637687, acc.: 57.81%] [G loss: 1.099844]\n",
      "2391 [D loss: 0.667766, acc.: 59.38%] [G loss: 0.923855]\n",
      "2392 [D loss: 0.670045, acc.: 57.81%] [G loss: 1.141674]\n",
      "2393 [D loss: 0.640930, acc.: 64.06%] [G loss: 0.932289]\n",
      "2394 [D loss: 0.589484, acc.: 68.75%] [G loss: 1.122524]\n",
      "2395 [D loss: 0.730727, acc.: 53.12%] [G loss: 1.100426]\n",
      "2396 [D loss: 0.614747, acc.: 62.50%] [G loss: 1.181799]\n",
      "2397 [D loss: 0.689962, acc.: 67.19%] [G loss: 0.930020]\n",
      "2398 [D loss: 0.659351, acc.: 67.19%] [G loss: 0.947153]\n",
      "2399 [D loss: 0.657592, acc.: 62.50%] [G loss: 1.113096]\n",
      "2400 [D loss: 0.672252, acc.: 60.94%] [G loss: 1.069931]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2401 [D loss: 0.645157, acc.: 67.19%] [G loss: 1.025958]\n",
      "2402 [D loss: 0.686895, acc.: 59.38%] [G loss: 0.910923]\n",
      "2403 [D loss: 0.652730, acc.: 62.50%] [G loss: 0.912993]\n",
      "2404 [D loss: 0.650694, acc.: 59.38%] [G loss: 1.169069]\n",
      "2405 [D loss: 0.628038, acc.: 64.06%] [G loss: 0.927463]\n",
      "2406 [D loss: 0.625777, acc.: 71.88%] [G loss: 1.084622]\n",
      "2407 [D loss: 0.668515, acc.: 56.25%] [G loss: 1.140071]\n",
      "2408 [D loss: 0.687910, acc.: 62.50%] [G loss: 0.874330]\n",
      "2409 [D loss: 0.683174, acc.: 56.25%] [G loss: 0.855736]\n",
      "2410 [D loss: 0.757489, acc.: 51.56%] [G loss: 0.716760]\n",
      "2411 [D loss: 0.703069, acc.: 56.25%] [G loss: 0.830184]\n",
      "2412 [D loss: 0.644841, acc.: 60.94%] [G loss: 0.982774]\n",
      "2413 [D loss: 0.606909, acc.: 65.62%] [G loss: 1.056403]\n",
      "2414 [D loss: 0.629847, acc.: 65.62%] [G loss: 0.902199]\n",
      "2415 [D loss: 0.644784, acc.: 62.50%] [G loss: 0.946123]\n",
      "2416 [D loss: 0.661199, acc.: 59.38%] [G loss: 1.080219]\n",
      "2417 [D loss: 0.651896, acc.: 62.50%] [G loss: 1.132998]\n",
      "2418 [D loss: 0.582122, acc.: 75.00%] [G loss: 1.131459]\n",
      "2419 [D loss: 0.709953, acc.: 59.38%] [G loss: 1.048716]\n",
      "2420 [D loss: 0.713534, acc.: 48.44%] [G loss: 0.986930]\n",
      "2421 [D loss: 0.735550, acc.: 43.75%] [G loss: 0.910257]\n",
      "2422 [D loss: 0.677122, acc.: 57.81%] [G loss: 0.917032]\n",
      "2423 [D loss: 0.733095, acc.: 54.69%] [G loss: 1.032793]\n",
      "2424 [D loss: 0.640734, acc.: 67.19%] [G loss: 0.936648]\n",
      "2425 [D loss: 0.560832, acc.: 78.12%] [G loss: 1.252774]\n",
      "2426 [D loss: 0.641162, acc.: 67.19%] [G loss: 0.966390]\n",
      "2427 [D loss: 0.731493, acc.: 50.00%] [G loss: 0.812761]\n",
      "2428 [D loss: 0.663691, acc.: 62.50%] [G loss: 1.071973]\n",
      "2429 [D loss: 0.741041, acc.: 51.56%] [G loss: 1.047810]\n",
      "2430 [D loss: 0.616567, acc.: 70.31%] [G loss: 0.889965]\n",
      "2431 [D loss: 0.760190, acc.: 53.12%] [G loss: 0.930060]\n",
      "2432 [D loss: 0.648330, acc.: 62.50%] [G loss: 0.945547]\n",
      "2433 [D loss: 0.586045, acc.: 71.88%] [G loss: 1.070539]\n",
      "2434 [D loss: 0.559344, acc.: 70.31%] [G loss: 1.056971]\n",
      "2435 [D loss: 0.642772, acc.: 62.50%] [G loss: 1.033385]\n",
      "2436 [D loss: 0.613414, acc.: 65.62%] [G loss: 0.935368]\n",
      "2437 [D loss: 0.607905, acc.: 64.06%] [G loss: 1.076198]\n",
      "2438 [D loss: 0.691181, acc.: 46.88%] [G loss: 0.997245]\n",
      "2439 [D loss: 0.704620, acc.: 54.69%] [G loss: 0.909097]\n",
      "2440 [D loss: 0.605748, acc.: 64.06%] [G loss: 0.938268]\n",
      "2441 [D loss: 0.694670, acc.: 59.38%] [G loss: 1.087843]\n",
      "2442 [D loss: 0.677923, acc.: 57.81%] [G loss: 0.998292]\n",
      "2443 [D loss: 0.759284, acc.: 51.56%] [G loss: 1.012617]\n",
      "2444 [D loss: 0.622111, acc.: 65.62%] [G loss: 1.091986]\n",
      "2445 [D loss: 0.636472, acc.: 70.31%] [G loss: 1.030373]\n",
      "2446 [D loss: 0.667553, acc.: 62.50%] [G loss: 0.943471]\n",
      "2447 [D loss: 0.641730, acc.: 65.62%] [G loss: 1.194978]\n",
      "2448 [D loss: 0.638090, acc.: 59.38%] [G loss: 1.051722]\n",
      "2449 [D loss: 0.723156, acc.: 51.56%] [G loss: 0.975781]\n",
      "2450 [D loss: 0.642252, acc.: 65.62%] [G loss: 0.953416]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2451 [D loss: 0.833389, acc.: 54.69%] [G loss: 1.035254]\n",
      "2452 [D loss: 0.686412, acc.: 60.94%] [G loss: 0.901126]\n",
      "2453 [D loss: 0.624493, acc.: 64.06%] [G loss: 1.013837]\n",
      "2454 [D loss: 0.636233, acc.: 64.06%] [G loss: 0.985937]\n",
      "2455 [D loss: 0.599196, acc.: 57.81%] [G loss: 0.931336]\n",
      "2456 [D loss: 0.729486, acc.: 59.38%] [G loss: 0.825499]\n",
      "2457 [D loss: 0.578380, acc.: 70.31%] [G loss: 1.093699]\n",
      "2458 [D loss: 0.661882, acc.: 53.12%] [G loss: 1.081250]\n",
      "2459 [D loss: 0.654710, acc.: 64.06%] [G loss: 1.018870]\n",
      "2460 [D loss: 0.698414, acc.: 51.56%] [G loss: 1.181937]\n",
      "2461 [D loss: 0.645002, acc.: 62.50%] [G loss: 1.013433]\n",
      "2462 [D loss: 0.624451, acc.: 68.75%] [G loss: 1.136957]\n",
      "2463 [D loss: 0.548788, acc.: 78.12%] [G loss: 0.945826]\n",
      "2464 [D loss: 0.627839, acc.: 64.06%] [G loss: 1.051900]\n",
      "2465 [D loss: 0.678510, acc.: 56.25%] [G loss: 0.843378]\n",
      "2466 [D loss: 0.695643, acc.: 54.69%] [G loss: 1.154050]\n",
      "2467 [D loss: 0.659949, acc.: 60.94%] [G loss: 0.900267]\n",
      "2468 [D loss: 0.586085, acc.: 71.88%] [G loss: 1.355699]\n",
      "2469 [D loss: 0.609683, acc.: 67.19%] [G loss: 1.065064]\n",
      "2470 [D loss: 0.730026, acc.: 50.00%] [G loss: 0.758058]\n",
      "2471 [D loss: 0.670509, acc.: 62.50%] [G loss: 1.175292]\n",
      "2472 [D loss: 0.584749, acc.: 71.88%] [G loss: 1.222934]\n",
      "2473 [D loss: 0.630351, acc.: 64.06%] [G loss: 0.876037]\n",
      "2474 [D loss: 0.650213, acc.: 56.25%] [G loss: 0.818859]\n",
      "2475 [D loss: 0.757227, acc.: 48.44%] [G loss: 1.018250]\n",
      "2476 [D loss: 0.652942, acc.: 60.94%] [G loss: 1.074947]\n",
      "2477 [D loss: 0.599214, acc.: 68.75%] [G loss: 1.083730]\n",
      "2478 [D loss: 0.574352, acc.: 68.75%] [G loss: 0.913836]\n",
      "2479 [D loss: 0.671171, acc.: 59.38%] [G loss: 0.998910]\n",
      "2480 [D loss: 0.683560, acc.: 54.69%] [G loss: 1.072678]\n",
      "2481 [D loss: 0.620675, acc.: 62.50%] [G loss: 0.982686]\n",
      "2482 [D loss: 0.756443, acc.: 51.56%] [G loss: 1.072318]\n",
      "2483 [D loss: 0.540046, acc.: 75.00%] [G loss: 1.074988]\n",
      "2484 [D loss: 0.713137, acc.: 54.69%] [G loss: 1.002181]\n",
      "2485 [D loss: 0.691204, acc.: 56.25%] [G loss: 0.877904]\n",
      "2486 [D loss: 0.615102, acc.: 62.50%] [G loss: 1.055810]\n",
      "2487 [D loss: 0.703856, acc.: 57.81%] [G loss: 0.921135]\n",
      "2488 [D loss: 0.670305, acc.: 56.25%] [G loss: 0.899366]\n",
      "2489 [D loss: 0.676225, acc.: 57.81%] [G loss: 1.020355]\n",
      "2490 [D loss: 0.541912, acc.: 71.88%] [G loss: 1.046247]\n",
      "2491 [D loss: 0.699278, acc.: 60.94%] [G loss: 0.985918]\n",
      "2492 [D loss: 0.573342, acc.: 75.00%] [G loss: 1.040314]\n",
      "2493 [D loss: 0.630055, acc.: 60.94%] [G loss: 0.985785]\n",
      "2494 [D loss: 0.623103, acc.: 64.06%] [G loss: 1.121485]\n",
      "2495 [D loss: 0.619338, acc.: 65.62%] [G loss: 1.046644]\n",
      "2496 [D loss: 0.589626, acc.: 67.19%] [G loss: 1.149328]\n",
      "2497 [D loss: 0.710841, acc.: 56.25%] [G loss: 1.002726]\n",
      "2498 [D loss: 0.608480, acc.: 68.75%] [G loss: 0.991453]\n",
      "2499 [D loss: 0.681225, acc.: 64.06%] [G loss: 0.943523]\n",
      "2500 [D loss: 0.749657, acc.: 54.69%] [G loss: 1.000607]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2501 [D loss: 0.723162, acc.: 54.69%] [G loss: 1.197620]\n",
      "2502 [D loss: 0.604915, acc.: 65.62%] [G loss: 0.903156]\n",
      "2503 [D loss: 0.662361, acc.: 59.38%] [G loss: 0.881814]\n",
      "2504 [D loss: 0.599216, acc.: 65.62%] [G loss: 1.033473]\n",
      "2505 [D loss: 0.593514, acc.: 71.88%] [G loss: 1.097054]\n",
      "2506 [D loss: 0.704989, acc.: 57.81%] [G loss: 1.097606]\n",
      "2507 [D loss: 0.705214, acc.: 56.25%] [G loss: 1.073860]\n",
      "2508 [D loss: 0.665546, acc.: 56.25%] [G loss: 1.035553]\n",
      "2509 [D loss: 0.651420, acc.: 57.81%] [G loss: 1.129481]\n",
      "2510 [D loss: 0.660770, acc.: 64.06%] [G loss: 1.085130]\n",
      "2511 [D loss: 0.593168, acc.: 65.62%] [G loss: 1.058644]\n",
      "2512 [D loss: 0.634187, acc.: 65.62%] [G loss: 1.022732]\n",
      "2513 [D loss: 0.610131, acc.: 67.19%] [G loss: 1.122115]\n",
      "2514 [D loss: 0.697183, acc.: 56.25%] [G loss: 0.924679]\n",
      "2515 [D loss: 0.683614, acc.: 57.81%] [G loss: 0.967329]\n",
      "2516 [D loss: 0.692216, acc.: 57.81%] [G loss: 0.990384]\n",
      "2517 [D loss: 0.639084, acc.: 62.50%] [G loss: 1.051424]\n",
      "2518 [D loss: 0.607242, acc.: 67.19%] [G loss: 0.899977]\n",
      "2519 [D loss: 0.659642, acc.: 62.50%] [G loss: 1.041248]\n",
      "2520 [D loss: 0.765923, acc.: 46.88%] [G loss: 0.913619]\n",
      "2521 [D loss: 0.597204, acc.: 65.62%] [G loss: 0.961296]\n",
      "2522 [D loss: 0.641881, acc.: 67.19%] [G loss: 1.002049]\n",
      "2523 [D loss: 0.633687, acc.: 65.62%] [G loss: 1.005878]\n",
      "2524 [D loss: 0.603386, acc.: 65.62%] [G loss: 1.059982]\n",
      "2525 [D loss: 0.643181, acc.: 62.50%] [G loss: 0.931292]\n",
      "2526 [D loss: 0.662760, acc.: 54.69%] [G loss: 1.129715]\n",
      "2527 [D loss: 0.725794, acc.: 56.25%] [G loss: 0.944360]\n",
      "2528 [D loss: 0.635625, acc.: 65.62%] [G loss: 1.041087]\n",
      "2529 [D loss: 0.664054, acc.: 56.25%] [G loss: 0.938201]\n",
      "2530 [D loss: 0.617632, acc.: 64.06%] [G loss: 0.962141]\n",
      "2531 [D loss: 0.613435, acc.: 64.06%] [G loss: 0.982127]\n",
      "2532 [D loss: 0.645988, acc.: 70.31%] [G loss: 1.127031]\n",
      "2533 [D loss: 0.658160, acc.: 62.50%] [G loss: 0.917003]\n",
      "2534 [D loss: 0.617145, acc.: 65.62%] [G loss: 0.997984]\n",
      "2535 [D loss: 0.643050, acc.: 60.94%] [G loss: 0.955164]\n",
      "2536 [D loss: 0.629580, acc.: 64.06%] [G loss: 1.046830]\n",
      "2537 [D loss: 0.635435, acc.: 60.94%] [G loss: 1.122766]\n",
      "2538 [D loss: 0.622587, acc.: 70.31%] [G loss: 0.957825]\n",
      "2539 [D loss: 0.669473, acc.: 62.50%] [G loss: 1.070462]\n",
      "2540 [D loss: 0.632111, acc.: 62.50%] [G loss: 1.014066]\n",
      "2541 [D loss: 0.722405, acc.: 53.12%] [G loss: 1.140612]\n",
      "2542 [D loss: 0.625563, acc.: 68.75%] [G loss: 1.050027]\n",
      "2543 [D loss: 0.696327, acc.: 54.69%] [G loss: 0.996376]\n",
      "2544 [D loss: 0.703901, acc.: 56.25%] [G loss: 1.131085]\n",
      "2545 [D loss: 0.722387, acc.: 51.56%] [G loss: 0.871809]\n",
      "2546 [D loss: 0.650569, acc.: 56.25%] [G loss: 1.049516]\n",
      "2547 [D loss: 0.633479, acc.: 64.06%] [G loss: 1.037397]\n",
      "2548 [D loss: 0.655277, acc.: 62.50%] [G loss: 1.031779]\n",
      "2549 [D loss: 0.701329, acc.: 56.25%] [G loss: 1.111898]\n",
      "2550 [D loss: 0.614110, acc.: 64.06%] [G loss: 0.983561]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2551 [D loss: 0.534225, acc.: 73.44%] [G loss: 1.130293]\n",
      "2552 [D loss: 0.655617, acc.: 60.94%] [G loss: 1.018547]\n",
      "2553 [D loss: 0.740696, acc.: 51.56%] [G loss: 0.914705]\n",
      "2554 [D loss: 0.618601, acc.: 68.75%] [G loss: 0.980146]\n",
      "2555 [D loss: 0.741879, acc.: 54.69%] [G loss: 1.011391]\n",
      "2556 [D loss: 0.640221, acc.: 64.06%] [G loss: 0.963842]\n",
      "2557 [D loss: 0.686374, acc.: 54.69%] [G loss: 1.003251]\n",
      "2558 [D loss: 0.680015, acc.: 60.94%] [G loss: 1.020533]\n",
      "2559 [D loss: 0.639924, acc.: 67.19%] [G loss: 0.876335]\n",
      "2560 [D loss: 0.633245, acc.: 62.50%] [G loss: 1.056323]\n",
      "2561 [D loss: 0.647997, acc.: 67.19%] [G loss: 1.028895]\n",
      "2562 [D loss: 0.650433, acc.: 65.62%] [G loss: 0.958466]\n",
      "2563 [D loss: 0.627827, acc.: 75.00%] [G loss: 1.102695]\n",
      "2564 [D loss: 0.699617, acc.: 59.38%] [G loss: 0.941057]\n",
      "2565 [D loss: 0.615856, acc.: 62.50%] [G loss: 1.002675]\n",
      "2566 [D loss: 0.787403, acc.: 46.88%] [G loss: 0.992016]\n",
      "2567 [D loss: 0.612325, acc.: 68.75%] [G loss: 1.231806]\n",
      "2568 [D loss: 0.765105, acc.: 51.56%] [G loss: 0.977015]\n",
      "2569 [D loss: 0.594062, acc.: 71.88%] [G loss: 0.927556]\n",
      "2570 [D loss: 0.692059, acc.: 50.00%] [G loss: 1.094045]\n",
      "2571 [D loss: 0.652796, acc.: 62.50%] [G loss: 0.944345]\n",
      "2572 [D loss: 0.750214, acc.: 56.25%] [G loss: 0.951602]\n",
      "2573 [D loss: 0.603085, acc.: 62.50%] [G loss: 1.193551]\n",
      "2574 [D loss: 0.666969, acc.: 62.50%] [G loss: 0.861672]\n",
      "2575 [D loss: 0.606954, acc.: 65.62%] [G loss: 0.922844]\n",
      "2576 [D loss: 0.594518, acc.: 56.25%] [G loss: 0.870311]\n",
      "2577 [D loss: 0.619170, acc.: 68.75%] [G loss: 1.102232]\n",
      "2578 [D loss: 0.613840, acc.: 65.62%] [G loss: 0.981608]\n",
      "2579 [D loss: 0.607251, acc.: 70.31%] [G loss: 1.040018]\n",
      "2580 [D loss: 0.573912, acc.: 65.62%] [G loss: 1.154271]\n",
      "2581 [D loss: 0.662161, acc.: 64.06%] [G loss: 1.221249]\n",
      "2582 [D loss: 0.688048, acc.: 57.81%] [G loss: 1.228070]\n",
      "2583 [D loss: 0.752527, acc.: 51.56%] [G loss: 0.946046]\n",
      "2584 [D loss: 0.704056, acc.: 59.38%] [G loss: 0.976250]\n",
      "2585 [D loss: 0.649161, acc.: 62.50%] [G loss: 0.936477]\n",
      "2586 [D loss: 0.688635, acc.: 60.94%] [G loss: 1.033378]\n",
      "2587 [D loss: 0.688119, acc.: 57.81%] [G loss: 1.089713]\n",
      "2588 [D loss: 0.633752, acc.: 59.38%] [G loss: 0.979178]\n",
      "2589 [D loss: 0.713687, acc.: 53.12%] [G loss: 1.042781]\n",
      "2590 [D loss: 0.663970, acc.: 64.06%] [G loss: 0.957278]\n",
      "2591 [D loss: 0.687588, acc.: 60.94%] [G loss: 0.897783]\n",
      "2592 [D loss: 0.736064, acc.: 51.56%] [G loss: 0.997479]\n",
      "2593 [D loss: 0.708393, acc.: 57.81%] [G loss: 1.142331]\n",
      "2594 [D loss: 0.665795, acc.: 62.50%] [G loss: 1.185463]\n",
      "2595 [D loss: 0.726847, acc.: 50.00%] [G loss: 1.012705]\n",
      "2596 [D loss: 0.610926, acc.: 68.75%] [G loss: 0.853003]\n",
      "2597 [D loss: 0.611726, acc.: 71.88%] [G loss: 0.951475]\n",
      "2598 [D loss: 0.685023, acc.: 59.38%] [G loss: 1.074210]\n",
      "2599 [D loss: 0.686346, acc.: 56.25%] [G loss: 1.014351]\n",
      "2600 [D loss: 0.701659, acc.: 54.69%] [G loss: 1.071782]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2601 [D loss: 0.675996, acc.: 59.38%] [G loss: 1.076040]\n",
      "2602 [D loss: 0.618007, acc.: 67.19%] [G loss: 0.962437]\n",
      "2603 [D loss: 0.561409, acc.: 75.00%] [G loss: 0.884116]\n",
      "2604 [D loss: 0.622988, acc.: 67.19%] [G loss: 0.989623]\n",
      "2605 [D loss: 0.582772, acc.: 78.12%] [G loss: 0.952678]\n",
      "2606 [D loss: 0.580050, acc.: 67.19%] [G loss: 0.998324]\n",
      "2607 [D loss: 0.737284, acc.: 54.69%] [G loss: 0.901320]\n",
      "2608 [D loss: 0.641293, acc.: 64.06%] [G loss: 1.084614]\n",
      "2609 [D loss: 0.671561, acc.: 60.94%] [G loss: 1.056658]\n",
      "2610 [D loss: 0.682474, acc.: 56.25%] [G loss: 0.982224]\n",
      "2611 [D loss: 0.600335, acc.: 71.88%] [G loss: 0.975598]\n",
      "2612 [D loss: 0.605570, acc.: 65.62%] [G loss: 0.984872]\n",
      "2613 [D loss: 0.689192, acc.: 59.38%] [G loss: 0.891389]\n",
      "2614 [D loss: 0.681957, acc.: 60.94%] [G loss: 1.093594]\n",
      "2615 [D loss: 0.666422, acc.: 59.38%] [G loss: 1.063954]\n",
      "2616 [D loss: 0.723600, acc.: 54.69%] [G loss: 1.005991]\n",
      "2617 [D loss: 0.601286, acc.: 64.06%] [G loss: 1.252219]\n",
      "2618 [D loss: 0.680036, acc.: 64.06%] [G loss: 0.869257]\n",
      "2619 [D loss: 0.665798, acc.: 60.94%] [G loss: 1.030709]\n",
      "2620 [D loss: 0.673906, acc.: 57.81%] [G loss: 0.907878]\n",
      "2621 [D loss: 0.669976, acc.: 60.94%] [G loss: 1.022387]\n",
      "2622 [D loss: 0.675380, acc.: 54.69%] [G loss: 0.967694]\n",
      "2623 [D loss: 0.681006, acc.: 59.38%] [G loss: 0.975490]\n",
      "2624 [D loss: 0.612465, acc.: 64.06%] [G loss: 1.076847]\n",
      "2625 [D loss: 0.628066, acc.: 64.06%] [G loss: 0.964323]\n",
      "2626 [D loss: 0.653124, acc.: 64.06%] [G loss: 0.945479]\n",
      "2627 [D loss: 0.594755, acc.: 67.19%] [G loss: 0.986678]\n",
      "2628 [D loss: 0.646112, acc.: 59.38%] [G loss: 1.072464]\n",
      "2629 [D loss: 0.690623, acc.: 60.94%] [G loss: 1.084352]\n",
      "2630 [D loss: 0.601510, acc.: 68.75%] [G loss: 1.177786]\n",
      "2631 [D loss: 0.636458, acc.: 62.50%] [G loss: 1.013238]\n",
      "2632 [D loss: 0.698733, acc.: 51.56%] [G loss: 1.052344]\n",
      "2633 [D loss: 0.608314, acc.: 65.62%] [G loss: 1.181244]\n",
      "2634 [D loss: 0.649547, acc.: 59.38%] [G loss: 0.984146]\n",
      "2635 [D loss: 0.688348, acc.: 59.38%] [G loss: 0.835010]\n",
      "2636 [D loss: 0.692127, acc.: 57.81%] [G loss: 0.886092]\n",
      "2637 [D loss: 0.617521, acc.: 65.62%] [G loss: 0.914624]\n",
      "2638 [D loss: 0.695222, acc.: 59.38%] [G loss: 0.990731]\n",
      "2639 [D loss: 0.656305, acc.: 64.06%] [G loss: 1.105864]\n",
      "2640 [D loss: 0.679543, acc.: 60.94%] [G loss: 1.059618]\n",
      "2641 [D loss: 0.673769, acc.: 67.19%] [G loss: 1.249786]\n",
      "2642 [D loss: 0.641546, acc.: 62.50%] [G loss: 1.034307]\n",
      "2643 [D loss: 0.682128, acc.: 54.69%] [G loss: 0.963160]\n",
      "2644 [D loss: 0.678551, acc.: 54.69%] [G loss: 1.028825]\n",
      "2645 [D loss: 0.661498, acc.: 62.50%] [G loss: 1.090566]\n",
      "2646 [D loss: 0.545731, acc.: 75.00%] [G loss: 1.114153]\n",
      "2647 [D loss: 0.671630, acc.: 57.81%] [G loss: 1.037538]\n",
      "2648 [D loss: 0.618214, acc.: 60.94%] [G loss: 0.921968]\n",
      "2649 [D loss: 0.635820, acc.: 67.19%] [G loss: 1.134216]\n",
      "2650 [D loss: 0.640039, acc.: 62.50%] [G loss: 1.120660]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2651 [D loss: 0.664909, acc.: 65.62%] [G loss: 1.019789]\n",
      "2652 [D loss: 0.599671, acc.: 67.19%] [G loss: 1.018375]\n",
      "2653 [D loss: 0.630562, acc.: 62.50%] [G loss: 0.962285]\n",
      "2654 [D loss: 0.679539, acc.: 56.25%] [G loss: 0.970130]\n",
      "2655 [D loss: 0.584299, acc.: 75.00%] [G loss: 0.984800]\n",
      "2656 [D loss: 0.626768, acc.: 64.06%] [G loss: 1.092110]\n",
      "2657 [D loss: 0.713776, acc.: 53.12%] [G loss: 0.965275]\n",
      "2658 [D loss: 0.551333, acc.: 71.88%] [G loss: 1.082651]\n",
      "2659 [D loss: 0.662113, acc.: 62.50%] [G loss: 1.002032]\n",
      "2660 [D loss: 0.716619, acc.: 54.69%] [G loss: 0.869821]\n",
      "2661 [D loss: 0.664404, acc.: 54.69%] [G loss: 0.872164]\n",
      "2662 [D loss: 0.691558, acc.: 54.69%] [G loss: 1.004576]\n",
      "2663 [D loss: 0.695678, acc.: 53.12%] [G loss: 1.151087]\n",
      "2664 [D loss: 0.652945, acc.: 60.94%] [G loss: 1.119057]\n",
      "2665 [D loss: 0.581302, acc.: 75.00%] [G loss: 1.027827]\n",
      "2666 [D loss: 0.701721, acc.: 59.38%] [G loss: 0.869009]\n",
      "2667 [D loss: 0.696283, acc.: 54.69%] [G loss: 1.118693]\n",
      "2668 [D loss: 0.710507, acc.: 54.69%] [G loss: 1.142710]\n",
      "2669 [D loss: 0.799823, acc.: 45.31%] [G loss: 0.996723]\n",
      "2670 [D loss: 0.604506, acc.: 65.62%] [G loss: 1.119872]\n",
      "2671 [D loss: 0.707046, acc.: 54.69%] [G loss: 0.978559]\n",
      "2672 [D loss: 0.697235, acc.: 54.69%] [G loss: 0.997013]\n",
      "2673 [D loss: 0.667853, acc.: 57.81%] [G loss: 1.023966]\n",
      "2674 [D loss: 0.522265, acc.: 76.56%] [G loss: 1.249086]\n",
      "2675 [D loss: 0.715390, acc.: 60.94%] [G loss: 0.917009]\n",
      "2676 [D loss: 0.670017, acc.: 53.12%] [G loss: 0.948458]\n",
      "2677 [D loss: 0.539669, acc.: 73.44%] [G loss: 0.998558]\n",
      "2678 [D loss: 0.692576, acc.: 57.81%] [G loss: 0.973907]\n",
      "2679 [D loss: 0.660876, acc.: 62.50%] [G loss: 0.976198]\n",
      "2680 [D loss: 0.609013, acc.: 67.19%] [G loss: 1.131016]\n",
      "2681 [D loss: 0.660356, acc.: 68.75%] [G loss: 1.297718]\n",
      "2682 [D loss: 0.618814, acc.: 70.31%] [G loss: 1.023359]\n",
      "2683 [D loss: 0.762860, acc.: 54.69%] [G loss: 1.024787]\n",
      "2684 [D loss: 0.583466, acc.: 65.62%] [G loss: 1.250091]\n",
      "2685 [D loss: 0.729653, acc.: 50.00%] [G loss: 1.069083]\n",
      "2686 [D loss: 0.724769, acc.: 51.56%] [G loss: 0.973055]\n",
      "2687 [D loss: 0.679781, acc.: 59.38%] [G loss: 0.977775]\n",
      "2688 [D loss: 0.554426, acc.: 78.12%] [G loss: 1.129380]\n",
      "2689 [D loss: 0.624022, acc.: 71.88%] [G loss: 1.005769]\n",
      "2690 [D loss: 0.785922, acc.: 56.25%] [G loss: 0.954574]\n",
      "2691 [D loss: 0.671745, acc.: 65.62%] [G loss: 1.008681]\n",
      "2692 [D loss: 0.627386, acc.: 68.75%] [G loss: 0.957281]\n",
      "2693 [D loss: 0.812066, acc.: 46.88%] [G loss: 0.973849]\n",
      "2694 [D loss: 0.770421, acc.: 51.56%] [G loss: 0.992261]\n",
      "2695 [D loss: 0.728338, acc.: 54.69%] [G loss: 1.028182]\n",
      "2696 [D loss: 0.621181, acc.: 67.19%] [G loss: 1.076975]\n",
      "2697 [D loss: 0.690396, acc.: 59.38%] [G loss: 1.099172]\n",
      "2698 [D loss: 0.730453, acc.: 46.88%] [G loss: 1.002292]\n",
      "2699 [D loss: 0.554028, acc.: 75.00%] [G loss: 1.078145]\n",
      "2700 [D loss: 0.562480, acc.: 70.31%] [G loss: 0.933662]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2701 [D loss: 0.594372, acc.: 68.75%] [G loss: 1.089445]\n",
      "2702 [D loss: 0.616925, acc.: 73.44%] [G loss: 1.021267]\n",
      "2703 [D loss: 0.630652, acc.: 67.19%] [G loss: 0.916445]\n",
      "2704 [D loss: 0.644557, acc.: 59.38%] [G loss: 1.071528]\n",
      "2705 [D loss: 0.663246, acc.: 60.94%] [G loss: 0.924252]\n",
      "2706 [D loss: 0.643223, acc.: 62.50%] [G loss: 1.096541]\n",
      "2707 [D loss: 0.616871, acc.: 71.88%] [G loss: 0.865750]\n",
      "2708 [D loss: 0.719893, acc.: 46.88%] [G loss: 1.033557]\n",
      "2709 [D loss: 0.718317, acc.: 62.50%] [G loss: 1.174318]\n",
      "2710 [D loss: 0.754008, acc.: 50.00%] [G loss: 1.002887]\n",
      "2711 [D loss: 0.705938, acc.: 51.56%] [G loss: 0.891516]\n",
      "2712 [D loss: 0.609634, acc.: 70.31%] [G loss: 1.086676]\n",
      "2713 [D loss: 0.531439, acc.: 76.56%] [G loss: 1.233762]\n",
      "2714 [D loss: 0.643497, acc.: 65.62%] [G loss: 1.150612]\n",
      "2715 [D loss: 0.648821, acc.: 60.94%] [G loss: 0.982674]\n",
      "2716 [D loss: 0.648212, acc.: 65.62%] [G loss: 1.001969]\n",
      "2717 [D loss: 0.542531, acc.: 75.00%] [G loss: 1.160195]\n",
      "2718 [D loss: 0.656730, acc.: 62.50%] [G loss: 1.055173]\n",
      "2719 [D loss: 0.683330, acc.: 57.81%] [G loss: 0.850420]\n",
      "2720 [D loss: 0.557902, acc.: 75.00%] [G loss: 0.976558]\n",
      "2721 [D loss: 0.636251, acc.: 65.62%] [G loss: 1.019533]\n",
      "2722 [D loss: 0.704865, acc.: 57.81%] [G loss: 0.974926]\n",
      "2723 [D loss: 0.719168, acc.: 56.25%] [G loss: 0.941550]\n",
      "2724 [D loss: 0.689193, acc.: 53.12%] [G loss: 0.949551]\n",
      "2725 [D loss: 0.775446, acc.: 45.31%] [G loss: 0.982867]\n",
      "2726 [D loss: 0.781185, acc.: 45.31%] [G loss: 0.952420]\n",
      "2727 [D loss: 0.683011, acc.: 51.56%] [G loss: 1.106060]\n",
      "2728 [D loss: 0.758737, acc.: 48.44%] [G loss: 1.036329]\n",
      "2729 [D loss: 0.651194, acc.: 65.62%] [G loss: 0.922725]\n",
      "2730 [D loss: 0.742216, acc.: 45.31%] [G loss: 1.031067]\n",
      "2731 [D loss: 0.601191, acc.: 64.06%] [G loss: 1.053398]\n",
      "2732 [D loss: 0.751408, acc.: 54.69%] [G loss: 0.951107]\n",
      "2733 [D loss: 0.658477, acc.: 59.38%] [G loss: 1.035381]\n",
      "2734 [D loss: 0.693478, acc.: 57.81%] [G loss: 1.019138]\n",
      "2735 [D loss: 0.644302, acc.: 60.94%] [G loss: 0.943866]\n",
      "2736 [D loss: 0.679264, acc.: 60.94%] [G loss: 0.995366]\n",
      "2737 [D loss: 0.609387, acc.: 64.06%] [G loss: 1.016598]\n",
      "2738 [D loss: 0.702845, acc.: 50.00%] [G loss: 0.821173]\n",
      "2739 [D loss: 0.598152, acc.: 68.75%] [G loss: 1.166771]\n",
      "2740 [D loss: 0.714430, acc.: 59.38%] [G loss: 0.885032]\n",
      "2741 [D loss: 0.571955, acc.: 68.75%] [G loss: 0.908305]\n",
      "2742 [D loss: 0.644439, acc.: 59.38%] [G loss: 1.131760]\n",
      "2743 [D loss: 0.757399, acc.: 51.56%] [G loss: 0.980062]\n",
      "2744 [D loss: 0.612758, acc.: 64.06%] [G loss: 0.903493]\n",
      "2745 [D loss: 0.612401, acc.: 64.06%] [G loss: 1.139710]\n",
      "2746 [D loss: 0.625570, acc.: 65.62%] [G loss: 1.105924]\n",
      "2747 [D loss: 0.611699, acc.: 60.94%] [G loss: 0.916192]\n",
      "2748 [D loss: 0.696455, acc.: 50.00%] [G loss: 0.998853]\n",
      "2749 [D loss: 0.489280, acc.: 87.50%] [G loss: 1.165680]\n",
      "2750 [D loss: 0.721769, acc.: 56.25%] [G loss: 1.000586]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2751 [D loss: 0.614119, acc.: 60.94%] [G loss: 1.145545]\n",
      "2752 [D loss: 0.670112, acc.: 62.50%] [G loss: 1.053089]\n",
      "2753 [D loss: 0.640915, acc.: 59.38%] [G loss: 1.079718]\n",
      "2754 [D loss: 0.664220, acc.: 56.25%] [G loss: 0.938072]\n",
      "2755 [D loss: 0.745614, acc.: 50.00%] [G loss: 0.893764]\n",
      "2756 [D loss: 0.645142, acc.: 64.06%] [G loss: 1.052415]\n",
      "2757 [D loss: 0.618521, acc.: 64.06%] [G loss: 1.040505]\n",
      "2758 [D loss: 0.709420, acc.: 56.25%] [G loss: 1.080684]\n",
      "2759 [D loss: 0.705606, acc.: 53.12%] [G loss: 0.925565]\n",
      "2760 [D loss: 0.632190, acc.: 68.75%] [G loss: 0.858244]\n",
      "2761 [D loss: 0.656878, acc.: 60.94%] [G loss: 0.952527]\n",
      "2762 [D loss: 0.625826, acc.: 70.31%] [G loss: 1.063614]\n",
      "2763 [D loss: 0.688248, acc.: 54.69%] [G loss: 0.871454]\n",
      "2764 [D loss: 0.651894, acc.: 59.38%] [G loss: 0.978563]\n",
      "2765 [D loss: 0.753903, acc.: 48.44%] [G loss: 0.969336]\n",
      "2766 [D loss: 0.662488, acc.: 65.62%] [G loss: 0.977143]\n",
      "2767 [D loss: 0.709280, acc.: 60.94%] [G loss: 0.932189]\n",
      "2768 [D loss: 0.646659, acc.: 62.50%] [G loss: 1.087335]\n",
      "2769 [D loss: 0.695342, acc.: 56.25%] [G loss: 1.019150]\n",
      "2770 [D loss: 0.659342, acc.: 56.25%] [G loss: 1.019943]\n",
      "2771 [D loss: 0.565019, acc.: 68.75%] [G loss: 1.033119]\n",
      "2772 [D loss: 0.662912, acc.: 57.81%] [G loss: 1.033170]\n",
      "2773 [D loss: 0.588770, acc.: 73.44%] [G loss: 1.094576]\n",
      "2774 [D loss: 0.566373, acc.: 71.88%] [G loss: 1.253666]\n",
      "2775 [D loss: 0.628916, acc.: 60.94%] [G loss: 1.035612]\n",
      "2776 [D loss: 0.643208, acc.: 59.38%] [G loss: 1.024106]\n",
      "2777 [D loss: 0.610950, acc.: 68.75%] [G loss: 0.995849]\n",
      "2778 [D loss: 0.642082, acc.: 62.50%] [G loss: 1.101052]\n",
      "2779 [D loss: 0.669239, acc.: 65.62%] [G loss: 1.029365]\n",
      "2780 [D loss: 0.710577, acc.: 54.69%] [G loss: 1.010261]\n",
      "2781 [D loss: 0.581323, acc.: 75.00%] [G loss: 1.172370]\n",
      "2782 [D loss: 0.604470, acc.: 65.62%] [G loss: 1.203386]\n",
      "2783 [D loss: 0.602000, acc.: 67.19%] [G loss: 1.229955]\n",
      "2784 [D loss: 0.663641, acc.: 62.50%] [G loss: 1.036097]\n",
      "2785 [D loss: 0.635463, acc.: 65.62%] [G loss: 1.010223]\n",
      "2786 [D loss: 0.597581, acc.: 62.50%] [G loss: 0.947159]\n",
      "2787 [D loss: 0.652662, acc.: 59.38%] [G loss: 0.957458]\n",
      "2788 [D loss: 0.713163, acc.: 53.12%] [G loss: 1.100551]\n",
      "2789 [D loss: 0.601506, acc.: 67.19%] [G loss: 1.229395]\n",
      "2790 [D loss: 0.682588, acc.: 60.94%] [G loss: 0.961974]\n",
      "2791 [D loss: 0.661016, acc.: 56.25%] [G loss: 1.045200]\n",
      "2792 [D loss: 0.604411, acc.: 73.44%] [G loss: 0.973127]\n",
      "2793 [D loss: 0.760838, acc.: 56.25%] [G loss: 1.006562]\n",
      "2794 [D loss: 0.619657, acc.: 62.50%] [G loss: 1.002206]\n",
      "2795 [D loss: 0.694569, acc.: 57.81%] [G loss: 0.978786]\n",
      "2796 [D loss: 0.596586, acc.: 67.19%] [G loss: 0.846990]\n",
      "2797 [D loss: 0.617712, acc.: 62.50%] [G loss: 0.973439]\n",
      "2798 [D loss: 0.723784, acc.: 60.94%] [G loss: 1.038955]\n",
      "2799 [D loss: 0.620536, acc.: 65.62%] [G loss: 1.060817]\n",
      "2800 [D loss: 0.659090, acc.: 62.50%] [G loss: 1.012726]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2801 [D loss: 0.672920, acc.: 57.81%] [G loss: 1.031550]\n",
      "2802 [D loss: 0.671082, acc.: 59.38%] [G loss: 0.964235]\n",
      "2803 [D loss: 0.701359, acc.: 59.38%] [G loss: 1.111754]\n",
      "2804 [D loss: 0.628411, acc.: 70.31%] [G loss: 0.934475]\n",
      "2805 [D loss: 0.669259, acc.: 57.81%] [G loss: 0.919902]\n",
      "2806 [D loss: 0.569859, acc.: 64.06%] [G loss: 1.188255]\n",
      "2807 [D loss: 0.574094, acc.: 68.75%] [G loss: 1.132690]\n",
      "2808 [D loss: 0.768898, acc.: 45.31%] [G loss: 0.954306]\n",
      "2809 [D loss: 0.680692, acc.: 60.94%] [G loss: 0.961469]\n",
      "2810 [D loss: 0.708251, acc.: 56.25%] [G loss: 1.005685]\n",
      "2811 [D loss: 0.679061, acc.: 62.50%] [G loss: 1.038007]\n",
      "2812 [D loss: 0.632381, acc.: 64.06%] [G loss: 1.047535]\n",
      "2813 [D loss: 0.698474, acc.: 56.25%] [G loss: 1.058048]\n",
      "2814 [D loss: 0.685748, acc.: 56.25%] [G loss: 1.054915]\n",
      "2815 [D loss: 0.685404, acc.: 57.81%] [G loss: 0.960827]\n",
      "2816 [D loss: 0.691322, acc.: 51.56%] [G loss: 0.855613]\n",
      "2817 [D loss: 0.757740, acc.: 50.00%] [G loss: 0.885248]\n",
      "2818 [D loss: 0.678430, acc.: 57.81%] [G loss: 0.838326]\n",
      "2819 [D loss: 0.581424, acc.: 75.00%] [G loss: 1.088380]\n",
      "2820 [D loss: 0.706698, acc.: 60.94%] [G loss: 1.031705]\n",
      "2821 [D loss: 0.559972, acc.: 79.69%] [G loss: 1.144175]\n",
      "2822 [D loss: 0.675255, acc.: 59.38%] [G loss: 0.893755]\n",
      "2823 [D loss: 0.623783, acc.: 67.19%] [G loss: 0.902104]\n",
      "2824 [D loss: 0.614212, acc.: 62.50%] [G loss: 0.939815]\n",
      "2825 [D loss: 0.591466, acc.: 65.62%] [G loss: 1.063633]\n",
      "2826 [D loss: 0.599422, acc.: 59.38%] [G loss: 0.994821]\n",
      "2827 [D loss: 0.603909, acc.: 65.62%] [G loss: 1.089397]\n",
      "2828 [D loss: 0.717817, acc.: 50.00%] [G loss: 1.056201]\n",
      "2829 [D loss: 0.640552, acc.: 65.62%] [G loss: 1.173817]\n",
      "2830 [D loss: 0.583004, acc.: 71.88%] [G loss: 0.956547]\n",
      "2831 [D loss: 0.701695, acc.: 56.25%] [G loss: 1.113657]\n",
      "2832 [D loss: 0.666130, acc.: 64.06%] [G loss: 1.176811]\n",
      "2833 [D loss: 0.694237, acc.: 57.81%] [G loss: 0.898805]\n",
      "2834 [D loss: 0.663247, acc.: 59.38%] [G loss: 0.999272]\n",
      "2835 [D loss: 0.672854, acc.: 56.25%] [G loss: 0.930030]\n",
      "2836 [D loss: 0.780315, acc.: 45.31%] [G loss: 0.844218]\n",
      "2837 [D loss: 0.641079, acc.: 67.19%] [G loss: 1.048223]\n",
      "2838 [D loss: 0.676921, acc.: 60.94%] [G loss: 0.951847]\n",
      "2839 [D loss: 0.724343, acc.: 48.44%] [G loss: 1.011670]\n",
      "2840 [D loss: 0.619099, acc.: 68.75%] [G loss: 1.040023]\n",
      "2841 [D loss: 0.631007, acc.: 65.62%] [G loss: 1.237339]\n",
      "2842 [D loss: 0.601280, acc.: 65.62%] [G loss: 1.003150]\n",
      "2843 [D loss: 0.561562, acc.: 75.00%] [G loss: 1.022551]\n",
      "2844 [D loss: 0.698051, acc.: 59.38%] [G loss: 0.933864]\n",
      "2845 [D loss: 0.671918, acc.: 56.25%] [G loss: 1.050217]\n",
      "2846 [D loss: 0.554758, acc.: 70.31%] [G loss: 1.050745]\n",
      "2847 [D loss: 0.722818, acc.: 60.94%] [G loss: 0.982403]\n",
      "2848 [D loss: 0.605529, acc.: 67.19%] [G loss: 0.925255]\n",
      "2849 [D loss: 0.705978, acc.: 60.94%] [G loss: 0.963564]\n",
      "2850 [D loss: 0.686304, acc.: 51.56%] [G loss: 0.875043]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2851 [D loss: 0.627162, acc.: 67.19%] [G loss: 0.890684]\n",
      "2852 [D loss: 0.685256, acc.: 54.69%] [G loss: 1.054503]\n",
      "2853 [D loss: 0.734667, acc.: 53.12%] [G loss: 1.073272]\n",
      "2854 [D loss: 0.646549, acc.: 64.06%] [G loss: 1.108831]\n",
      "2855 [D loss: 0.724609, acc.: 57.81%] [G loss: 1.000303]\n",
      "2856 [D loss: 0.610514, acc.: 64.06%] [G loss: 0.969391]\n",
      "2857 [D loss: 0.619820, acc.: 68.75%] [G loss: 0.999508]\n",
      "2858 [D loss: 0.545561, acc.: 70.31%] [G loss: 0.977127]\n",
      "2859 [D loss: 0.676432, acc.: 53.12%] [G loss: 1.028496]\n",
      "2860 [D loss: 0.620105, acc.: 62.50%] [G loss: 0.942416]\n",
      "2861 [D loss: 0.562856, acc.: 70.31%] [G loss: 0.993489]\n",
      "2862 [D loss: 0.749029, acc.: 53.12%] [G loss: 0.876619]\n",
      "2863 [D loss: 0.647732, acc.: 62.50%] [G loss: 0.994384]\n",
      "2864 [D loss: 0.716044, acc.: 51.56%] [G loss: 0.933524]\n",
      "2865 [D loss: 0.720624, acc.: 59.38%] [G loss: 0.856236]\n",
      "2866 [D loss: 0.695849, acc.: 57.81%] [G loss: 0.978135]\n",
      "2867 [D loss: 0.636280, acc.: 70.31%] [G loss: 0.896784]\n",
      "2868 [D loss: 0.660348, acc.: 60.94%] [G loss: 1.199382]\n",
      "2869 [D loss: 0.597190, acc.: 65.62%] [G loss: 0.939983]\n",
      "2870 [D loss: 0.629894, acc.: 64.06%] [G loss: 1.001012]\n",
      "2871 [D loss: 0.639359, acc.: 70.31%] [G loss: 1.028090]\n",
      "2872 [D loss: 0.705036, acc.: 56.25%] [G loss: 1.100805]\n",
      "2873 [D loss: 0.559954, acc.: 71.88%] [G loss: 1.052006]\n",
      "2874 [D loss: 0.595841, acc.: 64.06%] [G loss: 1.089570]\n",
      "2875 [D loss: 0.696124, acc.: 51.56%] [G loss: 0.945515]\n",
      "2876 [D loss: 0.565603, acc.: 70.31%] [G loss: 1.097342]\n",
      "2877 [D loss: 0.669830, acc.: 54.69%] [G loss: 0.995593]\n",
      "2878 [D loss: 0.539754, acc.: 70.31%] [G loss: 1.301511]\n",
      "2879 [D loss: 0.786247, acc.: 53.12%] [G loss: 0.770655]\n",
      "2880 [D loss: 0.593584, acc.: 62.50%] [G loss: 0.941370]\n",
      "2881 [D loss: 0.635884, acc.: 62.50%] [G loss: 0.933285]\n",
      "2882 [D loss: 0.641951, acc.: 64.06%] [G loss: 1.082738]\n",
      "2883 [D loss: 0.628392, acc.: 64.06%] [G loss: 1.022874]\n",
      "2884 [D loss: 0.707988, acc.: 57.81%] [G loss: 1.008249]\n",
      "2885 [D loss: 0.698671, acc.: 56.25%] [G loss: 1.032945]\n",
      "2886 [D loss: 0.708077, acc.: 53.12%] [G loss: 0.952694]\n",
      "2887 [D loss: 0.614097, acc.: 65.62%] [G loss: 0.909960]\n",
      "2888 [D loss: 0.685112, acc.: 56.25%] [G loss: 0.897280]\n",
      "2889 [D loss: 0.628951, acc.: 67.19%] [G loss: 1.019874]\n",
      "2890 [D loss: 0.603149, acc.: 65.62%] [G loss: 1.127710]\n",
      "2891 [D loss: 0.739138, acc.: 50.00%] [G loss: 0.889015]\n",
      "2892 [D loss: 0.639014, acc.: 64.06%] [G loss: 1.038703]\n",
      "2893 [D loss: 0.727958, acc.: 60.94%] [G loss: 1.067519]\n",
      "2894 [D loss: 0.657066, acc.: 54.69%] [G loss: 1.107333]\n",
      "2895 [D loss: 0.639461, acc.: 60.94%] [G loss: 1.040012]\n",
      "2896 [D loss: 0.707040, acc.: 51.56%] [G loss: 0.891553]\n",
      "2897 [D loss: 0.609665, acc.: 68.75%] [G loss: 0.917497]\n",
      "2898 [D loss: 0.534240, acc.: 73.44%] [G loss: 1.051731]\n",
      "2899 [D loss: 0.619072, acc.: 65.62%] [G loss: 1.071991]\n",
      "2900 [D loss: 0.713357, acc.: 53.12%] [G loss: 0.957744]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2901 [D loss: 0.645279, acc.: 62.50%] [G loss: 0.928319]\n",
      "2902 [D loss: 0.560077, acc.: 71.88%] [G loss: 1.044229]\n",
      "2903 [D loss: 0.636879, acc.: 65.62%] [G loss: 0.980437]\n",
      "2904 [D loss: 0.592303, acc.: 70.31%] [G loss: 1.018811]\n",
      "2905 [D loss: 0.598135, acc.: 75.00%] [G loss: 0.963140]\n",
      "2906 [D loss: 0.659499, acc.: 59.38%] [G loss: 1.115159]\n",
      "2907 [D loss: 0.731093, acc.: 59.38%] [G loss: 0.954039]\n",
      "2908 [D loss: 0.680111, acc.: 57.81%] [G loss: 0.935061]\n",
      "2909 [D loss: 0.580380, acc.: 71.88%] [G loss: 1.059659]\n",
      "2910 [D loss: 0.640390, acc.: 65.62%] [G loss: 1.078021]\n",
      "2911 [D loss: 0.677516, acc.: 54.69%] [G loss: 1.089894]\n",
      "2912 [D loss: 0.584330, acc.: 67.19%] [G loss: 1.074000]\n",
      "2913 [D loss: 0.628844, acc.: 62.50%] [G loss: 1.095581]\n",
      "2914 [D loss: 0.710722, acc.: 57.81%] [G loss: 0.963595]\n",
      "2915 [D loss: 0.727741, acc.: 51.56%] [G loss: 0.947391]\n",
      "2916 [D loss: 0.631918, acc.: 65.62%] [G loss: 1.163942]\n",
      "2917 [D loss: 0.735274, acc.: 51.56%] [G loss: 0.944923]\n",
      "2918 [D loss: 0.656703, acc.: 64.06%] [G loss: 0.999854]\n",
      "2919 [D loss: 0.708588, acc.: 53.12%] [G loss: 0.969221]\n",
      "2920 [D loss: 0.681150, acc.: 53.12%] [G loss: 0.964215]\n",
      "2921 [D loss: 0.672367, acc.: 59.38%] [G loss: 1.006108]\n",
      "2922 [D loss: 0.676095, acc.: 54.69%] [G loss: 1.179067]\n",
      "2923 [D loss: 0.721045, acc.: 51.56%] [G loss: 1.048494]\n",
      "2924 [D loss: 0.720811, acc.: 51.56%] [G loss: 0.887036]\n",
      "2925 [D loss: 0.723231, acc.: 51.56%] [G loss: 0.860981]\n",
      "2926 [D loss: 0.588949, acc.: 71.88%] [G loss: 0.895442]\n",
      "2927 [D loss: 0.680557, acc.: 51.56%] [G loss: 0.947695]\n",
      "2928 [D loss: 0.686722, acc.: 56.25%] [G loss: 0.952177]\n",
      "2929 [D loss: 0.673542, acc.: 60.94%] [G loss: 0.928232]\n",
      "2930 [D loss: 0.607164, acc.: 62.50%] [G loss: 1.122994]\n",
      "2931 [D loss: 0.622820, acc.: 68.75%] [G loss: 1.131862]\n",
      "2932 [D loss: 0.796520, acc.: 40.62%] [G loss: 1.019401]\n",
      "2933 [D loss: 0.727301, acc.: 48.44%] [G loss: 1.042091]\n",
      "2934 [D loss: 0.736731, acc.: 50.00%] [G loss: 0.854037]\n",
      "2935 [D loss: 0.682165, acc.: 57.81%] [G loss: 1.213530]\n",
      "2936 [D loss: 0.729681, acc.: 57.81%] [G loss: 0.864950]\n",
      "2937 [D loss: 0.708548, acc.: 59.38%] [G loss: 1.070482]\n",
      "2938 [D loss: 0.629623, acc.: 64.06%] [G loss: 0.970765]\n",
      "2939 [D loss: 0.663258, acc.: 59.38%] [G loss: 0.961410]\n",
      "2940 [D loss: 0.722678, acc.: 53.12%] [G loss: 0.895798]\n",
      "2941 [D loss: 0.693064, acc.: 57.81%] [G loss: 0.787653]\n",
      "2942 [D loss: 0.646719, acc.: 65.62%] [G loss: 0.991170]\n",
      "2943 [D loss: 0.674700, acc.: 56.25%] [G loss: 1.030897]\n",
      "2944 [D loss: 0.676633, acc.: 64.06%] [G loss: 1.021560]\n",
      "2945 [D loss: 0.680963, acc.: 53.12%] [G loss: 0.941012]\n",
      "2946 [D loss: 0.650211, acc.: 65.62%] [G loss: 1.007748]\n",
      "2947 [D loss: 0.711383, acc.: 54.69%] [G loss: 0.918419]\n",
      "2948 [D loss: 0.673293, acc.: 51.56%] [G loss: 0.933445]\n",
      "2949 [D loss: 0.711958, acc.: 46.88%] [G loss: 1.036232]\n",
      "2950 [D loss: 0.654057, acc.: 56.25%] [G loss: 0.875905]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "2951 [D loss: 0.599515, acc.: 65.62%] [G loss: 1.145698]\n",
      "2952 [D loss: 0.741147, acc.: 54.69%] [G loss: 1.007434]\n",
      "2953 [D loss: 0.649888, acc.: 59.38%] [G loss: 0.920730]\n",
      "2954 [D loss: 0.614605, acc.: 60.94%] [G loss: 0.964923]\n",
      "2955 [D loss: 0.615637, acc.: 68.75%] [G loss: 0.924750]\n",
      "2956 [D loss: 0.653901, acc.: 54.69%] [G loss: 1.018512]\n",
      "2957 [D loss: 0.677568, acc.: 64.06%] [G loss: 1.031816]\n",
      "2958 [D loss: 0.736938, acc.: 59.38%] [G loss: 0.995798]\n",
      "2959 [D loss: 0.655891, acc.: 64.06%] [G loss: 1.002549]\n",
      "2960 [D loss: 0.624661, acc.: 67.19%] [G loss: 1.255436]\n",
      "2961 [D loss: 0.524740, acc.: 78.12%] [G loss: 0.997951]\n",
      "2962 [D loss: 0.668175, acc.: 54.69%] [G loss: 1.029099]\n",
      "2963 [D loss: 0.583607, acc.: 79.69%] [G loss: 1.035899]\n",
      "2964 [D loss: 0.613255, acc.: 64.06%] [G loss: 0.985037]\n",
      "2965 [D loss: 0.671639, acc.: 64.06%] [G loss: 0.814472]\n",
      "2966 [D loss: 0.666815, acc.: 57.81%] [G loss: 1.006222]\n",
      "2967 [D loss: 0.736024, acc.: 57.81%] [G loss: 1.070783]\n",
      "2968 [D loss: 0.722352, acc.: 57.81%] [G loss: 0.918387]\n",
      "2969 [D loss: 0.690447, acc.: 57.81%] [G loss: 1.041671]\n",
      "2970 [D loss: 0.643857, acc.: 67.19%] [G loss: 0.935188]\n",
      "2971 [D loss: 0.625951, acc.: 60.94%] [G loss: 1.109746]\n",
      "2972 [D loss: 0.720953, acc.: 53.12%] [G loss: 0.905816]\n",
      "2973 [D loss: 0.725091, acc.: 60.94%] [G loss: 1.102787]\n",
      "2974 [D loss: 0.590940, acc.: 75.00%] [G loss: 0.986377]\n",
      "2975 [D loss: 0.661587, acc.: 62.50%] [G loss: 0.971530]\n",
      "2976 [D loss: 0.720293, acc.: 62.50%] [G loss: 0.791413]\n",
      "2977 [D loss: 0.639133, acc.: 67.19%] [G loss: 1.038513]\n",
      "2978 [D loss: 0.661957, acc.: 60.94%] [G loss: 0.891650]\n",
      "2979 [D loss: 0.620117, acc.: 64.06%] [G loss: 0.970717]\n",
      "2980 [D loss: 0.676613, acc.: 54.69%] [G loss: 0.983065]\n",
      "2981 [D loss: 0.738078, acc.: 48.44%] [G loss: 1.095961]\n",
      "2982 [D loss: 0.630967, acc.: 60.94%] [G loss: 1.110646]\n",
      "2983 [D loss: 0.673293, acc.: 65.62%] [G loss: 1.021130]\n",
      "2984 [D loss: 0.689525, acc.: 62.50%] [G loss: 0.939347]\n",
      "2985 [D loss: 0.651711, acc.: 64.06%] [G loss: 1.040416]\n",
      "2986 [D loss: 0.655254, acc.: 62.50%] [G loss: 0.968489]\n",
      "2987 [D loss: 0.797259, acc.: 48.44%] [G loss: 1.000015]\n",
      "2988 [D loss: 0.655567, acc.: 60.94%] [G loss: 1.036659]\n",
      "2989 [D loss: 0.597915, acc.: 67.19%] [G loss: 1.060981]\n",
      "2990 [D loss: 0.713273, acc.: 57.81%] [G loss: 0.973556]\n",
      "2991 [D loss: 0.704640, acc.: 57.81%] [G loss: 1.126721]\n",
      "2992 [D loss: 0.710390, acc.: 50.00%] [G loss: 1.163715]\n",
      "2993 [D loss: 0.691164, acc.: 56.25%] [G loss: 1.092047]\n",
      "2994 [D loss: 0.621369, acc.: 65.62%] [G loss: 1.075185]\n",
      "2995 [D loss: 0.657179, acc.: 60.94%] [G loss: 1.059515]\n",
      "2996 [D loss: 0.629038, acc.: 62.50%] [G loss: 0.885352]\n",
      "2997 [D loss: 0.623597, acc.: 62.50%] [G loss: 0.860617]\n",
      "2998 [D loss: 0.587376, acc.: 65.62%] [G loss: 1.061938]\n",
      "2999 [D loss: 0.661237, acc.: 62.50%] [G loss: 0.884047]\n",
      "3000 [D loss: 0.629230, acc.: 62.50%] [G loss: 0.848003]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3001 [D loss: 0.629181, acc.: 65.62%] [G loss: 1.074727]\n",
      "3002 [D loss: 0.611464, acc.: 67.19%] [G loss: 1.020589]\n",
      "3003 [D loss: 0.702016, acc.: 60.94%] [G loss: 1.008992]\n",
      "3004 [D loss: 0.627749, acc.: 60.94%] [G loss: 1.103343]\n",
      "3005 [D loss: 0.634683, acc.: 62.50%] [G loss: 0.866287]\n",
      "3006 [D loss: 0.582053, acc.: 70.31%] [G loss: 1.042340]\n",
      "3007 [D loss: 0.665449, acc.: 59.38%] [G loss: 1.020674]\n",
      "3008 [D loss: 0.613967, acc.: 68.75%] [G loss: 1.035948]\n",
      "3009 [D loss: 0.576846, acc.: 70.31%] [G loss: 1.135330]\n",
      "3010 [D loss: 0.641535, acc.: 67.19%] [G loss: 1.080542]\n",
      "3011 [D loss: 0.581811, acc.: 73.44%] [G loss: 0.888713]\n",
      "3012 [D loss: 0.562596, acc.: 78.12%] [G loss: 0.949425]\n",
      "3013 [D loss: 0.692932, acc.: 60.94%] [G loss: 0.931369]\n",
      "3014 [D loss: 0.611343, acc.: 67.19%] [G loss: 1.176028]\n",
      "3015 [D loss: 0.568179, acc.: 75.00%] [G loss: 1.082970]\n",
      "3016 [D loss: 0.643571, acc.: 57.81%] [G loss: 1.029770]\n",
      "3017 [D loss: 0.613654, acc.: 67.19%] [G loss: 1.086399]\n",
      "3018 [D loss: 0.706160, acc.: 53.12%] [G loss: 1.061478]\n",
      "3019 [D loss: 0.683635, acc.: 59.38%] [G loss: 1.181661]\n",
      "3020 [D loss: 0.663340, acc.: 57.81%] [G loss: 1.086451]\n",
      "3021 [D loss: 0.657438, acc.: 62.50%] [G loss: 1.005298]\n",
      "3022 [D loss: 0.622896, acc.: 59.38%] [G loss: 0.819745]\n",
      "3023 [D loss: 0.582753, acc.: 70.31%] [G loss: 1.121077]\n",
      "3024 [D loss: 0.596629, acc.: 62.50%] [G loss: 1.017542]\n",
      "3025 [D loss: 0.645674, acc.: 62.50%] [G loss: 1.083938]\n",
      "3026 [D loss: 0.698431, acc.: 50.00%] [G loss: 0.868162]\n",
      "3027 [D loss: 0.676301, acc.: 60.94%] [G loss: 0.918187]\n",
      "3028 [D loss: 0.626463, acc.: 60.94%] [G loss: 0.944551]\n",
      "3029 [D loss: 0.609102, acc.: 70.31%] [G loss: 1.042947]\n",
      "3030 [D loss: 0.559533, acc.: 65.62%] [G loss: 1.061734]\n",
      "3031 [D loss: 0.603863, acc.: 67.19%] [G loss: 1.105689]\n",
      "3032 [D loss: 0.700569, acc.: 50.00%] [G loss: 0.935088]\n",
      "3033 [D loss: 0.701097, acc.: 59.38%] [G loss: 1.011376]\n",
      "3034 [D loss: 0.642061, acc.: 65.62%] [G loss: 1.083924]\n",
      "3035 [D loss: 0.616168, acc.: 68.75%] [G loss: 1.150565]\n",
      "3036 [D loss: 0.616275, acc.: 64.06%] [G loss: 0.987885]\n",
      "3037 [D loss: 0.640851, acc.: 60.94%] [G loss: 1.059686]\n",
      "3038 [D loss: 0.649008, acc.: 60.94%] [G loss: 0.918561]\n",
      "3039 [D loss: 0.682742, acc.: 53.12%] [G loss: 0.900249]\n",
      "3040 [D loss: 0.717941, acc.: 53.12%] [G loss: 0.948757]\n",
      "3041 [D loss: 0.673428, acc.: 54.69%] [G loss: 1.041877]\n",
      "3042 [D loss: 0.716279, acc.: 59.38%] [G loss: 0.943201]\n",
      "3043 [D loss: 0.659956, acc.: 68.75%] [G loss: 0.983886]\n",
      "3044 [D loss: 0.692596, acc.: 54.69%] [G loss: 0.984541]\n",
      "3045 [D loss: 0.627258, acc.: 68.75%] [G loss: 1.181182]\n",
      "3046 [D loss: 0.569448, acc.: 78.12%] [G loss: 1.144395]\n",
      "3047 [D loss: 0.672614, acc.: 64.06%] [G loss: 1.224853]\n",
      "3048 [D loss: 0.621552, acc.: 65.62%] [G loss: 1.013811]\n",
      "3049 [D loss: 0.655692, acc.: 62.50%] [G loss: 0.899580]\n",
      "3050 [D loss: 0.615293, acc.: 67.19%] [G loss: 0.996600]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3051 [D loss: 0.609072, acc.: 68.75%] [G loss: 1.134928]\n",
      "3052 [D loss: 0.613101, acc.: 60.94%] [G loss: 0.959414]\n",
      "3053 [D loss: 0.654592, acc.: 64.06%] [G loss: 1.180776]\n",
      "3054 [D loss: 0.662169, acc.: 56.25%] [G loss: 1.097086]\n",
      "3055 [D loss: 0.640985, acc.: 62.50%] [G loss: 1.109379]\n",
      "3056 [D loss: 0.611923, acc.: 65.62%] [G loss: 1.070337]\n",
      "3057 [D loss: 0.752294, acc.: 48.44%] [G loss: 0.808607]\n",
      "3058 [D loss: 0.718567, acc.: 56.25%] [G loss: 1.142702]\n",
      "3059 [D loss: 0.685288, acc.: 54.69%] [G loss: 0.708685]\n",
      "3060 [D loss: 0.564751, acc.: 70.31%] [G loss: 1.100858]\n",
      "3061 [D loss: 0.590951, acc.: 70.31%] [G loss: 1.141186]\n",
      "3062 [D loss: 0.735611, acc.: 50.00%] [G loss: 1.062436]\n",
      "3063 [D loss: 0.818576, acc.: 43.75%] [G loss: 0.876027]\n",
      "3064 [D loss: 0.726441, acc.: 54.69%] [G loss: 0.913118]\n",
      "3065 [D loss: 0.681836, acc.: 57.81%] [G loss: 1.043295]\n",
      "3066 [D loss: 0.662807, acc.: 59.38%] [G loss: 1.020185]\n",
      "3067 [D loss: 0.638334, acc.: 68.75%] [G loss: 1.104697]\n",
      "3068 [D loss: 0.606604, acc.: 64.06%] [G loss: 1.121780]\n",
      "3069 [D loss: 0.644031, acc.: 67.19%] [G loss: 1.015566]\n",
      "3070 [D loss: 0.565669, acc.: 73.44%] [G loss: 0.873514]\n",
      "3071 [D loss: 0.571836, acc.: 62.50%] [G loss: 1.027212]\n",
      "3072 [D loss: 0.680537, acc.: 60.94%] [G loss: 1.081675]\n",
      "3073 [D loss: 0.630424, acc.: 59.38%] [G loss: 0.934924]\n",
      "3074 [D loss: 0.601794, acc.: 70.31%] [G loss: 0.891340]\n",
      "3075 [D loss: 0.693595, acc.: 56.25%] [G loss: 1.136385]\n",
      "3076 [D loss: 0.588310, acc.: 70.31%] [G loss: 0.978881]\n",
      "3077 [D loss: 0.667614, acc.: 56.25%] [G loss: 0.975459]\n",
      "3078 [D loss: 0.712616, acc.: 56.25%] [G loss: 1.025035]\n",
      "3079 [D loss: 0.675070, acc.: 67.19%] [G loss: 1.088594]\n",
      "3080 [D loss: 0.672512, acc.: 62.50%] [G loss: 0.983371]\n",
      "3081 [D loss: 0.651546, acc.: 64.06%] [G loss: 0.954172]\n",
      "3082 [D loss: 0.747650, acc.: 53.12%] [G loss: 0.833749]\n",
      "3083 [D loss: 0.652453, acc.: 59.38%] [G loss: 1.102387]\n",
      "3084 [D loss: 0.642372, acc.: 64.06%] [G loss: 0.893767]\n",
      "3085 [D loss: 0.557986, acc.: 76.56%] [G loss: 1.214100]\n",
      "3086 [D loss: 0.676533, acc.: 56.25%] [G loss: 0.931275]\n",
      "3087 [D loss: 0.594694, acc.: 68.75%] [G loss: 1.187995]\n",
      "3088 [D loss: 0.686434, acc.: 57.81%] [G loss: 1.054038]\n",
      "3089 [D loss: 0.654216, acc.: 64.06%] [G loss: 1.015446]\n",
      "3090 [D loss: 0.718006, acc.: 54.69%] [G loss: 1.008320]\n",
      "3091 [D loss: 0.690424, acc.: 60.94%] [G loss: 0.898124]\n",
      "3092 [D loss: 0.565332, acc.: 71.88%] [G loss: 1.094801]\n",
      "3093 [D loss: 0.602366, acc.: 73.44%] [G loss: 0.862622]\n",
      "3094 [D loss: 0.770221, acc.: 54.69%] [G loss: 1.048133]\n",
      "3095 [D loss: 0.618839, acc.: 65.62%] [G loss: 1.115987]\n",
      "3096 [D loss: 0.647002, acc.: 62.50%] [G loss: 1.002876]\n",
      "3097 [D loss: 0.667723, acc.: 56.25%] [G loss: 1.045238]\n",
      "3098 [D loss: 0.686701, acc.: 57.81%] [G loss: 1.124206]\n",
      "3099 [D loss: 0.626911, acc.: 62.50%] [G loss: 1.007045]\n",
      "3100 [D loss: 0.601022, acc.: 73.44%] [G loss: 1.023108]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3101 [D loss: 0.735997, acc.: 50.00%] [G loss: 0.887076]\n",
      "3102 [D loss: 0.734198, acc.: 56.25%] [G loss: 0.921084]\n",
      "3103 [D loss: 0.628404, acc.: 67.19%] [G loss: 1.124214]\n",
      "3104 [D loss: 0.648940, acc.: 57.81%] [G loss: 1.045995]\n",
      "3105 [D loss: 0.670987, acc.: 59.38%] [G loss: 1.031070]\n",
      "3106 [D loss: 0.719812, acc.: 56.25%] [G loss: 0.949685]\n",
      "3107 [D loss: 0.722630, acc.: 56.25%] [G loss: 1.025729]\n",
      "3108 [D loss: 0.693243, acc.: 54.69%] [G loss: 0.959333]\n",
      "3109 [D loss: 0.661206, acc.: 62.50%] [G loss: 0.961327]\n",
      "3110 [D loss: 0.609389, acc.: 67.19%] [G loss: 1.094129]\n",
      "3111 [D loss: 0.586122, acc.: 75.00%] [G loss: 1.069576]\n",
      "3112 [D loss: 0.782198, acc.: 46.88%] [G loss: 0.857437]\n",
      "3113 [D loss: 0.692745, acc.: 54.69%] [G loss: 1.197393]\n",
      "3114 [D loss: 0.638655, acc.: 57.81%] [G loss: 1.015235]\n",
      "3115 [D loss: 0.693614, acc.: 51.56%] [G loss: 1.005939]\n",
      "3116 [D loss: 0.684554, acc.: 53.12%] [G loss: 1.058551]\n",
      "3117 [D loss: 0.563269, acc.: 71.88%] [G loss: 0.854542]\n",
      "3118 [D loss: 0.587899, acc.: 73.44%] [G loss: 0.943918]\n",
      "3119 [D loss: 0.658017, acc.: 59.38%] [G loss: 0.938441]\n",
      "3120 [D loss: 0.699372, acc.: 54.69%] [G loss: 0.955085]\n",
      "3121 [D loss: 0.610157, acc.: 68.75%] [G loss: 1.180948]\n",
      "3122 [D loss: 0.612676, acc.: 67.19%] [G loss: 0.850472]\n",
      "3123 [D loss: 0.704260, acc.: 59.38%] [G loss: 0.879965]\n",
      "3124 [D loss: 0.632776, acc.: 64.06%] [G loss: 1.082964]\n",
      "3125 [D loss: 0.617492, acc.: 65.62%] [G loss: 1.146667]\n",
      "3126 [D loss: 0.653450, acc.: 64.06%] [G loss: 0.971816]\n",
      "3127 [D loss: 0.555039, acc.: 75.00%] [G loss: 0.904508]\n",
      "3128 [D loss: 0.636646, acc.: 59.38%] [G loss: 1.011300]\n",
      "3129 [D loss: 0.674046, acc.: 67.19%] [G loss: 1.029855]\n",
      "3130 [D loss: 0.726353, acc.: 53.12%] [G loss: 1.016225]\n",
      "3131 [D loss: 0.643951, acc.: 60.94%] [G loss: 1.113492]\n",
      "3132 [D loss: 0.700372, acc.: 60.94%] [G loss: 0.970064]\n",
      "3133 [D loss: 0.652163, acc.: 65.62%] [G loss: 0.817324]\n",
      "3134 [D loss: 0.694033, acc.: 59.38%] [G loss: 0.859880]\n",
      "3135 [D loss: 0.688500, acc.: 59.38%] [G loss: 1.001061]\n",
      "3136 [D loss: 0.604378, acc.: 70.31%] [G loss: 1.075168]\n",
      "3137 [D loss: 0.586844, acc.: 73.44%] [G loss: 1.003892]\n",
      "3138 [D loss: 0.625496, acc.: 68.75%] [G loss: 0.966647]\n",
      "3139 [D loss: 0.604337, acc.: 65.62%] [G loss: 1.122889]\n",
      "3140 [D loss: 0.719063, acc.: 56.25%] [G loss: 1.058772]\n",
      "3141 [D loss: 0.648398, acc.: 64.06%] [G loss: 1.026456]\n",
      "3142 [D loss: 0.609394, acc.: 67.19%] [G loss: 1.024854]\n",
      "3143 [D loss: 0.698255, acc.: 59.38%] [G loss: 0.893731]\n",
      "3144 [D loss: 0.560857, acc.: 73.44%] [G loss: 1.008906]\n",
      "3145 [D loss: 0.687648, acc.: 53.12%] [G loss: 0.852107]\n",
      "3146 [D loss: 0.611743, acc.: 62.50%] [G loss: 1.083300]\n",
      "3147 [D loss: 0.631742, acc.: 67.19%] [G loss: 1.093152]\n",
      "3148 [D loss: 0.603756, acc.: 71.88%] [G loss: 1.068381]\n",
      "3149 [D loss: 0.795026, acc.: 46.88%] [G loss: 0.899257]\n",
      "3150 [D loss: 0.682951, acc.: 59.38%] [G loss: 0.864938]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3151 [D loss: 0.728525, acc.: 48.44%] [G loss: 1.012990]\n",
      "3152 [D loss: 0.721431, acc.: 57.81%] [G loss: 1.060644]\n",
      "3153 [D loss: 0.635289, acc.: 64.06%] [G loss: 1.106532]\n",
      "3154 [D loss: 0.620625, acc.: 65.62%] [G loss: 1.119775]\n",
      "3155 [D loss: 0.734312, acc.: 54.69%] [G loss: 0.789310]\n",
      "3156 [D loss: 0.692021, acc.: 54.69%] [G loss: 0.957519]\n",
      "3157 [D loss: 0.556513, acc.: 62.50%] [G loss: 0.981800]\n",
      "3158 [D loss: 0.707749, acc.: 53.12%] [G loss: 1.070836]\n",
      "3159 [D loss: 0.605331, acc.: 71.88%] [G loss: 0.969250]\n",
      "3160 [D loss: 0.638736, acc.: 64.06%] [G loss: 1.076390]\n",
      "3161 [D loss: 0.648297, acc.: 64.06%] [G loss: 0.873570]\n",
      "3162 [D loss: 0.589468, acc.: 71.88%] [G loss: 1.113505]\n",
      "3163 [D loss: 0.653060, acc.: 57.81%] [G loss: 1.127179]\n",
      "3164 [D loss: 0.601461, acc.: 71.88%] [G loss: 1.019318]\n",
      "3165 [D loss: 0.734277, acc.: 56.25%] [G loss: 0.921732]\n",
      "3166 [D loss: 0.691734, acc.: 57.81%] [G loss: 1.168581]\n",
      "3167 [D loss: 0.599317, acc.: 67.19%] [G loss: 1.059887]\n",
      "3168 [D loss: 0.649199, acc.: 65.62%] [G loss: 1.001957]\n",
      "3169 [D loss: 0.689811, acc.: 60.94%] [G loss: 0.973350]\n",
      "3170 [D loss: 0.640381, acc.: 60.94%] [G loss: 0.975135]\n",
      "3171 [D loss: 0.660092, acc.: 59.38%] [G loss: 1.046853]\n",
      "3172 [D loss: 0.759471, acc.: 48.44%] [G loss: 1.093752]\n",
      "3173 [D loss: 0.554890, acc.: 76.56%] [G loss: 1.201334]\n",
      "3174 [D loss: 0.649563, acc.: 54.69%] [G loss: 0.967814]\n",
      "3175 [D loss: 0.679410, acc.: 54.69%] [G loss: 1.013093]\n",
      "3176 [D loss: 0.753578, acc.: 51.56%] [G loss: 0.984216]\n",
      "3177 [D loss: 0.657239, acc.: 60.94%] [G loss: 0.948429]\n",
      "3178 [D loss: 0.742631, acc.: 54.69%] [G loss: 0.819947]\n",
      "3179 [D loss: 0.635356, acc.: 67.19%] [G loss: 0.992270]\n",
      "3180 [D loss: 0.685368, acc.: 54.69%] [G loss: 0.961958]\n",
      "3181 [D loss: 0.593104, acc.: 73.44%] [G loss: 0.924420]\n",
      "3182 [D loss: 0.717410, acc.: 50.00%] [G loss: 0.963620]\n",
      "3183 [D loss: 0.561443, acc.: 78.12%] [G loss: 1.128639]\n",
      "3184 [D loss: 0.679534, acc.: 54.69%] [G loss: 1.037961]\n",
      "3185 [D loss: 0.676821, acc.: 59.38%] [G loss: 1.039395]\n",
      "3186 [D loss: 0.628867, acc.: 64.06%] [G loss: 0.977819]\n",
      "3187 [D loss: 0.612243, acc.: 60.94%] [G loss: 0.991279]\n",
      "3188 [D loss: 0.569498, acc.: 73.44%] [G loss: 0.939868]\n",
      "3189 [D loss: 0.711742, acc.: 60.94%] [G loss: 0.848945]\n",
      "3190 [D loss: 0.613678, acc.: 71.88%] [G loss: 1.009353]\n",
      "3191 [D loss: 0.617312, acc.: 73.44%] [G loss: 1.211043]\n",
      "3192 [D loss: 0.618816, acc.: 62.50%] [G loss: 0.999083]\n",
      "3193 [D loss: 0.642344, acc.: 65.62%] [G loss: 1.012913]\n",
      "3194 [D loss: 0.599497, acc.: 70.31%] [G loss: 1.069953]\n",
      "3195 [D loss: 0.656827, acc.: 54.69%] [G loss: 1.124141]\n",
      "3196 [D loss: 0.626996, acc.: 73.44%] [G loss: 1.014284]\n",
      "3197 [D loss: 0.562990, acc.: 68.75%] [G loss: 1.182704]\n",
      "3198 [D loss: 0.775653, acc.: 48.44%] [G loss: 0.803175]\n",
      "3199 [D loss: 0.612773, acc.: 65.62%] [G loss: 1.007120]\n",
      "3200 [D loss: 0.639941, acc.: 59.38%] [G loss: 0.911594]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3201 [D loss: 0.794772, acc.: 50.00%] [G loss: 1.033293]\n",
      "3202 [D loss: 0.674477, acc.: 56.25%] [G loss: 0.989445]\n",
      "3203 [D loss: 0.725663, acc.: 59.38%] [G loss: 1.024502]\n",
      "3204 [D loss: 0.645243, acc.: 57.81%] [G loss: 1.098710]\n",
      "3205 [D loss: 0.632196, acc.: 67.19%] [G loss: 1.087114]\n",
      "3206 [D loss: 0.602904, acc.: 67.19%] [G loss: 1.055991]\n",
      "3207 [D loss: 0.658394, acc.: 62.50%] [G loss: 1.053501]\n",
      "3208 [D loss: 0.671260, acc.: 64.06%] [G loss: 0.869764]\n",
      "3209 [D loss: 0.616076, acc.: 64.06%] [G loss: 1.006868]\n",
      "3210 [D loss: 0.585650, acc.: 65.62%] [G loss: 0.909258]\n",
      "3211 [D loss: 0.721363, acc.: 45.31%] [G loss: 0.868389]\n",
      "3212 [D loss: 0.687007, acc.: 64.06%] [G loss: 0.979355]\n",
      "3213 [D loss: 0.615444, acc.: 70.31%] [G loss: 1.055900]\n",
      "3214 [D loss: 0.772425, acc.: 42.19%] [G loss: 0.841103]\n",
      "3215 [D loss: 0.630986, acc.: 60.94%] [G loss: 1.112950]\n",
      "3216 [D loss: 0.659039, acc.: 57.81%] [G loss: 0.915848]\n",
      "3217 [D loss: 0.607671, acc.: 71.88%] [G loss: 1.070812]\n",
      "3218 [D loss: 0.673687, acc.: 62.50%] [G loss: 0.961498]\n",
      "3219 [D loss: 0.656036, acc.: 60.94%] [G loss: 1.070075]\n",
      "3220 [D loss: 0.733842, acc.: 50.00%] [G loss: 0.958961]\n",
      "3221 [D loss: 0.665407, acc.: 62.50%] [G loss: 1.036757]\n",
      "3222 [D loss: 0.753358, acc.: 48.44%] [G loss: 0.978464]\n",
      "3223 [D loss: 0.716484, acc.: 57.81%] [G loss: 0.827613]\n",
      "3224 [D loss: 0.692496, acc.: 60.94%] [G loss: 0.981596]\n",
      "3225 [D loss: 0.636510, acc.: 67.19%] [G loss: 1.032134]\n",
      "3226 [D loss: 0.622593, acc.: 70.31%] [G loss: 1.049127]\n",
      "3227 [D loss: 0.696200, acc.: 54.69%] [G loss: 0.957143]\n",
      "3228 [D loss: 0.648684, acc.: 54.69%] [G loss: 0.994755]\n",
      "3229 [D loss: 0.555400, acc.: 65.62%] [G loss: 1.032426]\n",
      "3230 [D loss: 0.607782, acc.: 67.19%] [G loss: 1.255612]\n",
      "3231 [D loss: 0.619712, acc.: 68.75%] [G loss: 1.021449]\n",
      "3232 [D loss: 0.623758, acc.: 68.75%] [G loss: 1.091937]\n",
      "3233 [D loss: 0.582127, acc.: 71.88%] [G loss: 1.083100]\n",
      "3234 [D loss: 0.642525, acc.: 67.19%] [G loss: 1.152859]\n",
      "3235 [D loss: 0.704147, acc.: 54.69%] [G loss: 1.057224]\n",
      "3236 [D loss: 0.640732, acc.: 71.88%] [G loss: 0.913453]\n",
      "3237 [D loss: 0.691984, acc.: 62.50%] [G loss: 0.923443]\n",
      "3238 [D loss: 0.622790, acc.: 62.50%] [G loss: 1.034848]\n",
      "3239 [D loss: 0.624269, acc.: 67.19%] [G loss: 0.895619]\n",
      "3240 [D loss: 0.746882, acc.: 45.31%] [G loss: 0.982429]\n",
      "3241 [D loss: 0.644003, acc.: 64.06%] [G loss: 1.064336]\n",
      "3242 [D loss: 0.597746, acc.: 68.75%] [G loss: 1.094275]\n",
      "3243 [D loss: 0.724165, acc.: 51.56%] [G loss: 1.002606]\n",
      "3244 [D loss: 0.694782, acc.: 50.00%] [G loss: 0.929057]\n",
      "3245 [D loss: 0.610473, acc.: 62.50%] [G loss: 1.038024]\n",
      "3246 [D loss: 0.718492, acc.: 48.44%] [G loss: 0.964594]\n",
      "3247 [D loss: 0.641829, acc.: 65.62%] [G loss: 1.021962]\n",
      "3248 [D loss: 0.689401, acc.: 57.81%] [G loss: 1.007379]\n",
      "3249 [D loss: 0.575203, acc.: 75.00%] [G loss: 0.992728]\n",
      "3250 [D loss: 0.755947, acc.: 46.88%] [G loss: 1.033923]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3251 [D loss: 0.721860, acc.: 46.88%] [G loss: 0.881151]\n",
      "3252 [D loss: 0.621367, acc.: 65.62%] [G loss: 1.167559]\n",
      "3253 [D loss: 0.624883, acc.: 64.06%] [G loss: 1.089092]\n",
      "3254 [D loss: 0.623471, acc.: 64.06%] [G loss: 1.019610]\n",
      "3255 [D loss: 0.576884, acc.: 59.38%] [G loss: 1.054599]\n",
      "3256 [D loss: 0.662616, acc.: 57.81%] [G loss: 1.030127]\n",
      "3257 [D loss: 0.577911, acc.: 71.88%] [G loss: 0.960297]\n",
      "3258 [D loss: 0.559565, acc.: 75.00%] [G loss: 1.144754]\n",
      "3259 [D loss: 0.640915, acc.: 59.38%] [G loss: 1.092838]\n",
      "3260 [D loss: 0.676196, acc.: 54.69%] [G loss: 0.951682]\n",
      "3261 [D loss: 0.665510, acc.: 59.38%] [G loss: 0.981198]\n",
      "3262 [D loss: 0.686321, acc.: 60.94%] [G loss: 1.022032]\n",
      "3263 [D loss: 0.638775, acc.: 60.94%] [G loss: 0.944339]\n",
      "3264 [D loss: 0.608799, acc.: 64.06%] [G loss: 1.175190]\n",
      "3265 [D loss: 0.635110, acc.: 65.62%] [G loss: 1.095005]\n",
      "3266 [D loss: 0.694607, acc.: 56.25%] [G loss: 1.005481]\n",
      "3267 [D loss: 0.578664, acc.: 71.88%] [G loss: 1.188633]\n",
      "3268 [D loss: 0.691368, acc.: 62.50%] [G loss: 0.935181]\n",
      "3269 [D loss: 0.728918, acc.: 53.12%] [G loss: 1.059245]\n",
      "3270 [D loss: 0.579973, acc.: 71.88%] [G loss: 1.174229]\n",
      "3271 [D loss: 0.614110, acc.: 67.19%] [G loss: 1.189113]\n",
      "3272 [D loss: 0.651346, acc.: 64.06%] [G loss: 0.961901]\n",
      "3273 [D loss: 0.675064, acc.: 60.94%] [G loss: 1.022472]\n",
      "3274 [D loss: 0.586471, acc.: 70.31%] [G loss: 0.986280]\n",
      "3275 [D loss: 0.614045, acc.: 64.06%] [G loss: 1.063438]\n",
      "3276 [D loss: 0.674678, acc.: 57.81%] [G loss: 0.949043]\n",
      "3277 [D loss: 0.557186, acc.: 73.44%] [G loss: 1.200095]\n",
      "3278 [D loss: 0.609784, acc.: 70.31%] [G loss: 0.813379]\n",
      "3279 [D loss: 0.784414, acc.: 46.88%] [G loss: 0.848550]\n",
      "3280 [D loss: 0.715949, acc.: 51.56%] [G loss: 0.996121]\n",
      "3281 [D loss: 0.669771, acc.: 60.94%] [G loss: 1.234579]\n",
      "3282 [D loss: 0.738030, acc.: 57.81%] [G loss: 0.980273]\n",
      "3283 [D loss: 0.623976, acc.: 65.62%] [G loss: 1.031546]\n",
      "3284 [D loss: 0.582799, acc.: 68.75%] [G loss: 0.891459]\n",
      "3285 [D loss: 0.652460, acc.: 65.62%] [G loss: 1.075193]\n",
      "3286 [D loss: 0.628624, acc.: 64.06%] [G loss: 1.050479]\n",
      "3287 [D loss: 0.614437, acc.: 62.50%] [G loss: 0.982805]\n",
      "3288 [D loss: 0.744902, acc.: 51.56%] [G loss: 1.118509]\n",
      "3289 [D loss: 0.699343, acc.: 56.25%] [G loss: 0.863184]\n",
      "3290 [D loss: 0.703070, acc.: 53.12%] [G loss: 1.032335]\n",
      "3291 [D loss: 0.669543, acc.: 65.62%] [G loss: 0.999239]\n",
      "3292 [D loss: 0.681798, acc.: 60.94%] [G loss: 0.992847]\n",
      "3293 [D loss: 0.702739, acc.: 53.12%] [G loss: 1.059178]\n",
      "3294 [D loss: 0.633815, acc.: 70.31%] [G loss: 1.257748]\n",
      "3295 [D loss: 0.612366, acc.: 65.62%] [G loss: 1.051965]\n",
      "3296 [D loss: 0.729793, acc.: 48.44%] [G loss: 1.025647]\n",
      "3297 [D loss: 0.647491, acc.: 62.50%] [G loss: 0.938605]\n",
      "3298 [D loss: 0.612659, acc.: 71.88%] [G loss: 1.128086]\n",
      "3299 [D loss: 0.667211, acc.: 62.50%] [G loss: 1.091781]\n",
      "3300 [D loss: 0.701449, acc.: 64.06%] [G loss: 0.861182]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3301 [D loss: 0.622745, acc.: 68.75%] [G loss: 1.006573]\n",
      "3302 [D loss: 0.638699, acc.: 64.06%] [G loss: 1.057524]\n",
      "3303 [D loss: 0.714348, acc.: 53.12%] [G loss: 0.988287]\n",
      "3304 [D loss: 0.600453, acc.: 67.19%] [G loss: 0.917690]\n",
      "3305 [D loss: 0.680426, acc.: 53.12%] [G loss: 1.033072]\n",
      "3306 [D loss: 0.778231, acc.: 51.56%] [G loss: 0.980015]\n",
      "3307 [D loss: 0.678532, acc.: 54.69%] [G loss: 1.255395]\n",
      "3308 [D loss: 0.661894, acc.: 57.81%] [G loss: 0.909074]\n",
      "3309 [D loss: 0.650527, acc.: 62.50%] [G loss: 1.102990]\n",
      "3310 [D loss: 0.641923, acc.: 64.06%] [G loss: 1.059617]\n",
      "3311 [D loss: 0.685079, acc.: 60.94%] [G loss: 1.076812]\n",
      "3312 [D loss: 0.610134, acc.: 67.19%] [G loss: 1.044656]\n",
      "3313 [D loss: 0.633915, acc.: 60.94%] [G loss: 0.917440]\n",
      "3314 [D loss: 0.719587, acc.: 43.75%] [G loss: 1.077777]\n",
      "3315 [D loss: 0.557566, acc.: 76.56%] [G loss: 1.041339]\n",
      "3316 [D loss: 0.672796, acc.: 60.94%] [G loss: 1.000376]\n",
      "3317 [D loss: 0.613240, acc.: 68.75%] [G loss: 1.025907]\n",
      "3318 [D loss: 0.666703, acc.: 59.38%] [G loss: 1.172194]\n",
      "3319 [D loss: 0.645644, acc.: 57.81%] [G loss: 1.028700]\n",
      "3320 [D loss: 0.558843, acc.: 75.00%] [G loss: 0.891301]\n",
      "3321 [D loss: 0.658665, acc.: 56.25%] [G loss: 0.974673]\n",
      "3322 [D loss: 0.666829, acc.: 60.94%] [G loss: 0.874465]\n",
      "3323 [D loss: 0.755210, acc.: 42.19%] [G loss: 0.873890]\n",
      "3324 [D loss: 0.679143, acc.: 59.38%] [G loss: 1.067685]\n",
      "3325 [D loss: 0.595007, acc.: 65.62%] [G loss: 0.967875]\n",
      "3326 [D loss: 0.583205, acc.: 68.75%] [G loss: 0.961887]\n",
      "3327 [D loss: 0.655477, acc.: 67.19%] [G loss: 1.072447]\n",
      "3328 [D loss: 0.586091, acc.: 75.00%] [G loss: 0.956423]\n",
      "3329 [D loss: 0.633335, acc.: 59.38%] [G loss: 1.069909]\n",
      "3330 [D loss: 0.637188, acc.: 60.94%] [G loss: 0.897756]\n",
      "3331 [D loss: 0.777176, acc.: 53.12%] [G loss: 0.760115]\n",
      "3332 [D loss: 0.601007, acc.: 67.19%] [G loss: 1.148563]\n",
      "3333 [D loss: 0.733539, acc.: 51.56%] [G loss: 0.900912]\n",
      "3334 [D loss: 0.639144, acc.: 60.94%] [G loss: 1.210127]\n",
      "3335 [D loss: 0.723323, acc.: 54.69%] [G loss: 1.101515]\n",
      "3336 [D loss: 0.698139, acc.: 57.81%] [G loss: 1.061121]\n",
      "3337 [D loss: 0.633781, acc.: 64.06%] [G loss: 1.087229]\n",
      "3338 [D loss: 0.606620, acc.: 71.88%] [G loss: 1.031914]\n",
      "3339 [D loss: 0.651116, acc.: 65.62%] [G loss: 1.113927]\n",
      "3340 [D loss: 0.629420, acc.: 62.50%] [G loss: 1.062241]\n",
      "3341 [D loss: 0.691680, acc.: 54.69%] [G loss: 1.022263]\n",
      "3342 [D loss: 0.655296, acc.: 64.06%] [G loss: 1.299119]\n",
      "3343 [D loss: 0.688093, acc.: 60.94%] [G loss: 1.056522]\n",
      "3344 [D loss: 0.667873, acc.: 59.38%] [G loss: 1.130539]\n",
      "3345 [D loss: 0.604679, acc.: 64.06%] [G loss: 0.949557]\n",
      "3346 [D loss: 0.652820, acc.: 65.62%] [G loss: 1.157503]\n",
      "3347 [D loss: 0.671060, acc.: 60.94%] [G loss: 0.901414]\n",
      "3348 [D loss: 0.654541, acc.: 64.06%] [G loss: 0.958278]\n",
      "3349 [D loss: 0.712034, acc.: 51.56%] [G loss: 1.094442]\n",
      "3350 [D loss: 0.671811, acc.: 62.50%] [G loss: 0.944651]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3351 [D loss: 0.724123, acc.: 57.81%] [G loss: 0.980877]\n",
      "3352 [D loss: 0.613659, acc.: 65.62%] [G loss: 1.070991]\n",
      "3353 [D loss: 0.680703, acc.: 59.38%] [G loss: 0.977991]\n",
      "3354 [D loss: 0.614156, acc.: 65.62%] [G loss: 0.905086]\n",
      "3355 [D loss: 0.721953, acc.: 46.88%] [G loss: 0.907453]\n",
      "3356 [D loss: 0.734628, acc.: 51.56%] [G loss: 1.057971]\n",
      "3357 [D loss: 0.612689, acc.: 65.62%] [G loss: 0.865063]\n",
      "3358 [D loss: 0.698917, acc.: 56.25%] [G loss: 1.017667]\n",
      "3359 [D loss: 0.665174, acc.: 57.81%] [G loss: 0.989916]\n",
      "3360 [D loss: 0.681444, acc.: 64.06%] [G loss: 0.985307]\n",
      "3361 [D loss: 0.586817, acc.: 67.19%] [G loss: 1.076959]\n",
      "3362 [D loss: 0.669501, acc.: 57.81%] [G loss: 0.955848]\n",
      "3363 [D loss: 0.659108, acc.: 64.06%] [G loss: 0.879011]\n",
      "3364 [D loss: 0.549723, acc.: 70.31%] [G loss: 1.180696]\n",
      "3365 [D loss: 0.597412, acc.: 67.19%] [G loss: 0.966309]\n",
      "3366 [D loss: 0.707615, acc.: 51.56%] [G loss: 1.235052]\n",
      "3367 [D loss: 0.664864, acc.: 60.94%] [G loss: 0.907022]\n",
      "3368 [D loss: 0.724336, acc.: 53.12%] [G loss: 0.986967]\n",
      "3369 [D loss: 0.641569, acc.: 56.25%] [G loss: 1.051589]\n",
      "3370 [D loss: 0.713371, acc.: 51.56%] [G loss: 1.099900]\n",
      "3371 [D loss: 0.669972, acc.: 62.50%] [G loss: 1.031489]\n",
      "3372 [D loss: 0.614486, acc.: 67.19%] [G loss: 1.141891]\n",
      "3373 [D loss: 0.582287, acc.: 65.62%] [G loss: 0.920977]\n",
      "3374 [D loss: 0.633993, acc.: 62.50%] [G loss: 1.072896]\n",
      "3375 [D loss: 0.683639, acc.: 48.44%] [G loss: 1.073091]\n",
      "3376 [D loss: 0.634528, acc.: 65.62%] [G loss: 0.994930]\n",
      "3377 [D loss: 0.614261, acc.: 71.88%] [G loss: 0.934239]\n",
      "3378 [D loss: 0.663624, acc.: 57.81%] [G loss: 1.086203]\n",
      "3379 [D loss: 0.620180, acc.: 60.94%] [G loss: 1.012295]\n",
      "3380 [D loss: 0.575268, acc.: 75.00%] [G loss: 1.140098]\n",
      "3381 [D loss: 0.593908, acc.: 70.31%] [G loss: 1.292420]\n",
      "3382 [D loss: 0.650541, acc.: 64.06%] [G loss: 0.920350]\n",
      "3383 [D loss: 0.525852, acc.: 81.25%] [G loss: 0.923394]\n",
      "3384 [D loss: 0.668995, acc.: 54.69%] [G loss: 0.923779]\n",
      "3385 [D loss: 0.644901, acc.: 62.50%] [G loss: 1.022171]\n",
      "3386 [D loss: 0.734622, acc.: 51.56%] [G loss: 1.141946]\n",
      "3387 [D loss: 0.734446, acc.: 51.56%] [G loss: 0.929833]\n",
      "3388 [D loss: 0.610010, acc.: 67.19%] [G loss: 0.968751]\n",
      "3389 [D loss: 0.663911, acc.: 59.38%] [G loss: 0.919907]\n",
      "3390 [D loss: 0.629264, acc.: 68.75%] [G loss: 1.282048]\n",
      "3391 [D loss: 0.660286, acc.: 60.94%] [G loss: 0.982557]\n",
      "3392 [D loss: 0.765897, acc.: 48.44%] [G loss: 1.070578]\n",
      "3393 [D loss: 0.626700, acc.: 65.62%] [G loss: 1.029827]\n",
      "3394 [D loss: 0.690809, acc.: 53.12%] [G loss: 0.999604]\n",
      "3395 [D loss: 0.797913, acc.: 45.31%] [G loss: 0.904511]\n",
      "3396 [D loss: 0.604596, acc.: 70.31%] [G loss: 1.350878]\n",
      "3397 [D loss: 0.642292, acc.: 62.50%] [G loss: 1.063522]\n",
      "3398 [D loss: 0.771193, acc.: 48.44%] [G loss: 0.967492]\n",
      "3399 [D loss: 0.631827, acc.: 65.62%] [G loss: 1.079803]\n",
      "3400 [D loss: 0.620936, acc.: 65.62%] [G loss: 0.922558]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3401 [D loss: 0.647304, acc.: 65.62%] [G loss: 1.017817]\n",
      "3402 [D loss: 0.644743, acc.: 62.50%] [G loss: 1.180258]\n",
      "3403 [D loss: 0.625122, acc.: 67.19%] [G loss: 1.060955]\n",
      "3404 [D loss: 0.641160, acc.: 71.88%] [G loss: 1.000955]\n",
      "3405 [D loss: 0.570354, acc.: 68.75%] [G loss: 1.177267]\n",
      "3406 [D loss: 0.668191, acc.: 56.25%] [G loss: 0.927530]\n",
      "3407 [D loss: 0.695984, acc.: 57.81%] [G loss: 1.071175]\n",
      "3408 [D loss: 0.586472, acc.: 70.31%] [G loss: 1.014682]\n",
      "3409 [D loss: 0.616222, acc.: 70.31%] [G loss: 1.055955]\n",
      "3410 [D loss: 0.560703, acc.: 71.88%] [G loss: 1.071659]\n",
      "3411 [D loss: 0.637774, acc.: 67.19%] [G loss: 0.952689]\n",
      "3412 [D loss: 0.661753, acc.: 57.81%] [G loss: 1.173278]\n",
      "3413 [D loss: 0.625605, acc.: 60.94%] [G loss: 0.976955]\n",
      "3414 [D loss: 0.619529, acc.: 65.62%] [G loss: 1.055204]\n",
      "3415 [D loss: 0.614148, acc.: 70.31%] [G loss: 1.201015]\n",
      "3416 [D loss: 0.856357, acc.: 40.62%] [G loss: 1.045875]\n",
      "3417 [D loss: 0.593912, acc.: 68.75%] [G loss: 1.074987]\n",
      "3418 [D loss: 0.739668, acc.: 59.38%] [G loss: 0.875628]\n",
      "3419 [D loss: 0.721616, acc.: 56.25%] [G loss: 0.878107]\n",
      "3420 [D loss: 0.645120, acc.: 70.31%] [G loss: 0.949992]\n",
      "3421 [D loss: 0.670156, acc.: 57.81%] [G loss: 1.047551]\n",
      "3422 [D loss: 0.618229, acc.: 64.06%] [G loss: 0.964091]\n",
      "3423 [D loss: 0.705348, acc.: 53.12%] [G loss: 0.980387]\n",
      "3424 [D loss: 0.670310, acc.: 62.50%] [G loss: 1.015666]\n",
      "3425 [D loss: 0.664428, acc.: 60.94%] [G loss: 1.008038]\n",
      "3426 [D loss: 0.651236, acc.: 59.38%] [G loss: 1.008128]\n",
      "3427 [D loss: 0.631752, acc.: 57.81%] [G loss: 1.212599]\n",
      "3428 [D loss: 0.620148, acc.: 60.94%] [G loss: 1.003958]\n",
      "3429 [D loss: 0.572476, acc.: 70.31%] [G loss: 0.842395]\n",
      "3430 [D loss: 0.616425, acc.: 65.62%] [G loss: 0.985769]\n",
      "3431 [D loss: 0.607494, acc.: 67.19%] [G loss: 1.064189]\n",
      "3432 [D loss: 0.623325, acc.: 67.19%] [G loss: 1.154387]\n",
      "3433 [D loss: 0.712335, acc.: 53.12%] [G loss: 1.049036]\n",
      "3434 [D loss: 0.764803, acc.: 56.25%] [G loss: 0.920598]\n",
      "3435 [D loss: 0.652699, acc.: 67.19%] [G loss: 1.128068]\n",
      "3436 [D loss: 0.608153, acc.: 67.19%] [G loss: 1.158310]\n",
      "3437 [D loss: 0.623307, acc.: 64.06%] [G loss: 1.073065]\n",
      "3438 [D loss: 0.655994, acc.: 65.62%] [G loss: 1.037440]\n",
      "3439 [D loss: 0.656325, acc.: 64.06%] [G loss: 0.895849]\n",
      "3440 [D loss: 0.707608, acc.: 46.88%] [G loss: 0.905737]\n",
      "3441 [D loss: 0.628698, acc.: 67.19%] [G loss: 0.947987]\n",
      "3442 [D loss: 0.656097, acc.: 53.12%] [G loss: 0.967017]\n",
      "3443 [D loss: 0.614089, acc.: 68.75%] [G loss: 1.153654]\n",
      "3444 [D loss: 0.567780, acc.: 76.56%] [G loss: 1.137782]\n",
      "3445 [D loss: 0.689696, acc.: 64.06%] [G loss: 1.122580]\n",
      "3446 [D loss: 0.738156, acc.: 46.88%] [G loss: 0.952052]\n",
      "3447 [D loss: 0.782407, acc.: 51.56%] [G loss: 0.961206]\n",
      "3448 [D loss: 0.659598, acc.: 64.06%] [G loss: 0.965806]\n",
      "3449 [D loss: 0.564137, acc.: 71.88%] [G loss: 1.119450]\n",
      "3450 [D loss: 0.677210, acc.: 62.50%] [G loss: 1.139883]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3451 [D loss: 0.651221, acc.: 59.38%] [G loss: 0.916452]\n",
      "3452 [D loss: 0.653700, acc.: 53.12%] [G loss: 1.133144]\n",
      "3453 [D loss: 0.552873, acc.: 70.31%] [G loss: 1.175444]\n",
      "3454 [D loss: 0.748518, acc.: 51.56%] [G loss: 0.877502]\n",
      "3455 [D loss: 0.666153, acc.: 60.94%] [G loss: 1.025970]\n",
      "3456 [D loss: 0.678351, acc.: 62.50%] [G loss: 0.943452]\n",
      "3457 [D loss: 0.649129, acc.: 57.81%] [G loss: 0.955732]\n",
      "3458 [D loss: 0.686961, acc.: 56.25%] [G loss: 0.942016]\n",
      "3459 [D loss: 0.683246, acc.: 62.50%] [G loss: 0.996195]\n",
      "3460 [D loss: 0.521731, acc.: 79.69%] [G loss: 1.060414]\n",
      "3461 [D loss: 0.566449, acc.: 73.44%] [G loss: 1.007882]\n",
      "3462 [D loss: 0.635960, acc.: 67.19%] [G loss: 0.734512]\n",
      "3463 [D loss: 0.864484, acc.: 37.50%] [G loss: 0.839000]\n",
      "3464 [D loss: 0.632495, acc.: 64.06%] [G loss: 0.838623]\n",
      "3465 [D loss: 0.764800, acc.: 48.44%] [G loss: 0.977200]\n",
      "3466 [D loss: 0.698545, acc.: 64.06%] [G loss: 0.851628]\n",
      "3467 [D loss: 0.714438, acc.: 53.12%] [G loss: 1.144586]\n",
      "3468 [D loss: 0.587151, acc.: 73.44%] [G loss: 1.003863]\n",
      "3469 [D loss: 0.616798, acc.: 62.50%] [G loss: 0.967586]\n",
      "3470 [D loss: 0.694265, acc.: 54.69%] [G loss: 0.985689]\n",
      "3471 [D loss: 0.656641, acc.: 64.06%] [G loss: 1.055717]\n",
      "3472 [D loss: 0.621037, acc.: 65.62%] [G loss: 1.039144]\n",
      "3473 [D loss: 0.638657, acc.: 70.31%] [G loss: 1.084370]\n",
      "3474 [D loss: 0.777103, acc.: 48.44%] [G loss: 1.020041]\n",
      "3475 [D loss: 0.612609, acc.: 60.94%] [G loss: 1.001451]\n",
      "3476 [D loss: 0.561834, acc.: 65.62%] [G loss: 1.106916]\n",
      "3477 [D loss: 0.674893, acc.: 59.38%] [G loss: 1.151835]\n",
      "3478 [D loss: 0.658417, acc.: 64.06%] [G loss: 0.893368]\n",
      "3479 [D loss: 0.652820, acc.: 54.69%] [G loss: 1.106099]\n",
      "3480 [D loss: 0.653563, acc.: 62.50%] [G loss: 0.950091]\n",
      "3481 [D loss: 0.602136, acc.: 68.75%] [G loss: 1.001595]\n",
      "3482 [D loss: 0.662887, acc.: 62.50%] [G loss: 1.072572]\n",
      "3483 [D loss: 0.688794, acc.: 57.81%] [G loss: 0.973647]\n",
      "3484 [D loss: 0.623007, acc.: 68.75%] [G loss: 1.007861]\n",
      "3485 [D loss: 0.621952, acc.: 65.62%] [G loss: 1.053221]\n",
      "3486 [D loss: 0.731753, acc.: 54.69%] [G loss: 1.052957]\n",
      "3487 [D loss: 0.629421, acc.: 62.50%] [G loss: 1.022622]\n",
      "3488 [D loss: 0.674311, acc.: 60.94%] [G loss: 0.996622]\n",
      "3489 [D loss: 0.678804, acc.: 56.25%] [G loss: 1.003983]\n",
      "3490 [D loss: 0.726820, acc.: 50.00%] [G loss: 0.998892]\n",
      "3491 [D loss: 0.560253, acc.: 79.69%] [G loss: 0.952992]\n",
      "3492 [D loss: 0.606675, acc.: 68.75%] [G loss: 1.042421]\n",
      "3493 [D loss: 0.626576, acc.: 64.06%] [G loss: 1.056301]\n",
      "3494 [D loss: 0.691383, acc.: 51.56%] [G loss: 1.203684]\n",
      "3495 [D loss: 0.536102, acc.: 71.88%] [G loss: 1.098155]\n",
      "3496 [D loss: 0.745943, acc.: 48.44%] [G loss: 0.916312]\n",
      "3497 [D loss: 0.675440, acc.: 65.62%] [G loss: 0.973028]\n",
      "3498 [D loss: 0.624457, acc.: 67.19%] [G loss: 1.057409]\n",
      "3499 [D loss: 0.625938, acc.: 65.62%] [G loss: 1.187943]\n",
      "3500 [D loss: 0.680127, acc.: 65.62%] [G loss: 1.061566]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3501 [D loss: 0.631875, acc.: 70.31%] [G loss: 0.981805]\n",
      "3502 [D loss: 0.594600, acc.: 70.31%] [G loss: 1.069782]\n",
      "3503 [D loss: 0.711750, acc.: 48.44%] [G loss: 1.046851]\n",
      "3504 [D loss: 0.598841, acc.: 76.56%] [G loss: 0.990454]\n",
      "3505 [D loss: 0.768001, acc.: 50.00%] [G loss: 0.929878]\n",
      "3506 [D loss: 0.596292, acc.: 70.31%] [G loss: 0.867432]\n",
      "3507 [D loss: 0.649599, acc.: 70.31%] [G loss: 1.044572]\n",
      "3508 [D loss: 0.651036, acc.: 64.06%] [G loss: 1.145551]\n",
      "3509 [D loss: 0.608199, acc.: 70.31%] [G loss: 1.016127]\n",
      "3510 [D loss: 0.662140, acc.: 68.75%] [G loss: 0.930775]\n",
      "3511 [D loss: 0.768698, acc.: 51.56%] [G loss: 1.061972]\n",
      "3512 [D loss: 0.600495, acc.: 67.19%] [G loss: 0.876344]\n",
      "3513 [D loss: 0.634959, acc.: 60.94%] [G loss: 1.131958]\n",
      "3514 [D loss: 0.623453, acc.: 68.75%] [G loss: 0.956682]\n",
      "3515 [D loss: 0.709921, acc.: 53.12%] [G loss: 0.788772]\n",
      "3516 [D loss: 0.675807, acc.: 53.12%] [G loss: 1.008055]\n",
      "3517 [D loss: 0.590093, acc.: 71.88%] [G loss: 1.071345]\n",
      "3518 [D loss: 0.658830, acc.: 67.19%] [G loss: 0.966033]\n",
      "3519 [D loss: 0.572361, acc.: 76.56%] [G loss: 1.068632]\n",
      "3520 [D loss: 0.686548, acc.: 56.25%] [G loss: 0.976327]\n",
      "3521 [D loss: 0.705642, acc.: 60.94%] [G loss: 0.934128]\n",
      "3522 [D loss: 0.715332, acc.: 53.12%] [G loss: 0.913783]\n",
      "3523 [D loss: 0.688175, acc.: 53.12%] [G loss: 1.001706]\n",
      "3524 [D loss: 0.697062, acc.: 60.94%] [G loss: 0.849912]\n",
      "3525 [D loss: 0.660355, acc.: 60.94%] [G loss: 1.029036]\n",
      "3526 [D loss: 0.741552, acc.: 50.00%] [G loss: 1.035051]\n",
      "3527 [D loss: 0.692570, acc.: 60.94%] [G loss: 0.906225]\n",
      "3528 [D loss: 0.743296, acc.: 43.75%] [G loss: 0.943708]\n",
      "3529 [D loss: 0.515713, acc.: 78.12%] [G loss: 0.943461]\n",
      "3530 [D loss: 0.659548, acc.: 59.38%] [G loss: 1.080718]\n",
      "3531 [D loss: 0.708959, acc.: 59.38%] [G loss: 1.047559]\n",
      "3532 [D loss: 0.603168, acc.: 65.62%] [G loss: 0.853686]\n",
      "3533 [D loss: 0.619490, acc.: 65.62%] [G loss: 1.138011]\n",
      "3534 [D loss: 0.603235, acc.: 65.62%] [G loss: 1.145220]\n",
      "3535 [D loss: 0.725056, acc.: 56.25%] [G loss: 0.957170]\n",
      "3536 [D loss: 0.756260, acc.: 40.62%] [G loss: 0.761875]\n",
      "3537 [D loss: 0.622360, acc.: 68.75%] [G loss: 0.974457]\n",
      "3538 [D loss: 0.577629, acc.: 70.31%] [G loss: 1.085237]\n",
      "3539 [D loss: 0.646469, acc.: 64.06%] [G loss: 1.109617]\n",
      "3540 [D loss: 0.642625, acc.: 59.38%] [G loss: 0.981226]\n",
      "3541 [D loss: 0.636140, acc.: 68.75%] [G loss: 1.093478]\n",
      "3542 [D loss: 0.638959, acc.: 65.62%] [G loss: 0.876037]\n",
      "3543 [D loss: 0.647887, acc.: 68.75%] [G loss: 0.858507]\n",
      "3544 [D loss: 0.553666, acc.: 71.88%] [G loss: 1.124406]\n",
      "3545 [D loss: 0.685107, acc.: 53.12%] [G loss: 0.972537]\n",
      "3546 [D loss: 0.558366, acc.: 75.00%] [G loss: 1.005834]\n",
      "3547 [D loss: 0.723855, acc.: 56.25%] [G loss: 1.013778]\n",
      "3548 [D loss: 0.558453, acc.: 73.44%] [G loss: 1.158555]\n",
      "3549 [D loss: 0.665180, acc.: 60.94%] [G loss: 0.982531]\n",
      "3550 [D loss: 0.630872, acc.: 62.50%] [G loss: 1.214894]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3551 [D loss: 0.678238, acc.: 59.38%] [G loss: 0.888810]\n",
      "3552 [D loss: 0.663798, acc.: 62.50%] [G loss: 0.903568]\n",
      "3553 [D loss: 0.587067, acc.: 78.12%] [G loss: 0.868723]\n",
      "3554 [D loss: 0.607451, acc.: 65.62%] [G loss: 1.063556]\n",
      "3555 [D loss: 0.604534, acc.: 71.88%] [G loss: 1.083542]\n",
      "3556 [D loss: 0.571699, acc.: 71.88%] [G loss: 1.104329]\n",
      "3557 [D loss: 0.656172, acc.: 60.94%] [G loss: 1.012823]\n",
      "3558 [D loss: 0.598796, acc.: 71.88%] [G loss: 1.041904]\n",
      "3559 [D loss: 0.609585, acc.: 67.19%] [G loss: 0.955740]\n",
      "3560 [D loss: 0.690769, acc.: 59.38%] [G loss: 1.131894]\n",
      "3561 [D loss: 0.582680, acc.: 70.31%] [G loss: 1.022919]\n",
      "3562 [D loss: 0.551000, acc.: 73.44%] [G loss: 1.083643]\n",
      "3563 [D loss: 0.632419, acc.: 60.94%] [G loss: 1.058546]\n",
      "3564 [D loss: 0.721736, acc.: 53.12%] [G loss: 0.831010]\n",
      "3565 [D loss: 0.763409, acc.: 45.31%] [G loss: 1.075160]\n",
      "3566 [D loss: 0.731962, acc.: 45.31%] [G loss: 0.998774]\n",
      "3567 [D loss: 0.712113, acc.: 51.56%] [G loss: 0.975623]\n",
      "3568 [D loss: 0.738314, acc.: 53.12%] [G loss: 1.098616]\n",
      "3569 [D loss: 0.606515, acc.: 70.31%] [G loss: 0.949487]\n",
      "3570 [D loss: 0.710813, acc.: 54.69%] [G loss: 0.805790]\n",
      "3571 [D loss: 0.660574, acc.: 67.19%] [G loss: 0.981887]\n",
      "3572 [D loss: 0.663070, acc.: 57.81%] [G loss: 1.209277]\n",
      "3573 [D loss: 0.667264, acc.: 62.50%] [G loss: 1.122307]\n",
      "3574 [D loss: 0.582656, acc.: 68.75%] [G loss: 1.015765]\n",
      "3575 [D loss: 0.690604, acc.: 54.69%] [G loss: 1.086658]\n",
      "3576 [D loss: 0.754458, acc.: 50.00%] [G loss: 0.900120]\n",
      "3577 [D loss: 0.657213, acc.: 62.50%] [G loss: 1.127498]\n",
      "3578 [D loss: 0.686116, acc.: 51.56%] [G loss: 0.979910]\n",
      "3579 [D loss: 0.681427, acc.: 54.69%] [G loss: 0.885213]\n",
      "3580 [D loss: 0.583852, acc.: 70.31%] [G loss: 0.918848]\n",
      "3581 [D loss: 0.617096, acc.: 70.31%] [G loss: 1.032577]\n",
      "3582 [D loss: 0.583792, acc.: 75.00%] [G loss: 1.101738]\n",
      "3583 [D loss: 0.632312, acc.: 68.75%] [G loss: 1.006726]\n",
      "3584 [D loss: 0.711318, acc.: 48.44%] [G loss: 0.933361]\n",
      "3585 [D loss: 0.566200, acc.: 68.75%] [G loss: 0.948413]\n",
      "3586 [D loss: 0.677447, acc.: 60.94%] [G loss: 0.982516]\n",
      "3587 [D loss: 0.561037, acc.: 73.44%] [G loss: 1.167957]\n",
      "3588 [D loss: 0.708429, acc.: 59.38%] [G loss: 0.926947]\n",
      "3589 [D loss: 0.625047, acc.: 60.94%] [G loss: 1.113543]\n",
      "3590 [D loss: 0.609387, acc.: 70.31%] [G loss: 0.872890]\n",
      "3591 [D loss: 0.636876, acc.: 59.38%] [G loss: 0.991468]\n",
      "3592 [D loss: 0.701259, acc.: 56.25%] [G loss: 1.006893]\n",
      "3593 [D loss: 0.661119, acc.: 60.94%] [G loss: 1.004786]\n",
      "3594 [D loss: 0.612548, acc.: 68.75%] [G loss: 1.102222]\n",
      "3595 [D loss: 0.724015, acc.: 56.25%] [G loss: 0.908725]\n",
      "3596 [D loss: 0.575607, acc.: 73.44%] [G loss: 0.911378]\n",
      "3597 [D loss: 0.753364, acc.: 50.00%] [G loss: 0.845148]\n",
      "3598 [D loss: 0.707103, acc.: 54.69%] [G loss: 0.986015]\n",
      "3599 [D loss: 0.600757, acc.: 62.50%] [G loss: 0.975019]\n",
      "3600 [D loss: 0.582252, acc.: 65.62%] [G loss: 1.091248]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3601 [D loss: 0.719386, acc.: 53.12%] [G loss: 1.009885]\n",
      "3602 [D loss: 0.724455, acc.: 54.69%] [G loss: 0.943351]\n",
      "3603 [D loss: 0.704902, acc.: 57.81%] [G loss: 0.911263]\n",
      "3604 [D loss: 0.662990, acc.: 59.38%] [G loss: 1.035027]\n",
      "3605 [D loss: 0.635436, acc.: 60.94%] [G loss: 1.211542]\n",
      "3606 [D loss: 0.701880, acc.: 56.25%] [G loss: 1.006003]\n",
      "3607 [D loss: 0.653964, acc.: 62.50%] [G loss: 0.924847]\n",
      "3608 [D loss: 0.695246, acc.: 64.06%] [G loss: 0.861050]\n",
      "3609 [D loss: 0.686809, acc.: 59.38%] [G loss: 1.045630]\n",
      "3610 [D loss: 0.623100, acc.: 67.19%] [G loss: 1.005147]\n",
      "3611 [D loss: 0.648378, acc.: 64.06%] [G loss: 1.065855]\n",
      "3612 [D loss: 0.559501, acc.: 71.88%] [G loss: 0.985867]\n",
      "3613 [D loss: 0.705791, acc.: 51.56%] [G loss: 1.071313]\n",
      "3614 [D loss: 0.639004, acc.: 62.50%] [G loss: 0.986968]\n",
      "3615 [D loss: 0.563295, acc.: 68.75%] [G loss: 1.092207]\n",
      "3616 [D loss: 0.550205, acc.: 73.44%] [G loss: 1.072385]\n",
      "3617 [D loss: 0.596894, acc.: 70.31%] [G loss: 0.953977]\n",
      "3618 [D loss: 0.732048, acc.: 56.25%] [G loss: 0.861389]\n",
      "3619 [D loss: 0.663669, acc.: 57.81%] [G loss: 1.028008]\n",
      "3620 [D loss: 0.634987, acc.: 60.94%] [G loss: 0.939170]\n",
      "3621 [D loss: 0.676013, acc.: 56.25%] [G loss: 1.099864]\n",
      "3622 [D loss: 0.545872, acc.: 73.44%] [G loss: 1.285844]\n",
      "3623 [D loss: 0.696192, acc.: 54.69%] [G loss: 0.910415]\n",
      "3624 [D loss: 0.795945, acc.: 46.88%] [G loss: 0.798353]\n",
      "3625 [D loss: 0.637754, acc.: 68.75%] [G loss: 1.038356]\n",
      "3626 [D loss: 0.649912, acc.: 56.25%] [G loss: 0.864151]\n",
      "3627 [D loss: 0.689830, acc.: 57.81%] [G loss: 1.046730]\n",
      "3628 [D loss: 0.694020, acc.: 56.25%] [G loss: 1.067252]\n",
      "3629 [D loss: 0.632448, acc.: 64.06%] [G loss: 1.120888]\n",
      "3630 [D loss: 0.541775, acc.: 78.12%] [G loss: 0.949135]\n",
      "3631 [D loss: 0.555794, acc.: 76.56%] [G loss: 0.968320]\n",
      "3632 [D loss: 0.650987, acc.: 60.94%] [G loss: 0.982450]\n",
      "3633 [D loss: 0.611823, acc.: 67.19%] [G loss: 1.013836]\n",
      "3634 [D loss: 0.751776, acc.: 54.69%] [G loss: 0.967479]\n",
      "3635 [D loss: 0.717256, acc.: 53.12%] [G loss: 0.959190]\n",
      "3636 [D loss: 0.558284, acc.: 76.56%] [G loss: 1.253810]\n",
      "3637 [D loss: 0.672085, acc.: 65.62%] [G loss: 0.964062]\n",
      "3638 [D loss: 0.654556, acc.: 62.50%] [G loss: 0.862509]\n",
      "3639 [D loss: 0.637735, acc.: 65.62%] [G loss: 0.961283]\n",
      "3640 [D loss: 0.586250, acc.: 67.19%] [G loss: 1.069832]\n",
      "3641 [D loss: 0.606245, acc.: 65.62%] [G loss: 0.996261]\n",
      "3642 [D loss: 0.702623, acc.: 56.25%] [G loss: 0.914170]\n",
      "3643 [D loss: 0.629410, acc.: 64.06%] [G loss: 1.175779]\n",
      "3644 [D loss: 0.681898, acc.: 60.94%] [G loss: 1.008854]\n",
      "3645 [D loss: 0.762587, acc.: 53.12%] [G loss: 0.933896]\n",
      "3646 [D loss: 0.664916, acc.: 57.81%] [G loss: 0.978320]\n",
      "3647 [D loss: 0.655841, acc.: 62.50%] [G loss: 1.055195]\n",
      "3648 [D loss: 0.562592, acc.: 75.00%] [G loss: 1.039962]\n",
      "3649 [D loss: 0.627611, acc.: 59.38%] [G loss: 0.863945]\n",
      "3650 [D loss: 0.625018, acc.: 62.50%] [G loss: 1.022666]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3651 [D loss: 0.617325, acc.: 70.31%] [G loss: 0.857992]\n",
      "3652 [D loss: 0.665450, acc.: 62.50%] [G loss: 1.019657]\n",
      "3653 [D loss: 0.562612, acc.: 73.44%] [G loss: 0.986194]\n",
      "3654 [D loss: 0.681724, acc.: 59.38%] [G loss: 1.026158]\n",
      "3655 [D loss: 0.632697, acc.: 64.06%] [G loss: 0.993643]\n",
      "3656 [D loss: 0.605417, acc.: 62.50%] [G loss: 0.956974]\n",
      "3657 [D loss: 0.615794, acc.: 64.06%] [G loss: 0.981607]\n",
      "3658 [D loss: 0.775662, acc.: 53.12%] [G loss: 0.938633]\n",
      "3659 [D loss: 0.626019, acc.: 75.00%] [G loss: 0.915733]\n",
      "3660 [D loss: 0.636377, acc.: 60.94%] [G loss: 0.845252]\n",
      "3661 [D loss: 0.651142, acc.: 60.94%] [G loss: 1.052192]\n",
      "3662 [D loss: 0.626015, acc.: 67.19%] [G loss: 1.163262]\n",
      "3663 [D loss: 0.627452, acc.: 60.94%] [G loss: 0.897174]\n",
      "3664 [D loss: 0.719943, acc.: 54.69%] [G loss: 0.978082]\n",
      "3665 [D loss: 0.646748, acc.: 67.19%] [G loss: 0.983191]\n",
      "3666 [D loss: 0.699980, acc.: 53.12%] [G loss: 0.892374]\n",
      "3667 [D loss: 0.661991, acc.: 60.94%] [G loss: 0.856475]\n",
      "3668 [D loss: 0.545187, acc.: 73.44%] [G loss: 1.026968]\n",
      "3669 [D loss: 0.618361, acc.: 70.31%] [G loss: 1.009147]\n",
      "3670 [D loss: 0.718492, acc.: 53.12%] [G loss: 0.856485]\n",
      "3671 [D loss: 0.582691, acc.: 68.75%] [G loss: 1.089945]\n",
      "3672 [D loss: 0.663403, acc.: 54.69%] [G loss: 0.962995]\n",
      "3673 [D loss: 0.565026, acc.: 67.19%] [G loss: 1.142091]\n",
      "3674 [D loss: 0.527287, acc.: 78.12%] [G loss: 0.991897]\n",
      "3675 [D loss: 0.585314, acc.: 70.31%] [G loss: 1.149099]\n",
      "3676 [D loss: 0.669771, acc.: 62.50%] [G loss: 1.152813]\n",
      "3677 [D loss: 0.676451, acc.: 60.94%] [G loss: 1.011518]\n",
      "3678 [D loss: 0.628238, acc.: 64.06%] [G loss: 0.933759]\n",
      "3679 [D loss: 0.652252, acc.: 64.06%] [G loss: 0.987727]\n",
      "3680 [D loss: 0.750013, acc.: 57.81%] [G loss: 1.129257]\n",
      "3681 [D loss: 0.590521, acc.: 67.19%] [G loss: 1.070013]\n",
      "3682 [D loss: 0.778396, acc.: 48.44%] [G loss: 1.140241]\n",
      "3683 [D loss: 0.703942, acc.: 64.06%] [G loss: 1.155263]\n",
      "3684 [D loss: 0.724035, acc.: 56.25%] [G loss: 1.044267]\n",
      "3685 [D loss: 0.668923, acc.: 62.50%] [G loss: 1.032439]\n",
      "3686 [D loss: 0.724698, acc.: 51.56%] [G loss: 0.895019]\n",
      "3687 [D loss: 0.694423, acc.: 56.25%] [G loss: 0.857491]\n",
      "3688 [D loss: 0.658371, acc.: 57.81%] [G loss: 1.099130]\n",
      "3689 [D loss: 0.650693, acc.: 64.06%] [G loss: 0.927491]\n",
      "3690 [D loss: 0.675090, acc.: 60.94%] [G loss: 1.014660]\n",
      "3691 [D loss: 0.608086, acc.: 67.19%] [G loss: 1.140320]\n",
      "3692 [D loss: 0.664733, acc.: 59.38%] [G loss: 0.964315]\n",
      "3693 [D loss: 0.633693, acc.: 59.38%] [G loss: 1.079618]\n",
      "3694 [D loss: 0.689764, acc.: 54.69%] [G loss: 1.096549]\n",
      "3695 [D loss: 0.644433, acc.: 57.81%] [G loss: 0.894065]\n",
      "3696 [D loss: 0.599480, acc.: 71.88%] [G loss: 1.009094]\n",
      "3697 [D loss: 0.679932, acc.: 60.94%] [G loss: 1.012625]\n",
      "3698 [D loss: 0.687807, acc.: 59.38%] [G loss: 0.931548]\n",
      "3699 [D loss: 0.673281, acc.: 60.94%] [G loss: 0.950379]\n",
      "3700 [D loss: 0.786419, acc.: 48.44%] [G loss: 0.887006]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3701 [D loss: 0.753003, acc.: 53.12%] [G loss: 0.871041]\n",
      "3702 [D loss: 0.545290, acc.: 75.00%] [G loss: 1.039537]\n",
      "3703 [D loss: 0.690054, acc.: 51.56%] [G loss: 0.967796]\n",
      "3704 [D loss: 0.546261, acc.: 71.88%] [G loss: 0.921936]\n",
      "3705 [D loss: 0.676805, acc.: 64.06%] [G loss: 0.965066]\n",
      "3706 [D loss: 0.693693, acc.: 57.81%] [G loss: 0.856468]\n",
      "3707 [D loss: 0.618913, acc.: 71.88%] [G loss: 1.005637]\n",
      "3708 [D loss: 0.725623, acc.: 57.81%] [G loss: 0.813521]\n",
      "3709 [D loss: 0.623369, acc.: 64.06%] [G loss: 1.169836]\n",
      "3710 [D loss: 0.593701, acc.: 70.31%] [G loss: 1.151134]\n",
      "3711 [D loss: 0.599238, acc.: 67.19%] [G loss: 1.141413]\n",
      "3712 [D loss: 0.668116, acc.: 62.50%] [G loss: 0.921906]\n",
      "3713 [D loss: 0.655954, acc.: 59.38%] [G loss: 0.932668]\n",
      "3714 [D loss: 0.622516, acc.: 62.50%] [G loss: 0.907005]\n",
      "3715 [D loss: 0.683029, acc.: 57.81%] [G loss: 1.033007]\n",
      "3716 [D loss: 0.698203, acc.: 56.25%] [G loss: 1.002289]\n",
      "3717 [D loss: 0.596667, acc.: 64.06%] [G loss: 1.255094]\n",
      "3718 [D loss: 0.612838, acc.: 64.06%] [G loss: 1.059600]\n",
      "3719 [D loss: 0.642042, acc.: 57.81%] [G loss: 1.147160]\n",
      "3720 [D loss: 0.728829, acc.: 53.12%] [G loss: 0.990437]\n",
      "3721 [D loss: 0.638514, acc.: 62.50%] [G loss: 1.020982]\n",
      "3722 [D loss: 0.580412, acc.: 76.56%] [G loss: 1.079911]\n",
      "3723 [D loss: 0.730864, acc.: 57.81%] [G loss: 0.981536]\n",
      "3724 [D loss: 0.637811, acc.: 62.50%] [G loss: 0.904854]\n",
      "3725 [D loss: 0.728071, acc.: 60.94%] [G loss: 0.942125]\n",
      "3726 [D loss: 0.804714, acc.: 43.75%] [G loss: 0.967286]\n",
      "3727 [D loss: 0.615062, acc.: 70.31%] [G loss: 1.009473]\n",
      "3728 [D loss: 0.691726, acc.: 53.12%] [G loss: 1.245645]\n",
      "3729 [D loss: 0.638446, acc.: 64.06%] [G loss: 1.026722]\n",
      "3730 [D loss: 0.633464, acc.: 62.50%] [G loss: 1.086556]\n",
      "3731 [D loss: 0.635374, acc.: 67.19%] [G loss: 1.014545]\n",
      "3732 [D loss: 0.648372, acc.: 65.62%] [G loss: 0.977399]\n",
      "3733 [D loss: 0.628395, acc.: 64.06%] [G loss: 1.136839]\n",
      "3734 [D loss: 0.668648, acc.: 62.50%] [G loss: 0.989858]\n",
      "3735 [D loss: 0.659034, acc.: 64.06%] [G loss: 0.959902]\n",
      "3736 [D loss: 0.694472, acc.: 56.25%] [G loss: 1.127514]\n",
      "3737 [D loss: 0.624152, acc.: 62.50%] [G loss: 1.178738]\n",
      "3738 [D loss: 0.648270, acc.: 65.62%] [G loss: 1.057251]\n",
      "3739 [D loss: 0.650538, acc.: 57.81%] [G loss: 0.844657]\n",
      "3740 [D loss: 0.617861, acc.: 65.62%] [G loss: 1.087262]\n",
      "3741 [D loss: 0.682254, acc.: 56.25%] [G loss: 1.030730]\n",
      "3742 [D loss: 0.767139, acc.: 51.56%] [G loss: 1.038377]\n",
      "3743 [D loss: 0.651322, acc.: 67.19%] [G loss: 1.002879]\n",
      "3744 [D loss: 0.628647, acc.: 68.75%] [G loss: 0.964542]\n",
      "3745 [D loss: 0.600115, acc.: 64.06%] [G loss: 1.060107]\n",
      "3746 [D loss: 0.606099, acc.: 68.75%] [G loss: 0.972204]\n",
      "3747 [D loss: 0.667597, acc.: 60.94%] [G loss: 1.075005]\n",
      "3748 [D loss: 0.706670, acc.: 57.81%] [G loss: 1.220275]\n",
      "3749 [D loss: 0.697397, acc.: 56.25%] [G loss: 0.986315]\n",
      "3750 [D loss: 0.643773, acc.: 62.50%] [G loss: 0.943863]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3751 [D loss: 0.622095, acc.: 68.75%] [G loss: 0.992878]\n",
      "3752 [D loss: 0.621833, acc.: 59.38%] [G loss: 0.853429]\n",
      "3753 [D loss: 0.591025, acc.: 67.19%] [G loss: 1.066303]\n",
      "3754 [D loss: 0.620798, acc.: 59.38%] [G loss: 0.968615]\n",
      "3755 [D loss: 0.638106, acc.: 70.31%] [G loss: 1.074302]\n",
      "3756 [D loss: 0.607835, acc.: 73.44%] [G loss: 0.946304]\n",
      "3757 [D loss: 0.584227, acc.: 71.88%] [G loss: 0.941289]\n",
      "3758 [D loss: 0.581001, acc.: 67.19%] [G loss: 1.023858]\n",
      "3759 [D loss: 0.561045, acc.: 78.12%] [G loss: 0.815688]\n",
      "3760 [D loss: 0.636010, acc.: 60.94%] [G loss: 0.966387]\n",
      "3761 [D loss: 0.668969, acc.: 62.50%] [G loss: 0.898465]\n",
      "3762 [D loss: 0.670163, acc.: 62.50%] [G loss: 0.978265]\n",
      "3763 [D loss: 0.554584, acc.: 79.69%] [G loss: 1.020167]\n",
      "3764 [D loss: 0.615250, acc.: 67.19%] [G loss: 1.160795]\n",
      "3765 [D loss: 0.623967, acc.: 64.06%] [G loss: 1.003095]\n",
      "3766 [D loss: 0.561342, acc.: 75.00%] [G loss: 1.081649]\n",
      "3767 [D loss: 0.583428, acc.: 64.06%] [G loss: 1.113928]\n",
      "3768 [D loss: 0.621785, acc.: 67.19%] [G loss: 1.066657]\n",
      "3769 [D loss: 0.583840, acc.: 67.19%] [G loss: 1.091624]\n",
      "3770 [D loss: 0.702821, acc.: 48.44%] [G loss: 1.013970]\n",
      "3771 [D loss: 0.638306, acc.: 57.81%] [G loss: 1.078943]\n",
      "3772 [D loss: 0.638236, acc.: 71.88%] [G loss: 0.908857]\n",
      "3773 [D loss: 0.734921, acc.: 48.44%] [G loss: 0.972965]\n",
      "3774 [D loss: 0.595715, acc.: 64.06%] [G loss: 1.013422]\n",
      "3775 [D loss: 0.605971, acc.: 67.19%] [G loss: 1.037187]\n",
      "3776 [D loss: 0.578928, acc.: 67.19%] [G loss: 0.959463]\n",
      "3777 [D loss: 0.608092, acc.: 73.44%] [G loss: 0.817213]\n",
      "3778 [D loss: 0.670002, acc.: 59.38%] [G loss: 1.066083]\n",
      "3779 [D loss: 0.557657, acc.: 79.69%] [G loss: 1.098255]\n",
      "3780 [D loss: 0.549896, acc.: 79.69%] [G loss: 1.156091]\n",
      "3781 [D loss: 0.611894, acc.: 62.50%] [G loss: 1.204787]\n",
      "3782 [D loss: 0.683305, acc.: 57.81%] [G loss: 0.933434]\n",
      "3783 [D loss: 0.696487, acc.: 56.25%] [G loss: 0.936144]\n",
      "3784 [D loss: 0.691520, acc.: 57.81%] [G loss: 0.989103]\n",
      "3785 [D loss: 0.635700, acc.: 65.62%] [G loss: 1.086870]\n",
      "3786 [D loss: 0.823033, acc.: 42.19%] [G loss: 0.760396]\n",
      "3787 [D loss: 0.614979, acc.: 64.06%] [G loss: 0.963898]\n",
      "3788 [D loss: 0.614408, acc.: 75.00%] [G loss: 0.889565]\n",
      "3789 [D loss: 0.632041, acc.: 65.62%] [G loss: 1.060122]\n",
      "3790 [D loss: 0.658893, acc.: 57.81%] [G loss: 0.839890]\n",
      "3791 [D loss: 0.568236, acc.: 73.44%] [G loss: 1.055097]\n",
      "3792 [D loss: 0.593883, acc.: 64.06%] [G loss: 1.228656]\n",
      "3793 [D loss: 0.609360, acc.: 68.75%] [G loss: 1.034403]\n",
      "3794 [D loss: 0.674779, acc.: 56.25%] [G loss: 1.050852]\n",
      "3795 [D loss: 0.670280, acc.: 59.38%] [G loss: 0.870867]\n",
      "3796 [D loss: 0.576940, acc.: 68.75%] [G loss: 1.086536]\n",
      "3797 [D loss: 0.629471, acc.: 70.31%] [G loss: 1.150146]\n",
      "3798 [D loss: 0.605485, acc.: 71.88%] [G loss: 1.009743]\n",
      "3799 [D loss: 0.648022, acc.: 64.06%] [G loss: 1.078915]\n",
      "3800 [D loss: 0.665635, acc.: 59.38%] [G loss: 0.946642]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3801 [D loss: 0.618192, acc.: 68.75%] [G loss: 1.049469]\n",
      "3802 [D loss: 0.637692, acc.: 68.75%] [G loss: 1.174497]\n",
      "3803 [D loss: 0.676737, acc.: 59.38%] [G loss: 1.137721]\n",
      "3804 [D loss: 0.714069, acc.: 56.25%] [G loss: 1.045435]\n",
      "3805 [D loss: 0.719631, acc.: 65.62%] [G loss: 0.954154]\n",
      "3806 [D loss: 0.630116, acc.: 62.50%] [G loss: 1.151356]\n",
      "3807 [D loss: 0.691575, acc.: 53.12%] [G loss: 1.160326]\n",
      "3808 [D loss: 0.643495, acc.: 59.38%] [G loss: 0.791968]\n",
      "3809 [D loss: 0.540847, acc.: 68.75%] [G loss: 1.141310]\n",
      "3810 [D loss: 0.616326, acc.: 67.19%] [G loss: 1.152308]\n",
      "3811 [D loss: 0.680521, acc.: 54.69%] [G loss: 0.768450]\n",
      "3812 [D loss: 0.641727, acc.: 64.06%] [G loss: 1.016005]\n",
      "3813 [D loss: 0.690663, acc.: 54.69%] [G loss: 1.040279]\n",
      "3814 [D loss: 0.605832, acc.: 70.31%] [G loss: 1.091439]\n",
      "3815 [D loss: 0.660686, acc.: 56.25%] [G loss: 0.993379]\n",
      "3816 [D loss: 0.607257, acc.: 65.62%] [G loss: 1.013564]\n",
      "3817 [D loss: 0.605455, acc.: 68.75%] [G loss: 1.002496]\n",
      "3818 [D loss: 0.602205, acc.: 68.75%] [G loss: 1.055232]\n",
      "3819 [D loss: 0.677720, acc.: 64.06%] [G loss: 0.996289]\n",
      "3820 [D loss: 0.623869, acc.: 62.50%] [G loss: 0.930494]\n",
      "3821 [D loss: 0.641544, acc.: 65.62%] [G loss: 0.945204]\n",
      "3822 [D loss: 0.604392, acc.: 67.19%] [G loss: 1.094393]\n",
      "3823 [D loss: 0.584366, acc.: 70.31%] [G loss: 1.118545]\n",
      "3824 [D loss: 0.698516, acc.: 54.69%] [G loss: 1.078699]\n",
      "3825 [D loss: 0.693817, acc.: 54.69%] [G loss: 0.884425]\n",
      "3826 [D loss: 0.736060, acc.: 54.69%] [G loss: 0.981397]\n",
      "3827 [D loss: 0.665071, acc.: 53.12%] [G loss: 1.124536]\n",
      "3828 [D loss: 0.787716, acc.: 46.88%] [G loss: 0.910437]\n",
      "3829 [D loss: 0.602202, acc.: 64.06%] [G loss: 1.114325]\n",
      "3830 [D loss: 0.577532, acc.: 67.19%] [G loss: 1.280293]\n",
      "3831 [D loss: 0.670394, acc.: 50.00%] [G loss: 1.045042]\n",
      "3832 [D loss: 0.621042, acc.: 64.06%] [G loss: 0.971930]\n",
      "3833 [D loss: 0.758680, acc.: 51.56%] [G loss: 0.914849]\n",
      "3834 [D loss: 0.577393, acc.: 73.44%] [G loss: 1.112688]\n",
      "3835 [D loss: 0.683396, acc.: 59.38%] [G loss: 1.008726]\n",
      "3836 [D loss: 0.573215, acc.: 71.88%] [G loss: 1.313579]\n",
      "3837 [D loss: 0.723804, acc.: 53.12%] [G loss: 0.928814]\n",
      "3838 [D loss: 0.675584, acc.: 62.50%] [G loss: 1.003126]\n",
      "3839 [D loss: 0.606969, acc.: 73.44%] [G loss: 0.908995]\n",
      "3840 [D loss: 0.556327, acc.: 71.88%] [G loss: 1.024387]\n",
      "3841 [D loss: 0.709845, acc.: 62.50%] [G loss: 1.072437]\n",
      "3842 [D loss: 0.679853, acc.: 60.94%] [G loss: 0.900459]\n",
      "3843 [D loss: 0.674979, acc.: 60.94%] [G loss: 0.965776]\n",
      "3844 [D loss: 0.615722, acc.: 64.06%] [G loss: 0.892895]\n",
      "3845 [D loss: 0.766587, acc.: 50.00%] [G loss: 1.056702]\n",
      "3846 [D loss: 0.657580, acc.: 62.50%] [G loss: 0.873607]\n",
      "3847 [D loss: 0.619112, acc.: 64.06%] [G loss: 1.012321]\n",
      "3848 [D loss: 0.686974, acc.: 51.56%] [G loss: 1.137755]\n",
      "3849 [D loss: 0.684588, acc.: 64.06%] [G loss: 1.006705]\n",
      "3850 [D loss: 0.515105, acc.: 84.38%] [G loss: 1.082688]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3851 [D loss: 0.552954, acc.: 71.88%] [G loss: 1.039137]\n",
      "3852 [D loss: 0.598984, acc.: 64.06%] [G loss: 0.998433]\n",
      "3853 [D loss: 0.673125, acc.: 57.81%] [G loss: 1.083344]\n",
      "3854 [D loss: 0.640679, acc.: 60.94%] [G loss: 0.884528]\n",
      "3855 [D loss: 0.596749, acc.: 67.19%] [G loss: 1.233640]\n",
      "3856 [D loss: 0.624802, acc.: 57.81%] [G loss: 1.012543]\n",
      "3857 [D loss: 0.658268, acc.: 51.56%] [G loss: 0.916152]\n",
      "3858 [D loss: 0.670304, acc.: 59.38%] [G loss: 1.122779]\n",
      "3859 [D loss: 0.559039, acc.: 65.62%] [G loss: 1.110271]\n",
      "3860 [D loss: 0.702792, acc.: 57.81%] [G loss: 0.869637]\n",
      "3861 [D loss: 0.706784, acc.: 46.88%] [G loss: 1.017270]\n",
      "3862 [D loss: 0.706557, acc.: 54.69%] [G loss: 0.997104]\n",
      "3863 [D loss: 0.594723, acc.: 67.19%] [G loss: 1.061521]\n",
      "3864 [D loss: 0.645274, acc.: 54.69%] [G loss: 1.087587]\n",
      "3865 [D loss: 0.589309, acc.: 70.31%] [G loss: 1.155997]\n",
      "3866 [D loss: 0.638114, acc.: 65.62%] [G loss: 0.973134]\n",
      "3867 [D loss: 0.715815, acc.: 59.38%] [G loss: 1.020913]\n",
      "3868 [D loss: 0.664052, acc.: 57.81%] [G loss: 0.955748]\n",
      "3869 [D loss: 0.692585, acc.: 56.25%] [G loss: 0.923517]\n",
      "3870 [D loss: 0.704812, acc.: 56.25%] [G loss: 0.987584]\n",
      "3871 [D loss: 0.605051, acc.: 64.06%] [G loss: 1.139263]\n",
      "3872 [D loss: 0.701523, acc.: 50.00%] [G loss: 0.999920]\n",
      "3873 [D loss: 0.670992, acc.: 54.69%] [G loss: 0.941150]\n",
      "3874 [D loss: 0.717747, acc.: 50.00%] [G loss: 0.991072]\n",
      "3875 [D loss: 0.591971, acc.: 68.75%] [G loss: 1.127830]\n",
      "3876 [D loss: 0.618259, acc.: 65.62%] [G loss: 0.999725]\n",
      "3877 [D loss: 0.757138, acc.: 46.88%] [G loss: 0.998881]\n",
      "3878 [D loss: 0.737922, acc.: 50.00%] [G loss: 0.855212]\n",
      "3879 [D loss: 0.689092, acc.: 57.81%] [G loss: 0.837985]\n",
      "3880 [D loss: 0.510175, acc.: 79.69%] [G loss: 1.289241]\n",
      "3881 [D loss: 0.744187, acc.: 45.31%] [G loss: 0.857262]\n",
      "3882 [D loss: 0.634441, acc.: 65.62%] [G loss: 0.895752]\n",
      "3883 [D loss: 0.648886, acc.: 67.19%] [G loss: 1.080096]\n",
      "3884 [D loss: 0.591959, acc.: 67.19%] [G loss: 1.046845]\n",
      "3885 [D loss: 0.567806, acc.: 65.62%] [G loss: 1.096591]\n",
      "3886 [D loss: 0.683800, acc.: 56.25%] [G loss: 1.066232]\n",
      "3887 [D loss: 0.741405, acc.: 50.00%] [G loss: 1.093100]\n",
      "3888 [D loss: 0.688938, acc.: 62.50%] [G loss: 1.095912]\n",
      "3889 [D loss: 0.548776, acc.: 71.88%] [G loss: 1.198230]\n",
      "3890 [D loss: 0.656050, acc.: 62.50%] [G loss: 0.960163]\n",
      "3891 [D loss: 0.567862, acc.: 73.44%] [G loss: 1.067341]\n",
      "3892 [D loss: 0.628558, acc.: 65.62%] [G loss: 1.040635]\n",
      "3893 [D loss: 0.701957, acc.: 59.38%] [G loss: 1.075503]\n",
      "3894 [D loss: 0.679876, acc.: 64.06%] [G loss: 0.780006]\n",
      "3895 [D loss: 0.625707, acc.: 62.50%] [G loss: 1.131921]\n",
      "3896 [D loss: 0.658400, acc.: 57.81%] [G loss: 1.188853]\n",
      "3897 [D loss: 0.657279, acc.: 59.38%] [G loss: 1.086520]\n",
      "3898 [D loss: 0.651478, acc.: 56.25%] [G loss: 1.151043]\n",
      "3899 [D loss: 0.601371, acc.: 67.19%] [G loss: 1.327971]\n",
      "3900 [D loss: 0.611376, acc.: 70.31%] [G loss: 1.240937]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3901 [D loss: 0.604849, acc.: 71.88%] [G loss: 1.031444]\n",
      "3902 [D loss: 0.767630, acc.: 48.44%] [G loss: 0.937049]\n",
      "3903 [D loss: 0.602698, acc.: 67.19%] [G loss: 1.011428]\n",
      "3904 [D loss: 0.657421, acc.: 57.81%] [G loss: 1.043023]\n",
      "3905 [D loss: 0.607912, acc.: 65.62%] [G loss: 0.970056]\n",
      "3906 [D loss: 0.734399, acc.: 56.25%] [G loss: 0.939083]\n",
      "3907 [D loss: 0.598246, acc.: 64.06%] [G loss: 0.980251]\n",
      "3908 [D loss: 0.675990, acc.: 59.38%] [G loss: 1.099697]\n",
      "3909 [D loss: 0.619501, acc.: 64.06%] [G loss: 1.052564]\n",
      "3910 [D loss: 0.678798, acc.: 54.69%] [G loss: 1.099570]\n",
      "3911 [D loss: 0.732982, acc.: 53.12%] [G loss: 0.991392]\n",
      "3912 [D loss: 0.697151, acc.: 62.50%] [G loss: 0.941118]\n",
      "3913 [D loss: 0.701110, acc.: 59.38%] [G loss: 0.845095]\n",
      "3914 [D loss: 0.601499, acc.: 70.31%] [G loss: 1.090189]\n",
      "3915 [D loss: 0.610927, acc.: 71.88%] [G loss: 1.247472]\n",
      "3916 [D loss: 0.699837, acc.: 59.38%] [G loss: 0.978809]\n",
      "3917 [D loss: 0.599131, acc.: 71.88%] [G loss: 1.055341]\n",
      "3918 [D loss: 0.672660, acc.: 64.06%] [G loss: 1.111049]\n",
      "3919 [D loss: 0.548930, acc.: 71.88%] [G loss: 1.032219]\n",
      "3920 [D loss: 0.680420, acc.: 50.00%] [G loss: 1.069990]\n",
      "3921 [D loss: 0.623758, acc.: 67.19%] [G loss: 1.216171]\n",
      "3922 [D loss: 0.742837, acc.: 54.69%] [G loss: 1.148386]\n",
      "3923 [D loss: 0.702230, acc.: 62.50%] [G loss: 0.937911]\n",
      "3924 [D loss: 0.555499, acc.: 75.00%] [G loss: 1.062764]\n",
      "3925 [D loss: 0.724344, acc.: 53.12%] [G loss: 0.984221]\n",
      "3926 [D loss: 0.680360, acc.: 57.81%] [G loss: 0.893408]\n",
      "3927 [D loss: 0.689710, acc.: 56.25%] [G loss: 0.891422]\n",
      "3928 [D loss: 0.576716, acc.: 70.31%] [G loss: 0.991835]\n",
      "3929 [D loss: 0.606315, acc.: 70.31%] [G loss: 0.850234]\n",
      "3930 [D loss: 0.540497, acc.: 78.12%] [G loss: 0.884553]\n",
      "3931 [D loss: 0.693413, acc.: 57.81%] [G loss: 0.980858]\n",
      "3932 [D loss: 0.615990, acc.: 68.75%] [G loss: 1.195382]\n",
      "3933 [D loss: 0.652957, acc.: 59.38%] [G loss: 1.003946]\n",
      "3934 [D loss: 0.726651, acc.: 53.12%] [G loss: 0.956475]\n",
      "3935 [D loss: 0.684262, acc.: 57.81%] [G loss: 0.999020]\n",
      "3936 [D loss: 0.617871, acc.: 70.31%] [G loss: 1.086199]\n",
      "3937 [D loss: 0.579513, acc.: 71.88%] [G loss: 1.039107]\n",
      "3938 [D loss: 0.642680, acc.: 60.94%] [G loss: 1.035076]\n",
      "3939 [D loss: 0.670256, acc.: 54.69%] [G loss: 1.120892]\n",
      "3940 [D loss: 0.544707, acc.: 78.12%] [G loss: 1.094369]\n",
      "3941 [D loss: 0.661937, acc.: 64.06%] [G loss: 0.928901]\n",
      "3942 [D loss: 0.716429, acc.: 50.00%] [G loss: 1.128783]\n",
      "3943 [D loss: 0.624164, acc.: 65.62%] [G loss: 0.904948]\n",
      "3944 [D loss: 0.703283, acc.: 53.12%] [G loss: 0.971839]\n",
      "3945 [D loss: 0.624148, acc.: 64.06%] [G loss: 1.091865]\n",
      "3946 [D loss: 0.551662, acc.: 75.00%] [G loss: 1.285684]\n",
      "3947 [D loss: 0.640298, acc.: 67.19%] [G loss: 1.170116]\n",
      "3948 [D loss: 0.671598, acc.: 59.38%] [G loss: 0.975363]\n",
      "3949 [D loss: 0.561161, acc.: 67.19%] [G loss: 1.040996]\n",
      "3950 [D loss: 0.721361, acc.: 56.25%] [G loss: 1.065632]\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "3951 [D loss: 0.687595, acc.: 64.06%] [G loss: 0.875446]\n",
      "3952 [D loss: 0.588522, acc.: 67.19%] [G loss: 1.117717]\n",
      "3953 [D loss: 0.642277, acc.: 62.50%] [G loss: 1.042324]\n",
      "3954 [D loss: 0.596459, acc.: 65.62%] [G loss: 1.152363]\n",
      "3955 [D loss: 0.600699, acc.: 68.75%] [G loss: 0.950477]\n",
      "3956 [D loss: 0.722733, acc.: 51.56%] [G loss: 0.861396]\n",
      "3957 [D loss: 0.674560, acc.: 62.50%] [G loss: 1.071395]\n",
      "3958 [D loss: 0.660014, acc.: 64.06%] [G loss: 1.026356]\n",
      "3959 [D loss: 0.638823, acc.: 60.94%] [G loss: 1.042479]\n",
      "3960 [D loss: 0.656919, acc.: 60.94%] [G loss: 0.807652]\n",
      "3961 [D loss: 0.743934, acc.: 56.25%] [G loss: 1.073653]\n",
      "3962 [D loss: 0.633913, acc.: 64.06%] [G loss: 1.119636]\n",
      "3963 [D loss: 0.734444, acc.: 54.69%] [G loss: 0.967198]\n",
      "3964 [D loss: 0.609275, acc.: 62.50%] [G loss: 1.026139]\n",
      "3965 [D loss: 0.547363, acc.: 73.44%] [G loss: 1.050472]\n",
      "3966 [D loss: 0.679855, acc.: 64.06%] [G loss: 0.973489]\n",
      "3967 [D loss: 0.643293, acc.: 62.50%] [G loss: 1.003906]\n",
      "3968 [D loss: 0.658895, acc.: 62.50%] [G loss: 0.992937]\n",
      "3969 [D loss: 0.540131, acc.: 81.25%] [G loss: 1.010000]\n",
      "3970 [D loss: 0.788065, acc.: 43.75%] [G loss: 0.868044]\n",
      "3971 [D loss: 0.583681, acc.: 70.31%] [G loss: 1.192970]\n",
      "3972 [D loss: 0.696543, acc.: 56.25%] [G loss: 1.071098]\n",
      "3973 [D loss: 0.686320, acc.: 57.81%] [G loss: 1.060331]\n",
      "3974 [D loss: 0.605037, acc.: 67.19%] [G loss: 1.182350]\n",
      "3975 [D loss: 0.748310, acc.: 46.88%] [G loss: 1.018984]\n",
      "3976 [D loss: 0.588523, acc.: 75.00%] [G loss: 1.166749]\n",
      "3977 [D loss: 0.754365, acc.: 51.56%] [G loss: 0.957853]\n",
      "3978 [D loss: 0.686419, acc.: 56.25%] [G loss: 1.068687]\n",
      "3979 [D loss: 0.658102, acc.: 60.94%] [G loss: 1.034972]\n",
      "3980 [D loss: 0.661825, acc.: 64.06%] [G loss: 0.921212]\n",
      "3981 [D loss: 0.723570, acc.: 46.88%] [G loss: 0.955527]\n",
      "3982 [D loss: 0.663236, acc.: 57.81%] [G loss: 0.993893]\n",
      "3983 [D loss: 0.613734, acc.: 75.00%] [G loss: 1.093142]\n",
      "3984 [D loss: 0.555353, acc.: 73.44%] [G loss: 1.114915]\n",
      "3985 [D loss: 0.551411, acc.: 78.12%] [G loss: 1.021244]\n",
      "3986 [D loss: 0.651790, acc.: 57.81%] [G loss: 1.081398]\n",
      "3987 [D loss: 0.616937, acc.: 62.50%] [G loss: 1.154740]\n",
      "3988 [D loss: 0.707699, acc.: 56.25%] [G loss: 1.035703]\n",
      "3989 [D loss: 0.616245, acc.: 59.38%] [G loss: 0.984022]\n",
      "3990 [D loss: 0.749101, acc.: 46.88%] [G loss: 0.963119]\n",
      "3991 [D loss: 0.648029, acc.: 60.94%] [G loss: 1.123984]\n",
      "3992 [D loss: 0.597042, acc.: 67.19%] [G loss: 0.918280]\n",
      "3993 [D loss: 0.720179, acc.: 54.69%] [G loss: 1.145205]\n",
      "3994 [D loss: 0.677669, acc.: 59.38%] [G loss: 0.771530]\n",
      "3995 [D loss: 0.574291, acc.: 73.44%] [G loss: 1.109882]\n",
      "3996 [D loss: 0.703684, acc.: 56.25%] [G loss: 1.105735]\n",
      "3997 [D loss: 0.591022, acc.: 67.19%] [G loss: 1.032633]\n",
      "3998 [D loss: 0.676383, acc.: 59.38%] [G loss: 0.709803]\n",
      "3999 [D loss: 0.573654, acc.: 67.19%] [G loss: 1.046362]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "                #checkpoint = ModelCheckpoint(\"./\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "                #callbacks_list = [checkpoint]\n",
    "                model_json = self.generator.to_json()\n",
    "                with open(\"/content/gdrive/My Drive/model_gen_%d.json\" %epoch, \"w\") as json_file:\n",
    "                   json_file.write(model_json)\n",
    "                # serialize weights to HDF5\n",
    "                self.generator.save_weights(\"/content/gdrive/My Drive/model_gen_%d.h5\" %epoch)\n",
    "                print(\"Saved model to disk\")\n",
    "\n",
    "                model_json = self.discriminator.to_json()\n",
    "                with open(\"/content/gdrive/My Drive/model_dis_%d.json\" %epoch, \"w\") as json_file:\n",
    "                   json_file.write(model_json)\n",
    "                # serialize weights to HDF5\n",
    "                self.discriminator.save_weights(\"/content/gdrive/My Drive/model_dis_%d.h5\" %epoch)\n",
    "                print(\"Saved model to disk\")\n",
    "                \n",
    "\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"./mnist_%d.png\" % epoch)\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1473,
     "status": "ok",
     "timestamp": 1553453553361,
     "user": {
      "displayName": "Megh Bhalerao",
      "photoUrl": "https://lh3.googleusercontent.com/-sFkXmKzG5os/AAAAAAAAAAI/AAAAAAAADCw/-HXSFmytHsY/s64/photo.jpg",
      "userId": "14122289173155118268"
     },
     "user_tz": -330
    },
    "id": "u_EXPUGYVd0b",
    "outputId": "5e883e01-e17a-4a42-bbad-725d69eb6df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
